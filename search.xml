<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[天池-二手车交易价格预测]]></title>
    <url>%2F2020%2F03%2F30%2F%E5%A4%A9%E6%B1%A0%20-%20%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[作为第一次参加数据分析的比赛，之前一直把知识都停留在表面，等到自己真正写起来，才发现很多地方都不会！！最后成绩197，虽然很低，但是也是自己努力的成果，从一个连基础的机器学习算法都不了解的小白，到一个可以进行简单预测，并调参的中白 :) 得益于这次比赛是由 Datawhale与天池联合发起的，在论坛提供了很多学习的知识，并且有大佬分析自己的baseline和思路，这种学习的氛围真的挺好的，否则自己太容易变成闷葫芦了~~防止闭门造车！！ 先上个代码~ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import numpy as npimport pandas as pdfrom sklearn import preprocessingfrom scipy.special import boxcox, inv_boxcoximport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings('ignore')data_train = pd.read_csv("./Train_data.csv", sep=",")# print(data_train.info())# print(data_train.describe())data_test = pd.read_csv("./TestA_data.csv", sep=",")train_num = data_train.shape[0]y_train = data_train['price']## 注意这里对y进行了boxcox变换y_train = boxcox(y_train, 0)all_data = pd.concat((data_train, data_test)).reset_index(drop=True)all_data.drop(['price'], axis=1, inplace=True)def processing_data(X): number_X = X.select_dtypes(exclude=object) ## 1.将数值型feature里的缺失值填补为他们每个featuer的均值 my_imputer = preprocessing.Imputer(strategy='mean') new_number_X = my_imputer.fit_transform(number_X) ## （偏度）绝对值大于0.75的特征进行一个boxcox变换 for i in number_X.columns: if abs(number_X[i].skew()) &gt; 0.75: number_X[i] = boxcox(number_X[i], 0) number_X = pd.DataFrame(new_number_X, columns=number_X.columns) # print('--------------------数字型特征已处理完毕--------------------') category_X = X.select_dtypes(include=object) for i in category_X.columns: category_X[i] = category_X[i].fillna('noinfo') # print('--------------------类别特征已处理完毕--------------------') X = pd.concat([number_X, category_X], axis=1) return X# all_data = processing_data(all_data)all_data = pd.get_dummies(all_data, drop_first=True)from xgboost import XGBRegressorfrom lightgbm import LGBMRegressorfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import mean_squared_log_errorimport lightgbm as lgbfrom sklearn.pipeline import make_pipelinefrom scipy.special import boxcox, inv_boxcoxfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.svm import SVRfrom sklearn.linear_model import Lasso, LassoCV, RidgeCVX_train = all_data[:150000]X_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)X_test = all_data[150000:]# 第一次# &#123;'subsample': 1, 'scale_pos_weight': 0.5, 'reg_lambda': 1, 'reg_alpha': 0.5,# 'n_estimators': 450, 'min_child_weight': 1, 'max_depth': 6, 'max_delta_step': 3,# 'learning_rate': 0.08, 'colsample_bytree': 0.7&#125;# &#123;'subsample': 0.1, 'num_leaves': 350, 'n_estimators': 300, 'min_child_samples': 4, 'max_depth': 90,# 'learning_rate': 0.15&#125;# 0.11190316301363079## def find_best_param(model):# xgb_param_test = &#123;# 'n_estimators': [450],# 'max_depth': [6, 7, 8],# 'learning_rate': [0.02, 0.05, 0.08],# 'min_child_weight': [0, 1,2],# 'max_delta_step': [ 3,4,5],# 'subsample': [0.5, 0.8, 1],# 'colsample_bytree': [0.5, 0.7, 1],# 'reg_alpha': [0.2, 0.5, 1],# 'reg_lambda': [0.3, 0.5, 1],# 'scale_pos_weight': [0.3, 0.5, 1]# &#125;# # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同# grid_search = RandomizedSearchCV(estimator=model, param_distributions=xgb_param_test, scoring='mean_squared_error',# cv=3)# # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5)# grid_search.fit(X_train, y_train)# print(grid_search.best_params_)### ## xgbreg = XGBRegressor()# find_best_param(xgbreg)### def find_best_param1(model):# lgb_param_test = &#123;# 'num_leaves': [250, 300, 350],# 'max_depth': [90, 100, 130],# 'subsample': [0.1, 0.12, 0.15],# 'min_child_samples': [3, 4, 5],# 'learning_rate': [0.05,0.1,0.15],# 'n_estimators': [100,200,300]# &#125;## # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同# grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error',# cv=3)# # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5)# grid_search.fit(X_train, y_train)# print(grid_search.best_params_)### lgbreg = LGBMRegressor(objective='regression_l1')# find_best_param1(lgbreg)# # ======================================================print('--------------------正在进行模型训练--------------------')# 最终模型与测试集答案相比的误差：0.10836896959704842，n=10000，0.05xgbreg1 = XGBRegressor(subsample=1, scale_pos_weight=0.5, reg_lambda=1, reg_alpha=0.5, n_estimators=10000, min_child_weight=1, max_depth=6, max_delta_step=3, learning_rate=0.05, colsample_bytree=0.7)xgbreg1.fit(X_train, y_train)xgb_train_pred = xgbreg1.predict(X_train)xgb_test_pred = xgbreg1.predict(x_val)#lgbreg = LGBMRegressor(objective='regression_l1', subsample=0.1, num_leaves=350, min_child_samples=4, max_depth=90,n_estimators=10000,learning_rate=0.05)lgbreg.fit(X_train, y_train)lgb_train_pred = lgbreg.predict(X_train)lgb_test_pred = lgbreg.predict(x_val)# def rmsle(y, y_pred):# return np.sqrt(mean_squared_log_error(y, y_pred))from sklearn.metrics import mean_absolute_error, mean_squared_log_error# y_train = inv_boxcox(y_train,0)# lgb_train_pred = inv_boxcox(lgb_train_pred,0)# xgb_train_pred = inv_boxcox(xgb_train_pred,0)print('LGBM与训练集答案相比的对数误差：&#123;&#125;'.format(mean_absolute_error(y_train, lgb_train_pred)))print('Xgboost与训练集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_train, xgb_train_pred)))## # y_val = inv_boxcox(y_val,0)# # lgb_test_pred = inv_boxcox(lgb_test_pred,0)# # xgb_test_pred = inv_boxcox(xgb_test_pred,0)ensemble_pred = 0.5 * xgb_test_pred + 0.5 * lgb_test_predprint('lgb与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, lgb_test_pred)))print('xgb与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, xgb_test_pred)))print('最终模型与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, ensemble_pred)))p1 = lgbreg.predict(X_test)p2 = xgbreg1.predict(X_test)p = 0.5*p1+0.5*p2p = inv_boxcox(p,0)sub = pd.DataFrame()sub['SaleID'] =X_test.indexsub['price'] = psub.to_csv('./house3.csv',index=False)# xgbreg1 = XGBRegressor(subsample=0.8, scale_pos_weight=0.5, reg_lambda=0.5, reg_alpha=1,# n_estimators=4000, min_child_weight=1, max_depth=7, max_delta_step=2,# learning_rate=0.08, colsample_bytree=0.7)# lgbreg = LGBMRegressor(objective='regression_l1', subsample=0.15, num_leaves=350, min_child_samples=4, max_depth=90,n_estimators=4000)]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二手车交易价格预测]]></title>
    <url>%2F2020%2F03%2F30%2F%E5%A4%A9%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[算法常见的评估指标分类算法常见的评估指标如下： 对于二类分类器/分类算法，评价指标主要有accuracy， [Precision，Recall，F-score，Pr曲线]，ROC-AUC曲线。 对于多类分类器/分类算法，评价指标主要有accuracy， [宏平均和微平均，F-score]。 预测（横） 实际（纵） + - + tp fn - fp tn tp tn 代表预测正确 准确率(Accuracy) accuracy是最常见也是最基本的评价指标。但是，在二分类且正负样本不平衡的情况下，尤其是对于较少数样本类感兴趣时，accuracy基本无参考价值。如欺诈检测、癌症检测等，100个样例中，99个负例，1个正例。模型将任意样本都分为负例，accuracy值为0.99。但是，拿这个模型去检测新样本，一个正例也分不出来。 精确率(Precision)、召回率(Recall) precision是相对于模型预测而言的，可以理解为模型做出新预测时的自信度得分是多少或做出这个预测是对的可能性是多少。自信度。 recall是相对于真实标签而言的，可以理解为模型预测出的正例占实际正例的比例。覆盖率。 如果模型很贪婪，想要覆盖更多的样本，那么它就有可能会犯错。这个时候的recall值常很高，precision常很低。如果模型很保守，只对很确定的样本做出预测，则precision值常很高，recall值常很低。我们可以选择只看我们感兴趣的样本类，也就是较少数样本类的precision和recall来评价模型的好坏。 疾病检测、反垃圾等，是在保证精确率的条件下提升召回率；搜索等是在保证召回率的情况下提升精确率。 F1值(F1-score) F1值是个综合考虑precision值和recall值的指标。 多类别分类时，有宏平均(macro-average)和微平均(micro-average)两种。 宏平均是指先对每个类别单独计算F1值。取这些值的算术平均值作为全局指标。这种方式平等地对待每个类别，所以其值主要受稀有类别的影响，更能体现模型在稀有类别上的表现。 微平均是指先累加各个类别的tp、fp、tn、fn值，再由这些值来计算F1值。这种方式平等地对待每个样本，所以其值主要受到常见类别的影响。 ROC-AUC 无论的真实概率是多少，都不会影响sensitivity和specificity。也就是说，这两个指标是不会受到不平衡数据的影响的。而是会受到数据集中正负比例的影响的。 ROC曲线(Receiver Operating Characteristic Curve)是一个以fpr为轴，tpr为轴，取不同的score threshold画出来的。 基本上，ROC曲线下面积即AUC越大，或者说曲线越接近于左上角(fpr=0, tpr=1)，那么模型的分类效果就越好。一般来说，最优score threshold就是ROC曲线离基准线最远的一点或者说是ROC曲线离左上角最近的一点对应的阈值，再或者是根据用户自定义的cost function来决定的。 AUC就是从所有正例样本中随机选择出一个样本，在所有负例样本中随机选择出一个样本，使用分类器进行预测。将正例样本预测为正的概率记作，将负例样本预测为负的概率记作，的概率就等于AUC值。因此，AUC反映的是分类器对于样本的排序能力。根据这个解释，如果我们完全随机地对样本进行分类，那么AUC应该接近于0.5。另外，AUC值对于样本类别是否均衡并不敏感，这也是不均衡样本通常使用AUC评价分类器性能的一个原因。通常使用AUC的目的，一是为了比较不同模型性能的好坏，二是为了找到得到最佳指标值的那个阈值点。 PR-AUC PR曲线，是以P为轴，以R为轴，取不同的概率阈值得到不同的(p,r)点后画成的线。 为了解决P、R、F-Measure(即)的单点局限性，得到一个能够反映全局的指标，使用PR-AUC/AP。同样地，PR-AUC值越大，或者说曲线越接近右上角(p=1, r=1)，那么模型就越理想、越好。 AAP(Approximated Average Precision) AAP将PR-AUC面积分割成不同的长方形然后求面积和。 IAP(Interpolated Average Precision) 如果存在r’&gt;r且p’&gt;p，使用p’代替p参与面积计算。AAP会比IAP离实际的PR-AUC更近，面积也会小点。 PASCAL VOC中使用IAP作为AP值，认为这一方法能够有效地减少PR曲线中的抖动。然后对于多类别进行AP取平均操作后得mAP值。 算法倾向如果是“宁可错杀一千，不可放过一个”，可以设定在合理的precision值下，最高的recall值作为最优点，找到这个点对应的阈值。总之，我们可以根据具体的应用或者是偏好，在曲线上找到最优的点，去调整模型的阈值，从而得到一个符合具体应用的模型。 对于回归预测类常见的评估指标如下: 平均绝对误差（Mean Absolute Error，MAE），均方误差（Mean Squared Error，MSE），平均绝对百分误差（Mean Absolute Percentage Error，MAPE），均方根误差（Root Mean Squared Error）， R2（R-Square） 平均绝对误差 平均绝对误差（Mean Absolute Error，MAE）:平均绝对误差，其能更好地反映预测值与真实值误差的实际情况，其计算公式如下：$$MAE=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}{i}\right|$$均方误差 均方误差（Mean Squared Error，MSE）,均方误差,其计算公式为：$$MSE=1NN∑i=1(yi−^yi)2MSE=1N∑i=1N(yi−y^i)2$$R2（R-Square）的公式为： 残差平方和：$$SSres=∑(yi−^yi)2SSres=∑(yi−y^i)2$$总平均值:$$SStot=∑(yi−¯¯¯yi)2SStot=∑(yi−y¯i)2$$其中¯¯¯yy¯表示yy的平均值 得到R2R2表达式为：$$R^{2}=1-\frac{SS{res}}{SS_{tot}}=1-\frac{\sum\left(y_{i}-\hat{y}{i}\right)^{2}}{\sum\left(y{i}-\overline{y}\right)^{2}}$$R2用于度量因变量的变异中可由自变量解释部分所占的比例，取值范围是 0~1，R2越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以R2也称为拟合优度（Goodness of Fit）的统计量。 yi表示真实值，^yi表示预测值，¯yii表示样本均值。得分越高拟合效果越好。 偏度和峰度]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池-饿了么智慧物流]]></title>
    <url>%2F2020%2F03%2F30%2F%E5%A4%A9%E6%B1%A0-%E9%A5%BF%E4%BA%86%E4%B9%88%2F</url>
    <content type="text"><![CDATA[top 5% 1.feature 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333#!/usr/bin/env python# coding: utf-8import osimport timefrom tqdm import tqdmimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.utils import shufflepd.set_option('display.max_columns', None)import warningswarnings.filterwarnings('ignore')def task1_featuretask(train_path,test_path): print('----------------------文件读取----------------------') # 读取数据并加入date列 def read_datafile(rootpath, selct): file_path = rootpath + selct + '/' data_list = [] for f in os.listdir(file_path): date = f.split('.')[0].split('_')[1] if selct == 'action': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str&#125;) elif selct == 'order': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str&#125;) elif selct == 'courier': df = pd.read_csv(file_path + f) elif selct == 'distance': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str, 'target_tracking_id': str&#125;) df['date'] = date data_list.append(df) return pd.concat(data_list) def majorid(df): df['majorid'] = df['date'].map(str) + df['courier_id'].map(str) + '_' + df['wave_index'].map(str) return df def dropdate(df): df.drop(['date', 'courier_id', 'wave_index'], axis=1, inplace=True) return df print('----------------------TASK1~特征工程----------------------') print('----------------------action的操作(1/4)----------------------') ## action的操作 # courier_id 骑士id # wave_index 波次index # tracking_id 订单id # courier_wave_start_lng 波次起始时刻骑士位置 # courier_wave_start_lat 波次起始时刻骑士位置 # action_type 行为类型 # expect_time 行为对应时刻 action_train = read_datafile(train_path, 'action') action_test = read_datafile(test_path, 'action') action_train = majorid(action_train) action_test = majorid(action_test) def action_train_group(df): groups = df.groupby(['majorid']) df_future = [] df_last = [] for name, group in tqdm(groups): # future_data代表后面55%个 future_data = group.tail(int(group.shape[0] * 0.55)) last_data = group.drop(future_data.index) # last操作 last_data = last_data.tail(1) # 此时last_data 为最后一组数 last_data.reset_index(drop=True, inplace=True) # 对future处理:把第一个样本标记为正样本 # (一般是把(二分类)任务中要查找(识别)出来的类别称做正类) future_data['label'] = 0 future_data.reset_index(drop=True, inplace=True) future_data.loc[0, 'label'] = 1 # 标记正负样本 df_future.append(future_data) df_last.append(last_data) return_last = pd.concat(df_last) return_future = pd.concat(df_future) return_last.rename(&#123;'expect_time': 'last_time'&#125;, axis=1, inplace=True) # 把expecttime列重命名 return_future = shuffle(return_future) # 随机打乱顺序 return return_last, return_future def action_test_group(df): groups = df.groupby(['majorid']) df_future = [] df_last = [] for name, group in tqdm(groups): future_data = group[group['expect_time'] == 0] last_data = group.drop(future_data.index) # last操作 last_data = last_data.tail(1) last_data.reset_index(drop=True, inplace=True) # future操作 future_data['label'] = None df_future.append(future_data) df_last.append(last_data) return_last = pd.concat(df_last) return_future = pd.concat(df_future) return_last.rename(&#123;'expect_time': 'last_time'&#125;, axis=1, inplace=True) return return_last, return_future action_train_last, action_train_future = action_train_group(action_train) action_test_last, action_test_future = action_test_group(action_test) print('----------------------distance操作(2/4)----------------------') # ## distance操作 # courier_id 骑士id # wave_index 波次id # tracking_id 源订单id # source_type 源点类型（Assign/PickFood/DeliverFood） # source_lng 源点经度 # source_lat 源点纬度 # target_tracking_id 目标订单id # target_type 目标点类型（Assign/PickFood/DeliverFood） # target_lng 目标点经度 # target_lat 目标点纬度 # grid_distance 源点与目标点的高德距离 distance_train = read_datafile(train_path, 'distance') distance_test = read_datafile(test_path, 'distance') distance_train = majorid(distance_train) distance_test = majorid(distance_test) distance_train = dropdate(distance_train) distance_test = dropdate(distance_test) # 计算曼哈顿距离 def tanlism_distance(df): df['target_tan'] = (df['source_lat'] - df['target_lat']) / ( df['source_lng'] - df['target_lng']) # df会自动处理出正无穷和负无穷，很秀 df['target_tan'] = np.arctan(df['target_tan']) df['target_tan'] = np.degrees(df['target_tan']) df['target_MHD'] = abs(df['source_lat'] - df['target_lat']) + abs( df['source_lng'] - df['target_lng']) # 加入曼哈顿距离 return df distance_train = tanlism_distance(distance_train) distance_test = tanlism_distance(distance_test) rename_rule = &#123;'source_type': 'action_type'&#125; distance_test.rename(rename_rule, axis=1, inplace=True) distance_train.rename(rename_rule, axis=1, inplace=True) feature_train = pd.merge(left=action_train_last, right=distance_train, on=['majorid', 'tracking_id', 'action_type'], how='left') feature_test = pd.merge(left=action_test_last, right=distance_test, on=['majorid', 'tracking_id', 'action_type'], how='left') rename_rule = &#123;'tracking_id': 'last_tracking_id', 'action_type': 'last_action_type', 'target_tracking_id': 'tracking_id', 'target_type': 'action_type'&#125; feature_test.rename(rename_rule, axis=1, inplace=True) feature_train.rename(rename_rule, axis=1, inplace=True) feature_test.drop(['courier_wave_start_lng', 'courier_wave_start_lat'], axis=1, inplace=True) feature_train.drop(['courier_wave_start_lng', 'courier_wave_start_lat'], axis=1, inplace=True) feature_train = dropdate(feature_train) feature_test = dropdate(feature_test) feature_train = pd.merge(left=action_train_future, right=feature_train, on=['majorid', 'tracking_id', 'action_type'], how='left') feature_test = pd.merge(left=action_test_future, right=feature_test, on=['majorid', 'tracking_id', 'action_type'], how='left') del action_test, action_test_future, action_test_last del action_train, action_train_future, action_train_last del distance_test, distance_train print('----------------------order操作(3/4)----------------------') # ## order操作 # courier_id 骑士id # wave_index 波次id # tracking_id 订单id # weather_grade 天气状况(正常天气/轻微恶劣天气/恶劣天气/极恶劣天气/罕见恶劣天气) # pick_lng 取餐经度 # pick_lat 取餐纬度 # deliver_lng 送餐经度 # deliver_lat 送餐纬度 # create_time 订单创建时间 # confirm_time 订单确认时间 # assigned_time 订单分配时间 # time 时间戳 # promise_deliver_time 承诺送达时间 # estimate_pick_time 预计取餐时间 # aoi_id 送餐点所在aoi id，aoi_id是可以唯一标识一个小区，写字楼园区，学校，医院等 # shop_id 商户id，可以唯一标识一个商户 order_train = read_datafile(train_path, 'order') order_test = read_datafile(test_path, 'order') order_test = majorid(order_test) order_train = majorid(order_train) order_test = dropdate(order_test) order_train = dropdate(order_train) def tanlism_order(df): df['delivery_tan'] = (df['deliver_lat'] - df['pick_lat']) / (df['deliver_lng'] - df['pick_lng']) df['delivery_tan'] = np.arctan(df['delivery_tan']) df['delivery_tan'] = np.degrees(df['delivery_tan']) df['delivery_MHD'] = abs(df['deliver_lat'] - df['pick_lat']) + abs( df['deliver_lng'] - df['pick_lng']) # 加入曼哈顿距离 df.drop(['deliver_lat', 'pick_lat', 'deliver_lng', 'pick_lng'], axis=1, inplace=True) return df order_test = tanlism_order(order_test) order_train = tanlism_order(order_train) feature_test = pd.merge(left=feature_test, right=order_test, on=['majorid', 'tracking_id'], how='left') feature_train = pd.merge(left=feature_train, right=order_train, on=['majorid', 'tracking_id'], how='left') print('----------------------courier操作(4/4)----------------------') # ## courier操作 # courier_id 骑士id # level 新老骑士 # speed 骑士速度 # max_load 背单能力 courier_train = read_datafile(train_path, 'courier') courier_test = read_datafile(test_path, 'courier') feature_test = pd.merge(left=feature_test, right=courier_test, on=['courier_id', 'date'], how='left') feature_train = pd.merge(left=feature_train, right=courier_train, on=['courier_id', 'date'], how='left') # 加入新的特征：id, rush和road feature_train['id'] = range(feature_train.shape[0]) feature_test['id'] = range(feature_test.shape[0]) def add_rush(df): df['rush'] = (df['last_time'] - df['create_time']) / (df['promise_deliver_time'] - df['create_time']) return df feature_train = add_rush(feature_train) feature_test = add_rush(feature_test) a = time.strftime('%a_%H', time.localtime(1582094246)) # 根据日期转化 分为 工作日/周末 高峰/非高峰 def add_road(df): df['now'] = df['last_time'].apply(lambda x: time.strftime('%a_%H', time.localtime(x))) # 格式化时间戳为本地的时间 df['is_holiday'] = df['now'].apply(lambda x: 1 if x.split('_')[0] in ['Sat', 'Sun'] else 0) busytime = ['7', '8', '11', '12', '17', '18'] normtime = ['5', '6', '9', '10', '13', '14', '15', '16', '19', '20', '21', '22'] df['road'] = df['now'].apply( lambda x: 1 if x.split('_')[1] in busytime else 2 if x.split('_')[1] in normtime else 3) df.drop(['now'], axis=1, inplace=True) return df feature_test = add_road(feature_test) feature_train = add_road(feature_train) # weather_grade转化为特征 def weather(x): if x == '正常天气': x = 4 elif x == '轻微恶劣天气': x = 3 elif x == '恶劣天气': x = 2 elif x == '极恶劣天气': x = 1 else: x = 0 return x feature_train['weather_grade'] = feature_train['weather_grade'].apply(lambda x: weather(x)) feature_test['weather_grade'] = feature_test['weather_grade'].apply(lambda x: weather(x)) # 加入expect_time-create_time和promise_deliver_time-expect_time作为预测的目标值 # 一个时间是 从 下订单 到 这一action 之间的时间--&gt;已花费时间 # 另一方 是 从 这一action 到 承诺送达的时间--&gt;剩余时间 # expect_time就相当于当前时间 feature_train['expect_used_time'] = feature_train['expect_time'] - feature_train['create_time'] feature_train['will_residue_time'] = feature_train['promise_deliver_time'] - feature_train['expect_time'] feature_test['expect_used_time'] = 0 feature_test['will_residue_time'] = 0 # feature_train.info() print('----------------------异常值处理----------------------') # # 异常值处理 # 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数 def deal_outliers(df, col): # df = feature_train # col = 'expect_used_time' def Box_outliers(data_ser): iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) val_low = data_ser.quantile(0.25) - iqr val_up = data_ser.quantile(0.75) + iqr return val_low, val_up data_ser = df[col] val_low, val_up = Box_outliers(df[col]) std_data = df[col].quantile(0.5) df[col] = df[col].apply(lambda x: std_data if x &lt; val_low else std_data if x &gt; val_up else x) sns.boxplot(y=df[col], data=df, palette="Set1") return df[col] # 下边是实际需要运行的 feature_train['expect_used_time'] = deal_outliers(feature_train, 'expect_used_time') feature_train['will_residue_time'] = deal_outliers(feature_train, 'will_residue_time') feature_train['expect_used_time'].describe() feature_test.to_pickle('../user_data/feature_test_reg.pkl') feature_train.to_pickle('../user_data/feature_train_reg.pkl') print('******************特征工程完毕******************')if __name__ == '__main__': train_path = '../data/eleme_round1_train_20200313/' test_path = '../data/eleme_round1_testB_20200413/' task1_featuretask(train_path,test_path) 2.model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478#!/usr/bin/env python# coding: utf-8import osimport timeimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn import metricsfrom sklearn.externals import joblibfrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import GroupKFold, KFoldimport gcimport lightgbm as lgbpd.set_option('display.max_columns', None)import warningswarnings.filterwarnings('ignore')def task2_regressortask(): print('----------------------TASK2~回归问题12----------------------') feature_test = pd.read_pickle('../user_data/feature_test_reg.pkl') feature_train = pd.read_pickle('../user_data/feature_train_reg.pkl') reg_prediction = feature_test reg_prediction['expect_time'] = 0 # 注意这边的 'expect_time' # 为什么是0，是因为expect_time 早已变成last_time reg_prediction['expect_used_time'] = 0 reg_prediction['will_residue_time'] = 0 print('----------------------回归问题1预测expect_used_time----------------------') # # 建立时间预测的回归任务1 # print('----------------------回归1调参----------------------') # y_col = 'expect_used_time' # # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123; # 'n_estimators': [10000], # 'learning_rate': [0.05], # 'min_child_weight': [0], # 'max_delta_step': [1], # 'colsample_bytree': [0.5,0.8], # 'reg_alpha': [0.8,1,1.2], # 'reg_lambda': [0.8,1,1.2], # 'scale_pos_weight': [0.8,1,1.2], # 'num_leaves': [340,350,400], # 'max_depth': [90,100,130], # 'subsample': [ 0.1,0.12,0.14], # 'min_child_samples': [1,2,4,3], # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='mae', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMRegressor(objective = 'regression_l1',metric = 'mae') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) # 建立时间预测的回归任务1 y_col = 'expect_used_time' # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] t0 = time.time() model = lgb.LGBMRegressor(objective='regression_l1', metric='mae', subsample=0.14, scale_pos_weight=1, reg_lambda=1.2, reg_alpha=1, num_leaves=400, n_estimators=10000, min_child_weight=0, min_child_samples=2, max_depth=130, max_delta_step=1, learning_rate=0.05, colsample_bytree=0.5 ) valueK = 10 oof = [] df_importance_list = [] kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020) for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold&#123;&#125; Training ======================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='mae', early_stopping_rounds=100 ) pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_) df_oof = feature_train.iloc[val_idx][['id', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_) reg_prediction['expect_used_time'] += (pred_test / valueK) df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_ &#125;) df_importance_list.append(df_importance) # break joblib.dump(lgb_model, '../user_data/model_data/model1&#123;&#125;.dat'.format(fold_id)) # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val gc.collect() t1 = time.time() print('end train, use&#123;&#125; second'.format(t1 - t0)) df_oof = pd.concat(oof) mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred']) print('mae:', mae) print('----------------------回归问题2预测will_residue_time----------------------') # # 建立时间预测的回归任务2 # print('----------------------回归2调参----------------------') # y_col = 'will_residue_time' # # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] # # x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD'] # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123;'n_estimators':[10000], # 'num_leaves':[300], # 'max_depth':[10,20], # 'learning_rate': [0.05], # # 'bagging_fraction': [0.5,0.8,1], # # 'feature_fraction': [0.8], # 'reg_alpha' : [0,0.2,0.5], # 'reg_lambda' : [0,0.2,0.5] # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='mae', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMRegressor(objective = 'regression_l1',metric = 'mae') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) # 建立时间预测的回归任务2 y_col = 'will_residue_time' # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', # 'target_MHD'] x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD'] t0 = time.time() model = lgb.LGBMRegressor( metric='mae', num_leaves=64, max_depth=7, n_estimators=300, learning_rate=0.05, bagging_fraction=1, feature_fraction=0.8, reg_alpha=0, reg_lambda=0 ) valueK = 10 oof = [] df_importance_list = [] kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020) for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold&#123;&#125; Training ======================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='mae', early_stopping_rounds=100 ) pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_) df_oof = feature_train.iloc[val_idx][['id', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_) reg_prediction['will_residue_time'] += (pred_test / valueK) df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_ &#125;) df_importance_list.append(df_importance) # break joblib.dump(lgb_model, '../user_data/model_data/model2&#123;&#125;.dat'.format(fold_id)) # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val # gc.collect() t1 = time.time() print('end train, use&#123;&#125; second'.format(t1 - t0)) df_oof = pd.concat(oof) mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred']) print('mae:', mae) print('----------------------计算expect_time----------------------') # 还原expect_time reg_prediction['expect_time'] = reg_prediction['promise_deliver_time'] - reg_prediction[ 'will_residue_time'] # 只使用后一个 feature_train.to_pickle('../user_data/regfuture_train_regend.pkl') reg_prediction.to_pickle('../user_data/regfuture_test_regend.pkl') print('******************回归问题完毕******************')def task3_classifiertask(): print('----------------------TASK3~分类问题----------------------') feature_train = pd.read_pickle('../user_data/regfuture_train_regend.pkl') feature_test = pd.read_pickle('../user_data/regfuture_test_regend.pkl') # 特征 deadline--&gt;还剩下的时间 def deadLine(df): df['deadline'] = df['promise_deliver_time'] - df['expect_time'] df['need_speed'] = df['grid_distance'] / df['deadline'] return df feature_train = deadLine(feature_train) feature_test = deadLine(feature_test) # 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数 def deal_outliers(df, col): # df = feature_train # col = 'expect_used_time' def Box_outliers(data_ser): iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) val_low = data_ser.quantile(0.25) - iqr val_up = data_ser.quantile(0.75) + iqr return val_low, val_up data_ser = df[col] val_low, val_up = Box_outliers(df[col]) std_data = df[col].quantile(0.5) df[col] = df[col].apply(lambda x: std_data if x &lt; val_low else std_data if x &gt; val_up else x) sns.boxplot(y=df[col], data=df, palette="Set1") return df[col] feature_train['need_speed'] = deal_outliers(feature_train, 'need_speed') feature_train['deadline'] = deal_outliers(feature_train, 'deadline') feature_train['deadline'].plot.hist() def is_Picked(df): df['is_picked'] = df['last_time'] - df['estimate_pick_time'] df['is_picked'] = df['is_picked'].apply(lambda x: 0 if x &lt; 0 else 1) return df feature_test = is_Picked(feature_test) feature_train = is_Picked(feature_train) feature_train.to_pickle('../user_data/future_train_clf.pkl') feature_test.to_pickle('../user_data/future_test_clf.pkl') # # 分类模型 # 目标：提高回归任务的mae，以提高expect_time预测精准度。 feature_train = pd.read_pickle('../user_data/future_train_clf.pkl') feature_test = pd.read_pickle('../user_data/future_test_clf.pkl') # 临时特征工程 feature_test['label'] = 0 feature_train['train'] = 1 feature_test['train'] = 0 feature = feature_train.append(feature_test) # 将送餐点和商户的信息 标签编码LabelEncoder for f in ['aoi_id', 'shop_id']: print(f) lbl = LabelEncoder() feature[f] = lbl.fit_transform(feature[f].astype(str)) feature_train = feature[feature['train'] == 1].copy() feature_test = feature[feature['train'] == 0].copy() def total_Residue_Time(df): df['total_residue_time'] = df['promise_deliver_time'] - df['estimate_pick_time'] return df feature_train = total_Residue_Time(feature_train) feature_test = total_Residue_Time(feature_test) # 回到模型 prediction = feature_test[ ['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date', 'id', 'majorid', 'label']] prediction['label'] = 0 # print('----------------------分类调参----------------------') # y_col = 'label' # # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked'] # # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']# 939-911 # x_col = ['grid_distance', 'target_MHD', 'weather_grade', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'deadline', 'need_speed', 'is_picked']# # # x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked']# 783-772-718 # # x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked', 'aoi_id', 'shop_id', 'courier_id']# 797-767-716 # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123; # 'n_estimators': [10000], # 'learning_rate': [0.1], # 'num_leaves': [10,6,7], # 'max_depth': [300,32,64,128], # 'subsample': [0.5,0.8,1], # 'reg_alpha': [0.8,1,0,0.5], # 'reg_lambda': [0.8,1,0,0.5], # 'feature_fraction': [0.5,0.8,1], # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='accuracy', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=lgb_param_test, scoring='accuracy', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='auc', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMClassifier(metric = 'auc') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) y_col = 'label' # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked'] # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']# 939-911 # x_col = ['grid_distance', 'target_MHD', 'weather_grade', 'delivery_MHD', 'level', 'speed', 'max_load', 'r# 783-772-718 x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked', 'aoi_id', 'shop_id', 'courier_id'] model = lgb.LGBMClassifier(metric='auc', num_leaves=7, max_depth=128, learning_rate=0.05, n_estimators=10000, subsample=0.8, feature_fraction=0.8, reg_alpha=1, reg_lambda=0.5, ) oof = [] df_importance_list = [] kfold = GroupKFold(n_splits=10) for fold_id, (trn_idx, val_idx) in enumerate( kfold.split(feature_train[x_col], feature_train[y_col], feature_train['majorid'])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold_&#123;&#125; Training ================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='auc', early_stopping_rounds=100 ) pred_val = lgb_model.predict_proba(X_val, num_iteration=lgb_model.best_iteration_)[:, 1] df_oof = feature_train.iloc[val_idx][['id', 'majorid', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict_proba(feature_test[x_col], num_iteration=lgb_model.best_iteration_)[:, 1] prediction['label'] += pred_test / 10 df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_, &#125;) df_importance_list.append(df_importance) joblib.dump(lgb_model, '../user_data/model_data/model3&#123;&#125;.dat'.format(fold_id)) # 验证recall，对feature_train 和 oof def result_func(majorid): majorid = majorid.values.tolist() max_index = majorid.index(max(majorid)) result = np.zeros(len(majorid)) result[max_index] = 1 return result def Acc(df): TP = df[(df['label'] == 1) &amp; (df['result'] == 1)].shape[0] # 预测正确，预测的值为真 TN = df[(df['label'] == 0) &amp; (df['result'] == 0)].shape[0] # 预测正确，预测的值为假 FP = df[(df['label'] == 0) &amp; (df['result'] == 1)].shape[0] # 预测错误，预测的值为真 FN = df[(df['label'] == 1) &amp; (df['result'] == 0)].shape[0] # 预测错误，预测的值为假 TP_FN = df[df['label'] == 1].shape[0] TP_FP = df[df['result'] == 1].shape[0] print('recall:&#123;&#125;'.format(TP / TP_FN)) print('acc:&#123;&#125;'.format(TP / TP_FP)) feature_train['pred_val'] = lgb_model.predict_proba(feature_train[x_col], num_iteration=lgb_model.best_iteration_)[ :, 1] feature_train['result'] = feature_train.groupby(['majorid'])['pred_val'].transform(result_func) Acc(feature_train) print('******************分类问题完毕******************') print('----------------------zip输出----------------------') # 输出 prediction['rusult'] = prediction.groupby(['majorid'])['label'].transform(result_func) subfile = prediction[prediction['rusult'] == 1] result = subfile[ ['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date']] import zipfile os.makedirs('../action_predict', exist_ok=True) # f = zipfile.ZipFile('./action_predict/&#123;&#125;.zip'.format('result'), 'w', zipfile.ZIP_DEFLATED) for date in result['date'].unique(): df_temp = result[result['date'] == date] del df_temp['date'] df_temp.to_csv('../action_predict/action_&#123;&#125;.txt'.format(date), index=False) # f.write('./action_predict/action_&#123;&#125;.txt'.format(date), 'action_&#123;&#125;.txt'.format(date)) # f.close()if __name__ == '__main__': task2_regressortask() task3_classifiertask() 3.main 12345678from feature.feature import task1_featuretaskfrom model.model import task2_regressortask,task3_classifiertasktrain_path = '../data/eleme_round1_train_20200313/'test_path = '../data/eleme_round1_testB_20200413/'task1_featuretask(train_path,test_path)task2_regressortask()task3_classifiertask()]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统知识学习]]></title>
    <url>%2F2020%2F03%2F16%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐系统基础推荐系统简介推荐的概念 信息过滤系统 解决 信息过载 用户需求不明确的问题 利用一定的规则将物品排序 展示给需求不明确的用户 推荐 搜索区别 推荐个性化较强，用户被动的接受，希望能够提供持续的服务 搜索个性化弱，用户主动搜索，快速满足用户的需求 推荐和 web项目区别 构建稳定的信息流通通道 推荐 信息过滤系统 web 对结果有明确预期 推荐 结果是概率问题 Lambda 架构介绍 离线计算和实时计算共同提供服务的问题 离线计算优缺点 优点 能够处理的数据量可以很大 比如pb级别 缺点 速度比较慢 分钟级别的延迟 实时计算 优点 响应快 来一条数据处理一条 ms级别响应 缺点 处理的数据量小一些 离线计算的框架 hadoop hdfs mapreduce spark core , spark sql hive 实时计算框架 spark streaming storm flink 消息中间件 flume 日志采集系统 kafka 消息队列 存储相关 hbase nosql数据库 hive sql操作hdfs数据 推荐算法架构 召回 协同过滤 算相似度 memory base 基于模型的 model base 矩阵分解 基于内容 分词 词权重（提取关键词） tf-idf word2Vec 词向量 物品向量 排序 逻辑回归 策略调整 整体流程 lambda内 推荐算法 推荐模型构建流程 推荐算法概述 基于协同过滤的推荐算法 协同过滤实现 一 推荐模型构建流程Data(数据)-&gt;Features(特征)-&gt;ML Algorithm(机器学习算法)-&gt;Prediction Output(预测输出) 数据清洗/数据处理 数据来源 显性数据 Rating 打分 Comments 评论/评价 隐形数据 Order history 历史订单 Cart events 加购物车 Page views 页面浏览 Click-thru 点击 Search log 搜索记录 数据量/数据能否满足要求 特征工程 从数据中筛选特征 一个给定的商品，可能被拥有类似品味或需求的用户购买 使用用户行为数据描述商品 用数据表示特征 将所有用户行为合并在一起 ，形成一个user-item 矩阵 选择合适的算法 产生推荐结果 二 最经典的推荐算法：协同过滤推荐算法（Collaborative Filtering）协同过滤(CF)思路介绍 CF 物以类聚人以群分 做协同过滤的话 首先特征工程把 用户-物品的评分矩阵创建出来 基于用户的协同过滤 给用户A 找到最相似的N个用户 N个用户消费过哪些物品 N个用户消费过的物品中-A用户消费过的就是推荐结果 基于物品的协同过滤 给物品A 找到最相似的N个物品 A用户消费记录 找到这些物品的相似物品 从这些相似物品先去重-A用户消费过的就是推荐结果 三 相似度计算(Similarity Calculation)相似度计算 余弦相似度、皮尔逊相关系数 向量的夹角余弦值 皮尔逊会对向量的每一个分量做中心化 余弦只考虑方向 不考虑向量长度 如果评分数据是连续的数值比较适合中余弦、皮尔逊计算相似度 杰卡德相似度 交集/并集 计算评分是0 1 布尔值的相似度 使用不同相似度计算方式实现协同过滤 如果 买/没买 点/没点数据 0/1 适合使用杰卡德相似度 from sklearn.metrics import jaccard_similarity_score jaccard_similarity_score(df[‘Item A’],df[‘Item B’]) from sklearn.metrics.pairwise import pairwise_distances user_similar = 1-pairwise_distances(df,metric=’jaccard’) 一般用评分去做协同过滤 推荐使用皮尔逊相关系数(sim是交集除并集) 评分预测 $$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$ 基于用户和基于物品的协同过滤 严格上说，属于两种算法，实践中可以都做出来，对比效果，选择最靠谱的 协同过滤推荐算法代码实现： 构建数据集： 12345678910users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 构建数据集datasets = [ ["buy",None,"buy","buy",None], ["buy",None,None,"buy","buy"], ["buy",None,"buy",None,None], [None,"buy",None,"buy","buy"], ["buy","buy","buy",None,"buy"],] 计算时我们数据通常都需要对数据进行处理，或者编码，目的是为了便于我们对数据进行运算处理，比如这里是比较简单的情形，我们用1、0分别来表示用户的是否购买过该物品，则我们的数据集其实应该是这样的： 12345678910111213141516users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]import pandas as pddf = pd.DataFrame(datasets, columns=items, index=users)print(df) 有了数据集，接下来我们就可以进行相似度的计算，不过对于相似度的计算其实是有很多专门的相似度计算方法的，比如余弦相似度、皮尔逊相关系数、杰卡德相似度等等。这里我们选择使用杰卡德相似系数[0,1] 12345678910111213141516171819# 直接计算某两项的杰卡德相似系数from sklearn.metrics import jaccard_similarity_score# 计算Item A 和Item B的相似度print(jaccard_similarity_score(df["Item A"], df["Item B"]))# 计算所有的数据两两的杰卡德相似系数(相似度=1-距离)from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar) 有了两两的相似度，接下来就可以筛选TOP-N相似结果，并进行推荐了 User-Based CF 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)topN_users = &#123;&#125;# 遍历每一行数据for i in user_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = user_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_users[i] = top2print("Top2相似用户：")pprint(topN_users)rs_results = &#123;&#125;# 构建推荐结果for user, sim_users in topN_users.items(): rs_result = set() # 存储推荐结果 for sim_user in sim_users: # 构建初始的推荐结果 #dropna()的使用 b=a.dropna()，则直接将a中的nan值给删除掉了 # 使用loc:只能指定行列索引的名字 # 使用iloc可以通过索引的下标去获取 # 使用ix进行下表和名称组合做引 rs_result = rs_result.union(set(df.ix[sim_user].replace(0,np.nan).dropna().index)) # 过滤掉已经购买过的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results) Item-Based CF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar)topN_items = &#123;&#125;# 遍历每一行数据for i in item_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = item_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_items[i] = top2print("Top2相似物品：")pprint(topN_items)rs_results = &#123;&#125;# 构建推荐结果for user in df.index: # 遍历所有用户 rs_result = set() for item in df.ix[user].replace(0,np.nan).dropna().index: # 取出每个用户当前已购物品列表 # 根据每个物品找出最相似的TOP-N物品，构建初始推荐结果 rs_result = rs_result.union(topN_items[item]) # 过滤掉用户已购的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) # 添加到结果中 rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results) 关于协同过滤推荐算法使用的数据集 在前面的demo中，我们只是使用用户对物品的一个购买记录，类似也可以是比如浏览点击记录、收听记录等等。这样数据我们预测的结果其实相当于是在预测用户是否对某物品感兴趣，对于喜好程度不能很好的预测。 因此在协同过滤推荐算法中其实会更多的利用用户对物品的“评分”数据来进行预测，通过评分数据集，我们可以预测用户对于他没有评分过的物品的评分。其实现原理和思想和都是一样的，只是使用的数据集是用户-物品的评分数据。 关于用户-物品评分矩阵 用户-物品的评分矩阵，根据评分矩阵的稀疏程度会有不同的解决方案 稠密评分矩阵 稀疏评分矩阵 这里先介绍稠密评分矩阵的处理，稀疏矩阵的处理相对会复杂一些，我们到后面再来介绍。 使用协同过滤推荐算法对用户进行评分预测 数据集： 目的：预测用户1对物品E的评分 构建数据集：注意这里构建评分数据时，对于缺失的部分我们需要保留为None，如果设置为0那么会被当作评分值为0去对待 12345678910users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [5,3,4,4,None], [3,1,2,3,3], [4,3,4,3,5], [3,3,1,5,4], [1,5,5,2,1],] 计算相似度：对于评分数据这里我们采用皮尔逊相关系数[-1,1]来计算，-1表示强负相关，+1表示强正相关 pandas中corr方法可直接用于计算皮尔逊相关系数 12345678910111213df = pd.DataFrame(datasets, columns=items, index=users)print("用户之间的两两相似度：")# 直接计算皮尔逊相关系数# 默认是按列进行计算，因此如果计算用户间的相似度，当前需要进行转置user_similar = df.T.corr()print(user_similar.round(4))print("物品之间的两两相似度：")item_similar = df.corr()print(item_similar.round(4)) 123456789101112131415# 运行结果：用户之间的两两相似度： User1 User2 User3 User4 User5User1 1.0000 0.8528 0.7071 0.0000 -0.7921User2 0.8528 1.0000 0.4677 0.4900 -0.9001User3 0.7071 0.4677 1.0000 -0.1612 -0.4666User4 0.0000 0.4900 -0.1612 1.0000 -0.6415User5 -0.7921 -0.9001 -0.4666 -0.6415 1.0000物品之间的两两相似度： Item A Item B Item C Item D Item EItem A 1.0000 -0.4767 -0.1231 0.5322 0.9695Item B -0.4767 1.0000 0.6455 -0.3101 -0.4781Item C -0.1231 0.6455 1.0000 -0.7206 -0.4276Item D 0.5322 -0.3101 -0.7206 1.0000 0.5817Item E 0.9695 -0.4781 -0.4276 0.5817 1.0000 可以看到与用户1最相似的是用户2和用户3；与物品A最相似的物品分别是物品E和物品D。 注意：我们在预测评分时，往往是通过与其有正相关的用户或物品进行预测，如果不存在正相关的情况，那么将无法做出预测。这一点尤其是在稀疏评分矩阵中尤为常见，因为稀疏评分矩阵中很难得出正相关系数。 评分预测： User-Based CF 评分预测：使用用户间的相似度进行预测 关于评分预测的方法也有比较多的方案，下面介绍一种效果比较好的方案，该方案考虑了用户本身的评分评分以及近邻用户的加权平均相似度打分来进行预测：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$我们要预测用户1对物品E的评分，那么可以根据与用户1最近邻的用户2和用户3进行预测，计算如下： ​$$pred(u_1, i_5) =\cfrac{0.853+0.715}{0.85+0.71} = 3.91$$最终预测出用户1对物品5的评分为3.91 Item-Based CF 评分预测：使用物品间的相似度进行预测 这里利用物品相似度预测的计算同上，同样考虑了用户自身的平均打分因素，结合预测物品与相似物品的加权平均相似度打分进行来进行预测$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{j\in I_{rated}}sim(i,j)r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}$$我们要预测用户1对物品E的评分，那么可以根据与物品E最近邻的物品A和物品D进行预测，计算如下：$$pred(u_1, i_5) = \cfrac {0.975+0.58*4}{0.97+0.58} = 4.63$$对比可见，User-Based CF预测评分和Item-Based CF的评分结果也是存在差异的，因为严格意义上他们其实应当属于两种不同的推荐算法，各自在不同的领域不同场景下，都会比另一种的效果更佳，但具体哪一种更佳，必须经过合理的效果评估，因此在实现推荐系统时这两种算法往往都是需要去实现的，然后对产生的推荐效果进行评估分析选出更优方案。 基于模型的方法–解决用户和物品矩阵比较稀疏的情况 思想 通过机器学习算法，在数据中找出模式，并将用户与物品间的互动方式模式化 基于模型的协同过滤方式是构建协同过滤更高级的算法 近邻模型的问题 物品之间存在相关性, 信息量并不随着向量维度增加而线性增加 矩阵元素稀疏, 计算结果不稳定,增减一个向量维度, 导致近邻结果差异很大的情况存在 算法分类 基于图的模型 基于矩阵分解的方法 基于图的模型 基于邻域的模型看做基于图的模型的简单形式 原理 将用户的行为数据表示为二分图 基于二分图为用户进行推荐 根据两个顶点之间的路径数、路径长度和经过的顶点数来评价两个顶点的相关性 基于矩阵分解的模型–降维 原理 根据用户与物品的潜在表现，我们就可以预测用户对未评分的物品的喜爱程度 把原来的大矩阵, 近似分解成两个小矩阵的乘积, 在实际推荐计算时不再使用大矩阵, 而是使用分解得到的两个小矩阵 用户-物品评分矩阵A是M X N维, 即一共有M个用户, n个物品 我们选一个很小的数 K (K&lt;&lt; M, K&lt;&lt;N) 通过计算得到两个矩阵U V U是M * K矩阵 , 矩阵V是 N * K $U_{mk} V^{T}_{nk} 约等于 A_{m*n}$ 类似这样的计算过程就是矩阵分解 基于矩阵分解的方法 ALS交替最小二乘 ALS-WR(加权正则化交替最小二乘法): alternating-least-squares with weighted-λ –regularization 将用户(user)对商品(item)的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的过程中，评分缺失项得到了填充，也就是说我们可以基于这个填充的评分来给用户做商品推荐了。 SVD奇异值分解矩阵 ALS方法–交替最小二乘法来优化损失 ALS的矩阵分解算法常应用于推荐系统中，将用户(user)对商品(item)的评分矩阵，分解为用户对商品隐含特征的偏好矩阵，和商品在隐含特征上的映射矩阵。 与传统的矩阵分解SVD方法来分解矩阵R(R∈ℝm×n)不同的是，ALS(alternating least squares)希望找到两个低维矩阵，以 R̃ =XY 来逼近矩阵R，其中 ，X∈ℝm×d，Y∈ℝd×n，这样，将问题的复杂度由O(mn)转换为O((m+n)d)。 计算X和Y过程：首先用一个小于1的随机数初始化Y，并根据公式求X，此时就可以得到初始的XY矩阵了，根据平方差和得到的X，重新计算并覆盖Y，计算差平方和，反复进行以上两步的计算，直到差平方和小于一个预设的数，或者迭代次数满足要求则停止 四 推荐系统评估 好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢 显示反馈和隐式反馈 显式反馈 隐式反馈 例子 电影/书籍评分 是否喜欢这个推荐 播放/点击 评论 下载 购买 准确性 高 低 数量 少 多 获取成本 高 低 常用评估指标 • 准确性 • 信任度• 满意度 • 实时性• 覆盖率 • 鲁棒性• 多样性 • 可扩展性• 新颖性 • 商业⽬标• 惊喜度 • ⽤户留存 准确性 (理论角度) Netflix 美国录像带租赁 评分预测–线性回归 RMSE MAE topN推荐 召回率 精准率 准确性 (业务角度) 覆盖度 信息熵 对于推荐越大越好 覆盖率 多样性&amp;新颖性&amp;惊喜性 多样性：推荐列表中两两物品的不相似性。（相似性如何度量？ 新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度 惊喜性：历史不相似（惊）但很满意（喜） 往往需要牺牲准确性 使⽤历史⾏为预测⽤户对某个物品的喜爱程度 系统过度强调实时性 Exploitation &amp; Exploration 探索与利用问题 Exploitation(开发 利用)：选择现在可能最佳的⽅案 Exploration(探测 搜索)：选择现在不确定的⼀些⽅案，但未来可能会有⾼收益的⽅案 在做两类决策的过程中，不断更新对所有决策的不确定性的认知，优化长期的⽬标 EE问题实践 兴趣扩展: 相似话题, 搭配推荐 人群算法: userCF 用户聚类 平衡个性化推荐和热门推荐比例 随机丢弃用户行为历史 随机扰动模型参数 EE可能带来的问题 探索伤害用户体验, 可能导致用户流失 探索带来的长期收益(留存率)评估周期长, KPI压力大 如何平衡实时兴趣和长期兴趣 如何平衡短期产品体验和长期系统生态 如何平衡大众口味和小众需求 评估方法 问卷调查: 成本高 离线评估: 只能在用户看到过的候选集上做评估, 且跟线上真实效果存在偏差 只能评估少数指标 速度快, 不损害用户体验 在线评估: 灰度发布 &amp; A/B测试 50% 全量上线 实践: 离线评估和在线评估结合, 定期做问卷调查 五 推荐系统的冷启动问题 推荐系统冷启动概念 ⽤户冷启动：如何为新⽤户做个性化推荐 物品冷启动：如何将新物品推荐给⽤户（协同过滤） 系统冷启动：⽤户冷启动+物品冷启动 本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好 用户冷启动 1.收集⽤户特征 ⽤户注册信息：性别、年龄、地域 设备信息：定位、⼿机型号、app列表 社交信息、推⼴素材、安装来源 2 引导用户填写兴趣 3 使用其它站点的行为数据, 例如腾讯视频&amp;QQ音乐 今日头条&amp;抖音 4 新老用户推荐策略的差异 新⽤户在冷启动阶段更倾向于热门排⾏榜，⽼⽤户会更加需要长尾推荐 Explore Exploit⼒度 使⽤单独的特征和模型预估 举例 性别与电视剧的关系 物品冷启动 给物品打标签–构建物品画像 利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。 系统冷启动 基于内容的推荐 系统早期 基于内容的推荐逐渐过渡到协同过滤 基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果 案例–基于协同过滤的电影推荐前面我们已经基本掌握了协同过滤推荐算法，以及其中两种最基本的实现方案：User-Based CF和Item-Based CF，下面我们将利用真是的数据来进行实战演练。 案例需求 演示效果 分析案例 数据集下载MovieLens Latest Datasets Small 建议下载ml-latest-small.zip，数据量小，便于我们单机使用和运行 目标：根据ml-latest-small/ratings.csv（用户-电影评分数据），分别实现User-Based CF和Item-Based CF，并进行电影评分的预测，然后为用户实现电影推荐 数据集加载 加载ratings.csv，并转换为用户-电影评分矩阵 123456789101112131415161718192021222324252627282930313233343536373839import osimport pandas as pdimport numpy as npDATA_PATH = "./datasets/ml-latest-small/ratings.csv"CACHE_DIR = "./datasets/cache/"def load_data(data_path): ''' 加载数据 :param data_path: 数据集路径 :param cache_path: 数据集缓存路径 :return: 用户-物品评分矩阵 ''' # 数据集缓存地址 cache_path = os.path.join(CACHE_DIR, "ratings_matrix.cache") print("开始加载数据集...") if os.path.exists(cache_path): # 判断是否存在缓存文件 print("加载缓存中...") ratings_matrix = pd.read_pickle(cache_path) # 转换成pickle加快操作 print("从缓存加载数据集完毕") else: print("加载新数据中...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) # 读取前三列 # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵 ratings_matrix = ratings.pivot_table(index=["userId"], columns=["movieId"], values="rating") # 存入缓存文件 ratings_matrix.to_pickle(cache_path) print("数据集加载完毕") return ratings_matrixif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) print(ratings_matrix) (https://www.cnblogs.com/Yanjy-OnlyOne/p/11195621.html) 透视表知识点补充 相似度计算 计算用户或物品两两相似度： 12345678910111213141516171819202122232425262728293031323334353637383940414243# ......def compute_pearson_similarity(ratings_matrix, based="user"): ''' 计算皮尔逊相关系数 :param ratings_matrix: 用户-物品评分矩阵 :param based: "user" or "item" :return: 相似度矩阵 ''' user_similarity_cache_path = os.path.join(CACHE_DIR, "user_similarity.cache") item_similarity_cache_path = os.path.join(CACHE_DIR, "item_similarity.cache") # 基于皮尔逊相关系数计算相似度 # 用户相似度 if based == "user": if os.path.exists(user_similarity_cache_path): print("正从缓存加载用户相似度矩阵") similarity = pd.read_pickle(user_similarity_cache_path) else: print("开始计算用户相似度矩阵") similarity = ratings_matrix.T.corr() similarity.to_pickle(user_similarity_cache_path) elif based == "item": if os.path.exists(item_similarity_cache_path): print("正从缓存加载物品相似度矩阵") similarity = pd.read_pickle(item_similarity_cache_path) else: print("开始计算物品相似度矩阵") similarity = ratings_matrix.corr() similarity.to_pickle(item_similarity_cache_path) else: raise Exception("Unhandled 'based' Value: %s"%based) print("相似度矩阵计算/加载完毕") return similarityif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") print(user_similar) item_similar = compute_pearson_similarity(ratings_matrix, based="item") print(item_similar) 注意以上实现，仅用于实验阶段，因为工业上、或生产环境中，数据量是远超过我们本例中使用的数据量的，而pandas是无法支撑起大批量数据的运算的，因此工业上通常会使用spark、mapReduce等分布式计算框架来实现，我们后面的课程中也是建立在此基础上进行实践的。 但是正如前面所说，推荐算法的思想和理念都是统一的，不论使用什么平台工具、有多大的数据体量，其背后的实现原理都是不变的。 所以在本节，大家要深刻去学习的是推荐算法的业务流程，以及在具体的业务场景中，如本例的电影推荐，如何实现出推荐算法，并产生推荐结果。 案例–算法实现：User-Based CF 预测评分评分预测公式：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$ 算法实现 实现评分预测方法：predict 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ......def predict(uid, iid, ratings_matrix, user_similar): ''' 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param user_similar: 用户两两相似度矩阵 :return: 预测的评分值 ''' print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."%(uid, iid)) # 1. 找出uid用户的相似用户 similar_users = user_similar[uid].drop([uid]).dropna() # 相似用户筛选规则：正相关的用户 similar_users = similar_users.where(similar_users&gt;0).dropna() if similar_users.empty is True: raise Exception("用户&lt;%d&gt;没有相似的用户" % uid) # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户 ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index) finally_similar_users = similar_users.ix[list(ids)] # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 for sim_uid, similarity in finally_similar_users.iteritems(): # 近邻用户的评分数据 sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna() # 近邻用户对iid物品的评分 sim_user_rating_for_item = sim_user_rated_movies[iid] # 计算分子的值 sum_up += similarity * sim_user_rating_for_item # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up/sum_down print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating)) return round(predict_rating, 2)if __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") # 预测用户1对物品1的评分 predict(1, 1, ratings_matrix, user_similar) # 预测用户1对物品2的评分 predict(1, 2, ratings_matrix, user_similar) 实现预测全部评分方法：predict_all 123456789101112131415161718192021222324252627# ......def predict_all(uid, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 准备要预测的物品的id列表 item_ids = ratings_matrix.columns # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") for i in predict_all(1, ratings_matrix, user_similar): pass 添加过滤规则 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def _predict_all(uid, item_ids, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param item_ids: 要预测的物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingdef predict_all(uid, ratings_matrix, user_similar, filter_rule=None): ''' 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None :return: 生成器，逐个返回预测评分 ''' if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == "unhot": '''过滤非热门电影''' # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count&gt;10).dropna().index elif isinstance(filter_rule, str) and filter_rule == "rated": '''过滤用户评分过的电影''' # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings&lt;6 item_ids = _.where(_==False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]): '''过滤非热门和用户已经评分过的电影''' count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1)&amp;set(ids2) else: raise Exception("无效的过滤参数") yield from _predict_all(uid, item_ids, ratings_matrix, user_similar)if __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") for result in predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]): print(result) 根据预测评分为指定用户进行TOP-N推荐： 123456789101112# ......def top_k_rs_result(k): ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") results = predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]) return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__': from pprint import pprint result = top_k_rs_result(20) pprint(result) 案例–算法实现：Item-Based CF 预测评分评分预测公式：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{j\in I_{rated}}sim(i,j)*r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}$$(与U-B一样) Model-Based 协同过滤算法随着机器学习技术的逐渐发展与完善，推荐系统也逐渐运用机器学习的思想来进行推荐。将机器学习应用到推荐系统中的方案真是不胜枚举。以下对Model-Based CF算法做一个大致的分类： 基于分类算法、回归算法、聚类算法 基于矩阵分解的推荐 基于神经网络算法 基于图模型算法 接下来我们重点学习以下几种应用较多的方案： 基于K最近邻的协同过滤推荐 基于回归模型的协同过滤推荐 基于矩阵分解的协同过滤推荐 基于K最近邻的协同过滤推荐基于K最近邻的协同过滤推荐其实本质上就是MemoryBased CF，只不过在选取近邻的时候，加上K最近邻的限制。 这里我们直接根据MemoryBased CF的代码实现 修改以下地方 123456789101112131415class CollaborativeFiltering(object): based = None def __init__(self, k=40, rules=None, use_cache=False, standard=None): ''' :param k: 取K个最近邻来进行预测 :param rules: 过滤规则，四选一，否则将抛异常："unhot", "rated", ["unhot","rated"], None :param use_cache: 相似度计算结果是否开启缓存 :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化 ''' self.k = 40 self.rules = rules self.use_cache = use_cache self.standard = standard 修改所有的选取近邻的地方的代码，根据相似度来选取K个最近邻 123similar_users = self.similar[uid].drop([uid]).dropna().sort_values(ascending=False)[:self.k]similar_items = self.similar[iid].drop([iid]).dropna().sort_values(ascending=False)[:self.k] 最终代码汇总 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251import osimport pandas as pdimport numpy as npDATA_PATH = "../datasets/ml-latest-small/ratings.csv"CACHE_DIR = "../datasets/cache/"class CollaborativeFiltering(object): based = None def __init__(self, k=40, rules=None, use_cache=False, standard=None): ''' :param k: 取K个最近邻来进行预测 :param rules: 过滤规则，四选一，否则将抛异常："unhot", "rated", ["unhot","rated"], None :param use_cache: 相似度计算结果是否开启缓存 :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化 ''' self.k = 40 self.rules = rules self.use_cache = use_cache self.standard = standard # 加载数据集 def load_data(self, data_path): ''' 加载数据 :param data_path: 数据集路径 :param cache_path: 数据集缓存路径 :return: 用户-物品评分矩阵 ''' # 数据集缓存地址 cache_path = os.path.join(CACHE_DIR, "ratings_matrix.cache") print("开始加载数据集...") if os.path.exists(cache_path): # 判断是否存在缓存文件 print("加载缓存中...") ratings_matrix = pd.read_pickle(cache_path) # 转换成pickle加快操作 print("从缓存加载数据集完毕") else: print("加载新数据中...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) # 读取前三列 # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵 ratings_matrix = ratings.pivot_table(index=["userId"], columns=["movieId"], values="rating") # 存入缓存文件 ratings_matrix.to_pickle(cache_path) print("数据集加载完毕") return ratings_matrix # 计算相似度 def compute_pearson_similarity(self, ratings_matrix, based="user"): ''' 计算皮尔逊相关系数 :param ratings_matrix: 用户-物品评分矩阵 :param based: "user" or "item" :return: 相似度矩阵 ''' user_similarity_cache_path = os.path.join(CACHE_DIR, "user_similarity.cache") item_similarity_cache_path = os.path.join(CACHE_DIR, "item_similarity.cache") # 基于皮尔逊相关系数计算相似度 # 用户相似度 if based == "user": if os.path.exists(user_similarity_cache_path): print("正从缓存加载用户相似度矩阵") similarity = pd.read_pickle(user_similarity_cache_path) else: print("开始计算用户相似度矩阵") similarity = ratings_matrix.T.corr() similarity.to_pickle(user_similarity_cache_path) elif based == "item": if os.path.exists(item_similarity_cache_path): print("正从缓存加载物品相似度矩阵") similarity = pd.read_pickle(item_similarity_cache_path) else: print("开始计算物品相似度矩阵") similarity = ratings_matrix.corr() similarity.to_pickle(item_similarity_cache_path) else: raise Exception("Unhandled 'based' Value: %s" % based) print("相似度矩阵计算/加载完毕") return similarity # 预测指定用户指定商品的评分 def predict(self, uid, iid, ratings_matrix, user_similar): ''' 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param user_similar: 用户两两相似度矩阵 :return: 预测的评分值 ''' print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..." % (uid, iid)) # 1. 找出uid用户的相似用户 similar_users = user_similar[uid].drop([uid]).dropna().sort_values(ascending=False)[:self.k] # 相似用户筛选规则：正相关的用户 similar_users = similar_users.where(similar_users &gt; 0).dropna() if similar_users.empty is True: raise Exception("用户&lt;%d&gt;没有相似的用户" % uid) # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户 # (其中就是筛选出来的用户以下条件:1.是uid的近邻用户 2. 且这些用户对iid有评分) ids = set(ratings_matrix[iid].dropna().index) &amp; set(similar_users.index) finally_similar_users = similar_users.ix[list(ids)] # ids内是索引 # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 # iteritems()与itemgetter()函数作用(https://www.cnblogs.com/SpringFull/p/10168533.html) for sim_uid, similarity in finally_similar_users.iteritems(): # 近邻用户的评分数据 sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna() # 近邻用户对iid物品的评分 sim_user_rating_for_item = sim_user_rated_movies[iid] # 计算分子的值 sum_up += similarity * sim_user_rating_for_item # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up / sum_down print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating)) return round(predict_rating, 2) # 预测指定用户的全部商品评分 # def predict_all(uid, ratings_matrix, user_similar): # ''' # 预测全部评分 # :param uid: 用户id # :param ratings_matrix: 用户-物品打分矩阵 # :param user_similar: 用户两两间的相似度 # :return: 生成器，逐个返回预测评分 # ''' # # 准备要预测的物品的id列表 # item_ids = ratings_matrix.columns # # 逐个预测 # for iid in item_ids: # try: # rating = predict(uid, iid, ratings_matrix, user_similar) # except Exception as e: # print(e) # else: # yield uid, iid, rating # 添加过滤条件 def _predict_all(self, uid, item_ids, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param item_ids: 要预测的物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 逐个预测 for iid in item_ids: try: rating = self.predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, rating # 预测指定用户的全部商品评分 def predict_all(self, uid, ratings_matrix, user_similar, filter_rule=None): ''' 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None :return: 生成器，逐个返回预测评分 ''' if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == "unhot": '''过滤非热门电影''' # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count &gt; 10).dropna().index elif isinstance(filter_rule, str) and filter_rule == "rated": '''过滤用户评分过的电影''' # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings &lt; 6 # 这里的 _ 存的是判断结果(T/F) item_ids = _.where(_ == False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]): '''过滤非热门和用户已经评分过的电影''' count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1) &amp; set(ids2) else: raise Exception("无效的过滤参数") yield from self._predict_all(uid, item_ids, ratings_matrix, user_similar) # 根据预测评分为指定用户进行TOP-N推荐 def top_k_rs_result(self, result, k): return sorted(result, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__': U_I = CollaborativeFiltering(k=5, rules='unhot', use_cache=False, standard=None) # 1.数据集加载 ratings_matrix = U_I.load_data(DATA_PATH) print(ratings_matrix) # 2.相似度计算-皮尔逊相似系数 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # print(user_similar) # item_similar = compute_pearson_similarity(ratings_matrix, based="item") # print(item_similar) # 3. 预测给定用户对给定物品的评分值 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # # 预测用户1对物品1的评分 # predict(1, 1, ratings_matrix, user_similar) # # 预测用户1对物品2的评分 # predict(1, 2, ratings_matrix, user_similar) # 4. 预测全部评分 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # for i in predict_all(1, ratings_matrix, user_similar): # pass # 5. 添加过滤规则后,预测指定用户的全部商品评分 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # # for result in predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]): # print(result) # 6.根据预测评分为指定用户进行TOP-N推荐 from pprint import pprint user_similar = U_I.compute_pearson_similarity(ratings_matrix, based="user") result = U_I.predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]) top_k = U_I.top_k_rs_result(result, 20) pprint(top_k) 但由于我们的原始数据较少，这里我们的KNN方法的效果会比纯粹的MemoryBasedCF要差 基于回归模型的协同过滤推荐如果我们将评分看作是一个连续的值而不是离散的值，那么就可以借助线性回归思想来预测目标用户对某物品的评分。其中一种实现策略被称为Baseline（基准预测）。 Baseline：基准预测Baseline设计思想基于以下的假设： 有些用户的评分普遍高于其他用户，有些用户的评分普遍低于其他用户。比如有些用户天生愿意给别人好评，心慈手软，比较好说话，而有的人就比较苛刻，总是评分不超过3分（5分满分） 一些物品的评分普遍高于其他物品，一些物品的评分普遍低于其他物品。比如一些物品一被生产便决定了它的地位，有的比较受人们欢迎，有的则被人嫌弃。 这个用户或物品普遍高于或低于平均值的差值，我们称为偏置(bias) Baseline目标： 找出每个用户普遍高于或低于他人的偏置值 $b_u$ 找出每件物品普遍高于或低于其他物品的偏置值$b_i$ 我们的目标也就转化为寻找最优的$b_u$和$b_i$ 使用Baseline的算法思想预测评分的步骤如下： 计算所有电影的平均评分$\mu$（即全局平均评分） 计算每个用户评分与平均评分$\mu$的偏置值$b_u$ 计算每部电影所接受的评分与平均评分$\mu$的偏置值$b_i$ 预测用户对电影的评分：$$\hat{r}{ui} = b{ui} = \mu + b_u + b_i$$ 举例： ​ 比如我们想通过Baseline来预测用户A对电影“阿甘正传”的评分，那么首先计算出整个评分数据集的平均评分$\mu$是3.5分；而用户A是一个比较苛刻的用户，他的评分比较严格，普遍比平均评分低0.5分，即用户A的偏置值$b_i$是-0.5；而电影“阿甘正传”是一部比较热门而且备受好评的电影，它的评分普遍比平均评分要高1.2分，那么电影“阿甘正传”的偏置值$b_i$是+1.2，因此就可以预测出用户A对电影“阿甘正传”的评分为：$3.5+(-0.5)+1.2$，也就是4.2分。 对于所有电影的平均评分$\mu$是直接能计算出的，因此问题在于要测出每个用户的$b_u$值和每部电影的$b_i$的值。对于线性回归问题，我们可以利用平方差构建损失函数如下：$$\begin{split}Cost &amp;= \sum_{u,i\in R}(r_{ui}-\hat{r}{ui})^2\&amp;=\sum{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2\end{split}$$ 加入L2正则化：$$Cost=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)$$公式解析： 公式第一部分$ \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2$是用来寻找与已知评分数据拟合最好的$b_u$和$b_i$ 公式第二部分$\lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)$是正则化项，用于避免过拟合现象 对于最小过程的求解，我们一般采用随机梯度下降法或者交替最小二乘法来优化实现。 方法一：随机梯度下降法优化梯度下降知识点补充 使用随机梯度下降优化算法预测Baseline偏置值 step 1：梯度下降法推导 –(要推出来b_u和b_i的表达式)损失函数：$$\begin{split}&amp;J(\theta)=Cost=f(b_u, b_i)\\&amp;J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)\end{split}$$梯度下降参数更新原始公式：$$\theta_j:=\theta_j-\alpha\cfrac{\partial }{\partial \theta_j}J(\theta)$$(对$b_u$和$b_i$求偏导) 梯度下降更新$b_u$: ​ 损失函数偏导推导：$$\begin{split}\cfrac{\partial}{\partial b_u} J(\theta)&amp;=\cfrac{\partial}{\partial b_u} f(b_u, b_i)\&amp;=2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)(-1) + 2\lambda{b_u}\&amp;=-2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambdab_u\end{split}$$​ $b_u$更新(因为alpha可以人为控制，所以2可以省略掉)：$$\begin{split}b_u&amp;:=b_u - \alpha(-\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + \lambda * b_u)\&amp;:=b_u + \alpha(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) - \lambda b_u)\end{split}$$同理可得，梯度下降更新$b_i$:$$b_i:=b_i + \alpha(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) -\lambdab_i)$$ step 2：随机梯度下降由于随机梯度下降法本质上利用每个样本的损失来更新参数，而不用每次求出全部的损失和，因此使用SGD时：(随机梯度下降每次只考虑一个样本，所以不用上面的求和) 单样本损失值：$$\begin{split}error &amp;=r_{ui}-\hat{r}{ui}\&amp;= r{ui}-(\mu+b_u+b_i)\&amp;= r_{ui}-\mu-b_u-b_i\end{split}$$参数更新：$$\begin{split}b_u&amp;:=b_u + \alpha((r_{ui}-\mu-b_u-b_i) -\lambda*b_u) \&amp;:=b_u + \alpha(error - \lambdab_u) \\b_i&amp;:=b_i + \alpha((r_{ui}-\mu-b_u-b_i) -\lambdab_i)\&amp;:=b_i + \alpha(error -\lambda*b_i)\end{split}$$ step 3：算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pdimport numpy as npclass BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): # for是为了把全部数据遍历一遍，上面的for保证了每个数据遍历20次 error = real_rating - (self.global_mean + bu[uid] + bi[iid]) # 这边的bu和bi都是每个用户每个商品都不一样，相当于每个数都计算了20次 bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFBySGD(20, 0.1, 0.1, ["userId", "movieId", "rating"]) bcf.fit(dataset) while True: uid = int(input("uid: ")) iid = int(input("iid: ")) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估 添加test方法，然后使用之前实现accuary方法计算准确性指标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): ''' 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 ''' print("开始切分数据集...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合 for uid in ratings.groupby("userId").any().index: user_rating_data = ratings.where(ratings["userId"]==uid).dropna() if random: # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print("完成数据集切分...") return trainset, testsetdef accuray(predict_results, method="all"): ''' 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: ''' def rmse(predict_results): ''' rmse评估指标 :param predict_results: :return: rmse ''' length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): ''' mae评估指标 :param predict_results: :return: mae ''' length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): ''' rmse和mae评估指标 :param predict_results: :return: rmse, mae ''' length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == "rmse": rmse(predict_results) elif method.lower() == "mae": mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): error = real_rating - (self.global_mean + bu[uid] + bi[iid]) bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): '''评分预测''' if iid not in self.items_ratings.index: raise Exception("无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据".format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': trainset, testset = data_split("datasets/ml-latest-small/ratings.csv", random=True) bcf = BaselineCFBySGD(20, 0.1, 0.1, ["userId", "movieId", "rating"]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print("rmse: ", rmse, "mae: ", mae) 方法二：交替最小二乘法优化使用交替最小二乘法优化算法预测Baseline偏置值 step 1: 交替最小二乘法推导最小二乘法和梯度下降法一样，可以用于求极值。 最小二乘法思想：对损失函数求偏导，然后再使偏导为0 同样，损失函数：$$J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda(\sum_u {b_u}^2 + \sum_i {b_i}^2)$$对损失函数求偏导：$$\cfrac{\partial}{\partial b_u} f(b_u, b_i) =-2 \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda * b_u$$令偏导为0，则可得：$$\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) = \lambda b_u\\sum_{u,i\in R}(r_{ui}-\mu-b_i) = \sum_{u,i\in R} b_u+\lambda * b_u$$为了简化公式，这里令$\sum_{u,i\in R} b_u \approx |R(u)|*b_u$，即直接假设每一项的偏置都相等，可得：$$b_u := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_i)}{\lambda_1 + |R(u)|}$$其中$|R(u)|$表示用户$u$的有过评分数量 同理可得：$$b_i := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_u)}{\lambda_2 + |R(i)|}$$其中$|R(i)|$表示物品$i$收到的评分数量 $b_u$和$b_i$分别属于用户和物品的偏置，因此他们的正则参数可以分别设置两个独立的参数 step 2: 交替最小二乘法应用通过最小二乘推导，我们最终分别得到了$b_u$和$b_i$的表达式，但他们的表达式中却又各自包含对方，因此这里我们将利用一种叫交替最小二乘的方法来计算他们的值： 计算其中一项，先固定其他未知参数，即看作其他未知参数为已知 如求$b_u$时，将$b_i$看作是已知；求$b_i$时，将$b_u$看作是已知；如此反复交替，不断更新二者的值，求得最终的结果。这就是交替最小二乘法（ALS） step 3: 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import pandas as pdimport numpy as npclass BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFByALS(20, 25, 15, ["userId", "movieId", "rating"]) bcf.fit(dataset) while True: uid = int(input("uid: ")) iid = int(input("iid: ")) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): ''' 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 ''' print("开始切分数据集...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合 for uid in ratings.groupby("userId").any().index: user_rating_data = ratings.where(ratings["userId"]==uid).dropna() if random: # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print("完成数据集切分...") return trainset, testsetdef accuray(predict_results, method="all"): ''' 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: ''' def rmse(predict_results): ''' rmse评估指标 :param predict_results: :return: rmse ''' length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): ''' mae评估指标 :param predict_results: :return: mae ''' length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): ''' rmse和mae评估指标 :param predict_results: :return: rmse, mae ''' length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == "rmse": rmse(predict_results) elif method.lower() == "mae": mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): '''评分预测''' if iid not in self.items_ratings.index: raise Exception("无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据".format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': trainset, testset = data_split("datasets/ml-latest-small/ratings.csv", random=True) bcf = BaselineCFByALS(20, 25, 15, ["userId", "movieId", "rating"]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print("rmse: ", rmse, "mae: ", mae) 基于矩阵分解的CF算法矩阵分解发展史Traditional SVD: 通常SVD矩阵分解指的是SVD（奇异值）分解技术，在这我们姑且将其命名为Traditional SVD（传统并经典着）其公式如下： Traditional SVD分解的形式为3个矩阵相乘，中间矩阵为奇异值矩阵。如果想运用SVD分解的话，有一个前提是要求矩阵是稠密的，即矩阵里的元素要非空，否则就不能运用SVD分解。 很显然我们的数据其实绝大多数情况下都是稀疏的，因此如果要使用Traditional SVD，一般的做法是先用均值或者其他统计学方法来填充矩阵，然后再运用Traditional SVD分解降维，但这样做明显对数据的原始性造成一定影响。 FunkSVD（LFM） 刚才提到的Traditional SVD首先需要填充矩阵，然后再进行分解降维，同时存在计算复杂度高的问题，因为要分解成3个矩阵，所以后来提出了Funk SVD的方法，它不在将矩阵分解为3个矩阵，而是分解为2个用户-隐含特征，项目-隐含特征的矩阵，Funk SVD也被称为最原始的LFM模型 借鉴线性回归的思想，通过最小化观察数据的平方来寻求最优的用户和项目的隐含向量表示。同时为了避免过度拟合（Overfitting）观测数据，又提出了带有L2正则项的FunkSVD，上公式： 以上两种最优化函数都可以通过梯度下降或者随机梯度下降法来寻求最优解。 BiasSVD: 在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解： 它基于的假设和Baseline基准预测是一样的，但这里将Baseline的偏置引入到了矩阵分解中 SVD++: 人们后来又提出了改进的BiasSVD，被称为SVD++，该算法是在BiasSVD的基础上添加了用户的隐式反馈信息： 显示反馈指的用户的评分这样的行为，隐式反馈指用户的浏览记录、购买记录、收听记录等。 SVD++是基于这样的假设：在BiasSVD基础上，认为用户对于项目的历史浏览记录、购买记录、收听记录等可以从侧面反映用户的偏好。 基于矩阵分解的CF算法实现（一）：LFMLFM也就是前面提到的Funk SVD矩阵分解 LFM原理解析LFM(latent factor model)隐语义模型核心思想是通过隐含特征联系用户和物品，如下图： P矩阵是User-LF矩阵，即用户和隐含特征矩阵。LF有三个，表示共总有三个隐含特征。 Q矩阵是LF-Item矩阵，即隐含特征和物品的矩阵 R矩阵是User-Item矩阵，有P*Q得来 能处理稀疏评分矩阵 利用矩阵分解技术，将原始User-Item的评分矩阵（稠密/稀疏）分解为P和Q矩阵，然后利用$P*Q$还原出User-Item评分矩阵$R$。整个过程相当于降维处理，其中： 矩阵值$P_{11}$表示用户1对隐含特征1的权重值 矩阵值$Q_{11}$表示隐含特征1在物品1上的权重值 矩阵值$R_{11}$就表示预测的用户1对物品1的评分，且$R_{11}=\vec{P_{1,k}}\cdot \vec{Q_{k,1}}$ 利用LFM预测用户对物品的评分，$k$表示隐含特征数量：$$\begin{split}\hat {r}{ui} &amp;=\vec {p{uk}}\cdot \vec {q_{ik}}\&amp;={\sum_{k=1}}^k p_{uk}q_{ik}\end{split}$$因此最终，我们的目标也就是要求出P矩阵和Q矩阵及其当中的每一个值，然后再对用户-物品的评分进行预测。 损失函数同样对于评分预测我们利用平方差来构建损失函数：$$\begin{split}Cost &amp;= \sum_{u,i\in R} (r_{ui}-\hat{r}{ui})^2\&amp;=\sum{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2\end{split}$$加入L2正则化：$$Cost = \sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)$$对损失函数求偏导：$$\begin{split}\cfrac {\partial}{\partial p_{uk}}Cost &amp;= \cfrac {\partial}{\partial p_{uk}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}\\\cfrac {\partial}{\partial q_{ik}}Cost &amp;= \cfrac {\partial}{\partial q_{ik}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\lambda q_{ik}\end{split}$$ 随机梯度下降法优化梯度下降更新参数$p_{uk}$：$$\begin{split}p_{uk}&amp;:=p_{uk} - \alpha\cfrac {\partial}{\partial p_{uk}}Cost\&amp;:=p_{uk}-\alpha [2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}]\&amp;:=p_{uk}+\alpha [\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda p_{uk}]\end{split}$$ 同理：$$\begin{split}q_{ik}&amp;:=q_{ik} + \alpha[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda q_{ik}]\end{split}$$随机梯度下降： 向量乘法 每一个分量相乘 求和$$\begin{split}&amp;p_{uk}:=p_{uk}+\alpha [(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda_1 p_{uk}]\&amp;q_{ik}:=q_{ik} + \alpha[(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda_2 q_{ik}]\end{split}$$由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\lambda_1$和$\lambda_2$ 算法实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113'''LFM Model'''import pandas as pdimport numpy as np# 评分预测 1-5class LFM(object): def __init__(self, alpha, reg_p, reg_q, number_LatentFactors=10, number_epochs=10, columns=["uid", "iid", "rating"]): self.alpha = alpha # 学习率 self.reg_p = reg_p # P矩阵正则 self.reg_q = reg_q # Q矩阵正则 self.number_LatentFactors = number_LatentFactors # 隐式类别数量 self.number_epochs = number_epochs # 最大迭代次数 self.columns = columns def fit(self, dataset): ''' fit dataset :param dataset: uid, iid, rating :return: ''' self.dataset = pd.DataFrame(dataset) self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] self.globalMean = self.dataset[self.columns[2]].mean() self.P, self.Q = self.sgd() def _init_matrix(self): ''' 初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值--防止×完全是0 :return: ''' # User-LF # random.rand(x)取值范围是 [0,1) P = dict(zip( self.users_ratings.index, np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32) )) # Item-LF Q = dict(zip( self.items_ratings.index, np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32) )) return P, Q def sgd(self): ''' 使用随机梯度下降，优化结果 :return: ''' P, Q = self._init_matrix() for i in range(self.number_epochs): print("iter%d"%i) error_list = [] for uid, iid, r_ui in self.dataset.itertuples(index=False): # User-LF P ## Item-LF Q v_pu = P[uid] #用户向量！！这是一个向量，PQ是矩阵 v_qi = Q[iid] #物品向量 err = np.float32(r_ui - np.dot(v_pu, v_qi)) v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu) v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi) P[uid] = v_pu Q[iid] = v_qi # 或者不从向量的角度考虑 # for k in range(self.number_of_LatentFactors): # v_pu[k] += self.alpha*(err*v_qi[k] - self.reg_p*v_pu[k]) # v_qi[k] += self.alpha*(err*v_pu[k] - self.reg_q*v_qi[k]) error_list.append(err ** 2) print(np.sqrt(np.mean(error_list))) return P, Q def predict(self, uid, iid): # 如果uid或iid不在，我们使用全剧平均分作为预测结果返回 if uid not in self.users_ratings.index or iid not in self.items_ratings.index: return self.globalMean p_u = self.P[uid] q_i = self.Q[iid] return np.dot(p_u, q_i) def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) lfm = LFM(0.02, 0.01, 0.01, 10, 100, ["userId", "movieId", "rating"]) lfm.fit(dataset) while True: uid = input("uid: ") iid = input("iid: ") print(lfm.predict(int(uid), int(iid))) 基于内容的推荐算法（Content-Based）简介基于内容的推荐方法是非常直接的，它以物品的内容描述信息为依据来做出的推荐，本质上是基于对物品和用户自身的特征或属性的直接分析和计算。 例如，假设已知电影A是一部喜剧，而恰巧我们得知某个用户喜欢看喜剧电影，那么我们基于这样的已知信息，就可以将电影A推荐给该用户。 基于内容的推荐实现步骤 画像构建。顾名思义，画像就是刻画物品或用户的特征。本质上就是给用户或物品贴标签。 物品画像：例如给电影《战狼2》贴标签，可以有哪些？ “动作”、”吴京”、”吴刚”、”张翰”、”大陆电影”、”国产”、”爱国”、”军事”等等一系列标签是不是都可以贴上 用户画像：例如已知用户的观影历史是：”《战狼1》”、”《战狼2》”、”《建党伟业》”、”《建军大业》”、”《建国大业》”、”《红海行动》”、”《速度与激情1-8》”等，我们是不是就可以分析出该用户的一些兴趣特征如：”爱国”、”战争”、”赛车”、”动作”、”军事”、”吴京”、”韩三平”等标签。 问题：物品的标签来自哪儿？ PGC 物品画像–冷启动 物品自带的属性（物品一产生就具备的）：如电影的标题、导演、演员、类型等等 服务提供方设定的属性（服务提供方为物品附加的属性）：如短视频话题、微博话题（平台拟定） 其他渠道：如爬虫 UGC 冷启动问题 用户在享受服务过程中提供的物品的属性：如用户评论内容，微博话题（用户拟定） 根据PGC内容构建的物品画像的可以解决物品的冷启动问题 基于内容推荐的算法流程： 根据PGC/UGC内容构建物品画像 根据用户行为记录生成用户画像 根据用户画像从物品中寻找最匹配的TOP-N物品进行推荐 物品冷启动处理： 根据PGC内容构建物品画像 利用物品画像计算物品间两两相似情况 为每个物品产生TOP-N最相似的物品进行相关推荐：如与该商品相似的商品有哪些？与该文章相似文章有哪些？ 基于内容的电影推荐：物品画像物品画像构建步骤： 利用tags.csv中每部电影的标签作为电影的候选关键词 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签 将电影的分类词直接作为每部电影的画像标签 基于TF-IDF的特征提取技术(算一下关键程度)前面提到，物品画像的特征标签主要都是指的如电影的导演、演员、图书的作者、出版社等结构话的数据，也就是他们的特征提取，尤其是体征向量的计算是比较简单的，如直接给作品的分类定义0或者1的状态。 但另外一些特征，比如电影的内容简介、电影的影评、图书的摘要等文本数据，这些被称为非结构化数据，首先他们本应该也属于物品的一个特征标签，但是这样的特征标签进行量化时，也就是计算它的特征向量时是很难去定义的。 因此这时就需要借助一些自然语言处理、信息检索等技术，将如用户的文本评论或其他文本内容信息的非结构化数据进行量化处理，从而实现更加完善的物品画像/用户画像。 TF-IDF算法便是其中一种在自然语言处理领域中应用比较广泛的一种算法。可用来提取目标文档中，并得到关键词用于计算对于目标文档的权重，并将这些权重组合到一起得到特征向量。 算法原理TF-IDF自然语言处理领域中计算文档中词或短语的权值的方法，是词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）的乘积。TF指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被正规化，以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。IDF是一个词语普遍重要性的度量，某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。 TF-IDF算法基于一个这样的假设：若一个词语在目标文档中出现的频率高而在其他文档中出现的频率低，那么这个词语就可以用来区分出目标文档。这个假设需要掌握的有两点： 在本文档出现的频率高； 在其他文档出现的频率低。 因此，TF-IDF算法的计算可以分为词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）两部分，由TF和IDF的乘积来设置文档词语的权重。 TF指的是一个词语在文档中的出现频率。假设文档集包含的文档数为$$N$$，文档集中包含关键词$$k_i$$的文档数为$$n_i$$，$$f_{ij}$$表示关键词$$k_i$$在文档$$d_j$$中出现的次数，$$f_{dj}$$表示文档$$d_j$$中出现的词语总数，$$k_i$$在文档dj中的词频$$TF_{ij}$$定义为：$$TF_{ij}=\frac {f_{ij}}{f_{dj}}$$。并且注意，这个数字通常会被正规化，以防止它偏向长的文件（指同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。 IDF是一个词语普遍重要性的度量。表示某一词语在整个文档集中出现的频率，由它计算的结果取对数得到关键词$$k_i$$的逆文档频率$$IDF_i$$：$$IDF_i=log\frac {N}{n_i}$$ 由TF和IDF计算词语的权重为：$$w_{ij}=TF_{ij}$$·$$IDF_{i}=\frac {f_{ij}}{f_{dj}}$$·$$log\frac {N}{n_i}$$ 结论：TF-IDF与词语在文档中的出现次数成正比，与该词在整个文档集中的出现次数成反比。 用途：在目标文档中，提取关键词(特征标签)的方法就是将该文档所有词语的TF-IDF计算出来并进行对比，取其中TF-IDF值最大的k个数组成目标文档的特征向量用以表示文档。 注意：文档中存在的停用词（Stop Words），如“是”、“的”之类的，对于文档的中心思想表达没有意义的词，在分词时需要先过滤掉再计算其他词语的TF-IDF值。 算法举例对于计算影评的TF-IDF，以电影“加勒比海盗：黑珍珠号的诅咒”为例，假设它总共有1000篇影评，其中一篇影评的总词语数为200，其中出现最频繁的词语为“海盗”、“船长”、“自由”，分别是20、15、10次，并且这3个词在所有影评中被提及的次数分别为1000、500、100，就这3个词语作为关键词的顺序计算如下。 将影评中出现的停用词过滤掉，计算其他词语的词频。以出现最多的三个词为例进行计算如下： “海盗”出现的词频为20/200＝0.1 “船长”出现的词频为15/200=0.075 “自由”出现的词频为10/200=0.05； 计算词语的逆文档频率如下： “海盗”的IDF为：log(1000/1000)=0 “船长”的IDF为：log(1000/500)=0.3“自由”的IDF为：log(1000/100)=1 由1和2计算的结果求出词语的TF-IDF结果，“海盗”为0，“船长”为0.0225，“自由”为0.05。 通过对比可得，该篇影评的关键词排序应为：“自由”、“船长”、“海盗”。把这些词语的TF-IDF值作为它们的权重按照对应的顺序依次排列，就得到这篇影评的特征向量，我们就用这个向量来代表这篇影评，向量中每一个维度的分量大小对应这个属性的重要性。 将总的影评集中所有的影评向量与特定的系数相乘求和，得到这部电影的综合影评向量，与电影的基本属性结合构建视频的物品画像，同理构建用户画像，可采用多种方法计算物品画像和用户画像之间的相似度，为用户做出推荐。 加载数据集1234567891011121314151617181920212223242526272829303132333435363738import pandas as pdimport numpy as np'''- 利用tags.csv中每部电影的标签作为电影的候选关键词- 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签- 并将电影的分类词直接作为每部电影的画像标签'''def get_movie_dataset(): # 加载基于所有电影的标签 # all-tags.csv来自ml-latest数据集中 # 由于ml-latest-small中标签数据太多，因此借助其来扩充 _tags = pd.read_csv("datasets/ml-latest-small/all-tags.csv", usecols=range(1, 3)).dropna() tags = _tags.groupby("movieId").agg(list) # 加载电影列表数据集 movies = pd.read_csv("datasets/ml-latest-small/movies.csv", index_col="movieId") # 将类别词分开 movies["genres"] = movies["genres"].apply(lambda x: x.split("|")) # 为每部电影匹配对应的标签数据，如果没有将会是NAN movies_index = set(movies.index) &amp; set(tags.index) new_tags = tags.loc[list(movies_index)] ret = movies.join(new_tags) # 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段 # 如果电影没有标签数据，那么就替换为空列表 # map(fun,可迭代对象) movie_dataset = pd.DataFrame( map( lambda x: (x[0], x[1], x[2], x[2]+x[3]) if x[3] is not np.nan else (x[0], x[1], x[2], []), ret.itertuples()) , columns=["movieId", "title", "genres","tags"] ) movie_dataset.set_index("movieId", inplace=True) return movie_datasetmovie_dataset = get_movie_dataset()print(movie_dataset) 基于TF·IDF提取TOP-N关键词，构建电影画像1234567891011121314151617181920212223242526272829303132333435363738from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): ''' 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: ''' dataset = movie_dataset["tags"].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) movie_profile = &#123;&#125; for i, mid in enumerate(movie_dataset.index): # 根据每条数据返回，向量 vector = model[corpus[i]] # 按照TF-IDF值得到top-n的关键词 movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] # 根据关键词提取对应的名称--dct[x[0]]是找 在movie_tags中的索引 movie_profile[mid] = dict(map(lambda x:(dct[x[0]], x[1]), movie_tags)) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 完善画像关键词123456789101112131415161718192021222324252627282930313233343536373839404142434445from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): ''' 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: ''' dataset = movie_dataset["tags"].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) _movie_profile = [] for i, data in enumerate(movie_dataset.itertuples()): mid = data[0] title = data[1] genres = data[2] vector = model[corpus[i]] movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] topN_tags_weights = dict(map(lambda x: (dct[x[0]], x[1]), movie_tags)) # 将类别词的添加进去，并设置权重值为1.0 for g in genres: topN_tags_weights[g] = 1.0 topN_tags = [i[0] for i in topN_tags_weights.items()] _movie_profile.append((mid, title, topN_tags, topN_tags_weights)) movie_profile = pd.DataFrame(_movie_profile, columns=["movieId", "title", "profile", "weights"]) movie_profile.set_index("movieId", inplace=True) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 为了根据指定关键词迅速匹配到对应的电影，因此需要对物品画像的标签词，建立倒排索引 倒排索引介绍 通常数据存储数据，都是以物品的ID作为索引，去提取物品的其他信息数据 而倒排索引就是用物品的其他数据作为索引，去提取它们对应的物品的ID列表 123456789101112131415161718# ......'''建立tag-物品的倒排索引'''def create_inverted_table(movie_profile): inverted_table = &#123;&#125; for mid, weights in movie_profile["weights"].iteritems(): for tag, weight in weights.items(): #到inverted_table dict 用tag作为Key去取值 如果取不到就返回[] _ = inverted_table.get(tag, []) _.append((mid, weight)) inverted_table.setdefault(tag, _) return inverted_tableinverted_table = create_inverted_table(movie_profile)pprint(inverted_table) 基于内容的电影推荐：用户画像用户画像构建步骤： 根据用户的评分历史，结合物品画像，将有观影记录的电影的画像标签作为初始标签反打到用户身上 通过对用户观影标签的次数进行统计，计算用户的每个初始标签的权重值，排序后选取TOP-N作为用户最终的画像标签 用户画像建立1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npfrom gensim.models import TfidfModelfrom functools import reduceimport collectionsfrom pprint import pprint# ......'''user profile画像建立：1. 提取用户观看列表2. 根据观看列表和物品画像为用户匹配关键词，并统计词频3. 根据词频排序，最多保留TOP-k个词，这里K设为100，作为用户的标签'''def create_user_profile(): watch_record = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(2), dtype=&#123;"userId":np.int32, "movieId": np.int32&#125;) watch_record = watch_record.groupby("userId").agg(list) # print(watch_record) movie_dataset = get_movie_dataset() movie_profile = create_movie_profile(movie_dataset) user_profile = &#123;&#125; for uid, mids in watch_record.itertuples(): record_movie_prifole = movie_profile.loc[list(mids)] counter = collections.Counter(reduce(lambda x, y: list(x)+list(y), record_movie_prifole["profile"].values)) # 兴趣词 interest_words = counter.most_common(50) maxcount = interest_words[0][1] interest_words = [(w,round(c/maxcount, 4)) for w,c in interest_words] user_profile[uid] = interest_words return user_profileuser_profile = create_user_profile()pprint(user_profile) 基于内容的电影推荐：为用户产生TOP-N推荐结果1234567891011121314151617181920212223242526272829# ......user_profile = create_user_profile()watch_record = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(2),dtype=&#123;"userId": np.int32, "movieId": np.int32&#125;)watch_record = watch_record.groupby("userId").agg(list)for uid, interest_words in user_profile.items(): result_table = &#123;&#125; # 电影id:[0.2,0.5,0.7] for interest_word, interest_weight in interest_words: related_movies = inverted_table[interest_word] for mid, related_weight in related_movies: _ = result_table.get(mid, []) _.append(interest_weight) # 只考虑用户的兴趣程度 # _.append(related_weight) # 只考虑兴趣词与电影的关联程度 # _.append(interest_weight*related_weight) # 二者都考虑 result_table.setdefault(mid, _) # 为什么要加起来是因为一个电影有多个标签 rs_result = map(lambda x: (x[0], sum(x[1])), result_table.items()) rs_result = sorted(rs_result, key=lambda x:x[1], reverse=True)[:100] print(uid) pprint(rs_result) break # 历史数据 ==&gt; 历史兴趣程度 ==&gt; 历史推荐结果 离线推荐 离线计算 # 在线推荐 ===&gt; 娱乐(王思聪) ===&gt; 我 ==&gt; 王思聪 100% # 近线：最近1天、3天、7天 实时计算 基于内容推荐流程总结① 建立物品画像 来源：①用户打tag ②电影的分类值 根据电影的id 把tag和分类值合并起来 求tf-idf 根据tf-idf的结果 为每一部电影筛选出 top-n（tf-idf比较大的）个关键词 电影id-关键词([&quot;profile&quot;])-关键词权重([&quot;weigts&quot;]) ② 建立倒排索引 通过关键词找到电影 遍历 电影id-关键词-关键词权重 数据， 读取每一个关键词，用关键词作为key [(关键词对应的电影id和tf-idf)] 作为value 保存到dict当中 ③ 用户画像 看用户看过那些电影， 到电影的 电影id-关键词-关键词权重 数据中 找到电影所对应的关键词 把用户看过的所有的关键词放到一起 统计词频 每个词出现了几次 出现次数多的关键词 作为用户的兴趣词，这个兴趣词实际上就是用户画像的关键词 ④ 根据用户的兴趣词 找到兴趣词对应的电影 多个兴趣词可能对应一个电影 {电影id：[关键词1权重，关键词2权重]} 把每一个部电影对应的关键词权重求和之后 排序 权重比较高的排在前面 推荐给用户 介绍一下各种命令hadoop/hdfsHDFS shell操作 调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式 ls使用方法：hadoop fs -ls 如果是文件，则按照如下格式返回文件信息：文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：目录名 修改日期 修改时间 权限 用户ID 组ID示例：hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile返回值：成功返回0，失败返回-1。 text使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile从标准输入中读取输入。 返回值： 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html HDFS shell操作练习 在centos 中创建 test.txt 1touch test.txt 在centos中为test.txt 添加文本内容 1vi test.txt 在HDFS中创建 hadoop001/test 文件夹 1hadoop fs -mkdir -p /hadoop001/test 把text.txt文件上传到HDFS中 1hadoop fs -put test.txt /hadoop001/test/ 查看hdfs中 hadoop001/test/test.txt 文件内容 1hadoop fs -cat /hadoop001/test/test.txt 将hdfs中 hadoop001/test/test.txt文件下载到centos 1hadoop fs -get /hadoop001/test/test.txt test.txt 删除HDFS中 hadoop001/test/ hadoop fs -rm -r /hadoop001]]></content>
      <tags>
        <tag>python</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos硬盘扩容到根目录]]></title>
    <url>%2F2020%2F03%2F14%2Fcentos%E7%A1%AC%E7%9B%98%E6%89%A9%E5%AE%B9%E5%88%B0%E6%A0%B9%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[问题描述centos6.4根目录扩容 几个查询磁盘/分区的命令以下均在root用户中进行 df -Th fdisk -l 1.先把根目录的分区变成最后一个如果有交换空间就swapoff /dev/sda3(交换空间) 2.删除所有的分区，并重新创建分区fdisk /dev/sda 按照提示进行操作 3.reboot4.resize2fs /dev/sda2完结！]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用PySpark分析空气质量]]></title>
    <url>%2F2020%2F03%2F12%2F%E4%BD%BF%E7%94%A8pyspark%E5%88%86%E6%9E%90%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%2F</url>
    <content type="text"><![CDATA[项目概述1. 数据来源: http://stateair.net/web/historical/1/1.html2. 根据北京的数据进行统计分析 空气质量指数(PM2.5) 健康建议 0-50 健康 51-100 中等 101-150 对敏感人群不健康 151-200 危险 201-300 非常不健康 301-500 危险 501-~ 报表 3. 数据分析–&gt;es–&gt;kibana先进行数据分析: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pyspark.sql import SparkSessionfrom pyspark.sql.types import *from pyspark.sql.functions import udfdef get_grade(value): if value &lt;= 50 and value &gt;= 0: return "健康" elif value &lt;= 100: return "中等" elif value &lt;= 150: return "对敏感人群不健康" elif value &lt;= 200: return "不健康" elif value &lt;= 300: return "非常不健康" elif value &lt;= 500: return "危险" elif value &gt; 500: return "爆表" else: return Noneif __name__ == '__main__': spark = SparkSession.builder.appName("project").getOrCreate() data2017 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2017_HourlyPM25_created20170803.csv").select("Year","Month","Day","Hour","Value","QC Name") data2016 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2016_HourlyPM25_created20170201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2015 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2015_HourlyPM25_created20160201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2017.show() data2016.show() data2015.show() grade_function_udf = udf(get_grade,StringType()) # 进来一个Value，出去一个Grade group2017 = data2017.withColumn("Grade",grade_function_udf(data2017['Value'])).groupBy("Grade").count() group2016 = data2016.withColumn("Grade",grade_function_udf(data2016['Value'])).groupBy("Grade").count() group2015 = data2015.withColumn("Grade",grade_function_udf(data2015['Value'])).groupBy("Grade").count() group2015.select("Grade", "count", group2015['count'] / data2015.count()).show() group2016.select("Grade", "count", group2016['count'] / data2016.count()).show() group2017.select("Grade", "count", group2017['count'] / data2017.count()).show() group2017.show() group2016.show() group2015.show() spark.stop() 得到: 并在yarn上运行 先在hadoop集群中创建新文件夹 hadoop fs -mkdir -p /data/（若已经有请删除hadoop fs -rmr /data） 把2017、16、15 年的数据上传至hadoop hadoop fs -put Beijing* /data/（参看是否上传成功hadoop fs -ls /data） 把pycharm中的程序改成yarn中的程序 再执行 ./spark-submit --master yarn ~/script/sparky.py 得到和上面一样的结果: es使用举例先es中创建目录 curl -XPUT &#39;http://192.168.211.4:9200/imooc_es&#39; 查看数据 curl -XGET &#39;http://192.168.211.4:9200/_search&#39; 创建一个索引 curl -XPOST &#39;http://hadoop000:9200/imooc_es/student/1&#39; -H &#39;Content-Type: application/json&#39; -d &#39;{ &quot;name&quot;:&quot;imooc&quot;, &quot;age&quot;:5, &quot;interests&quot;:[&quot;Spark&quot;,&quot;Hadoop&quot;] }&#39; es–&gt;kibana(注意这两个启动都在bin目录下)12345678910111213141516171819202122232425262728293031323334353637383940414243444546from pyspark.sql import *from pyspark.sql.functions import *from pyspark.sql.types import *def get_grade(value): if value &lt;= 50: return "健康" elif value &lt;= 100: return "中等" elif value &lt;= 150: return "对敏感人群不健康" elif value &lt;= 200: return "不健康" elif value &lt;= 300: return "非常不健康" elif value &lt;= 500: return "危险" elif value &gt; 500: return "爆表" else: return Noneif __name__ == '__main__': spark = SparkSession.builder.appName("project").getOrCreate() data2017 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2017_HourlyPM25_created20170803.csv").select("Year","Month","Day","Hour","Value","QC Name") data2016 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2016_HourlyPM25_created20170201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2015 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2015_HourlyPM25_created20160201.csv").select("Year","Month","Day","Hour","Value","QC Name") grade_function_udf = udf(get_grade, StringType()) group2017 = data2017.withColumn("Grade", grade_function_udf(data2017['Value'])).groupBy("Grade").count() group2016 = data2016.withColumn("Grade", grade_function_udf(data2016['Value'])).groupBy("Grade").count() group2015 = data2015.withColumn("Grade", grade_function_udf(data2015['Value'])).groupBy("Grade").count() result2017 = group2017.select("Grade", "count").withColumn("precent",group2017['count'] / data2017.count()*100) result2016 = group2016.select("Grade", "count").withColumn("precent",group2016['count'] / data2016.count()*100) result2015 = group2015.select("Grade", "count").withColumn("precent",group2015['count'] / data2015.count()*100)# 将数据写入到es中 result2017.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2017/pm") result2016.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2016/pm") result2015.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2015/pm") spark.stop() 然后再到192.168.211.4:5601里面查看es的数据是否导进来，再利用kibanna进行可视化]]></content>
      <tags>
        <tag>python</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo新电脑的设置]]></title>
    <url>%2F2020%2F03%2F08%2Fhexo%E6%96%B0%E7%94%B5%E8%84%91%E7%9A%84%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装hexo博客必要的软件下载安装Git客户端安装node.js Github添加新电脑生成的密钥打开git bash输入如下命令： 1ssh-keygen -t rsa -C &quot;xxxxx@163.com&quot; 邮箱为GitHub注册邮箱，输入命令后直接回车，生成密钥对。根据提示找到密钥对所在位置，将id_rsa.pub公钥内容复制粘贴到Github-settings-‘SSH and GPG keys’-‘SSH keys’中。使用ssh -T git@github.com测试公钥是否添加成功 安装Hexo在cmd或者创建博客主文件夹，右键git bash内输入下面的命令： 1npm install hexo-cli -g 文件移动进入博客目录，如D:\blog\，Hexo init，这一步有可能在windows上特别慢，看了网上说先退出，再hexo init，结果秒完成，玄学。然后把之前复制的文件全部粘贴过来，选择全部覆盖。这个时候，这个本地环境的文件已经是要发布的文件了，接着安装一些模块插件，否则看不到你的一些功能（比如字数统计，访问量统计等）。 That`s all12$ hexo clean # 如果配置文件没有更改，忽略该命令$ hexo g -d 参考[1] https://fl4g.cn/2018/08/03/Hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%85%B6%E4%BB%96%E7%94%B5%E8%84%91/ [2] https://blog.csdn.net/m0_37286282/article/details/89496837?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task]]></content>
  </entry>
  <entry>
    <title><![CDATA[HCIA-BigData题目]]></title>
    <url>%2F2020%2F01%2F25%2FHCIA-BigData%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[HDFSHDFS： QUESTION 10-m Hadoop 的 HDFS 是一种分布式文件系统，适合以下哪种应用场景的数据存储和管理？ A. 大量小文件存储 B. 高容错、高吞吐量 C. 低延迟读取 D. 流式数据访问 QUESTION 15 华为 FusionInsight HD 系统中关于 HDFS 的 DataNode 的说法正确的是？ A. 不会检查数据的有效性 B. 周期性地将本节点的 Block 相关信息发送给 NameNode C. 不同的 DataNode 存储的 Block 一定是不同的 D. 一个 DataNode 上的 Block 可以是相同的 QUESTION 39HDFS 机制中 NameNode 负责管理元数据， Client 端每次读请求都需要从 NameNode 的元数据磁盘中读取元数据信息，以此获取所读文件在 DataNode 中的位置。A. 对B. 错 QUESTION 47HDFS 的 Client 写入文件时，数据的第一副本写入位置由 NameNode 确定，其他副本的写入位置由 DataNode 确定。A. 对B. 错 QUESTION 53下列哪条 HDFS 命令可用于检测数据块的完整性？A. hdfs fsck /B. hdfs fsck – deleteC. hdfs dfsadmin – reportD. hdfs balancer – threshold 1 QUESTION 65-mHDFS 系统中对备用 NameNode 的作用的描述正确的有？A. 主 NameNode 的热备B. 备 NameNode 对内存没有要求C. 帮助主 NameNode 合并 编辑日志，减少主 NameNode 的启动时间D. 备 NameNode 应与主 NameNode 部署到一个节点 QUESTION 76FusionInsight HD 系统中 HDFS 默认 Block Size 是多少？A. 32MB. 64MC. 128M QUESTION 84-mFusionInsight HD 集群中包含多种服务，每种服务又由若干角色组成，下面哪些是服务的角色？A. HDFSB. NameNodeC. DataNodeD. HBase QUESTION 96HDFS 的 NameNode 节点主备状态管理及元数据文件合并分别由哪两个角色负责?A. ZKFC 和备 NameNodeB. 主 NameNode 和备 NameNodeC. ZKFC 和主 NameNodeD. 主 NameNode 和 JournalNode QUESTION 100Hadoop 系统中关于客户端向 HDFS 文件系统上传文件说法正确的是？A. 客户端的文件数据经过 NameNode 传递给 DataNodeB. 客户端将文件划分为多个 Block,根据 DataNode 的地址信息，按顺序写入每一个DataNode 中C. 客户端根据 DataNode 的地址信息，按顺序将整个文件写入每一个 DataNode 中，然后由 DataNode 将文件划分为多个 BlockD. 客户端只上传数据到一个 DataNode ，然后由 NameNode 负责 Block 复制 QUESTION 106HDFS 支持大文件存储，同时支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。A. 正确B. 错误 QUESTION 117Fusionlnsight HD 系统中 HDFS 的 Block 默认保存几份？A. 3 份B. 2 份C. 1 份D. 不确定 QUESTION 128华为 Fusionlnsight HD 系统中关于 HDFS 的 DataNode 说法正确的是？A. 不会检查数据的有效性B. 周期性的将本节点的 Block 发送给 NameNodeC. 不同的 DataNode 存储的 Block 一定是不同的D. 一个 DataNode 上的的 Block 可以是相同的 QUESTION 151HDFS 的 Client 写入文件时，数据的第一副本写入位置是由 NameNode 确定，其他副本的写入位置由 DataNode 确定A. 正确B. 错误 QUESTION 156Hadoop 中哪个模块负责 HDFS 的数据存储？A. NameNodeB. DataNodeC. ZooKeeperD. JobTraoker QUESTION 186-m安全模式下安装 Fusionlnsight HD 集群时，哪些组件是必须安装的？A. ZookeeperB. LDAPServerC. KrbServerD. HDFS QUESTION 223HDFS 机制中， NameNode 负责管理元数据， Client 端每次请求都需要从 NameNode 的元数据盘中读取元数据信息以此获取所读文件在 DataNode 的位置A. 正确B. 错误 QUESTION 224-m以下关于 Hadoop 分布式文件系统 HDFS 联邦描述正确的有？A. 一个 Namespace 使用一个 block pool 管理数据块B. 一个 Namespace 可使用多个 block pool 管理数据库C. 每个 block pool 的磁盘空间是物理共享的，逻辑空间是隔离的D. 支持 NameNode/NameSpace 水平扩展 QUESTION 259-mFusionlnsight HD 系统中，关于 Solt 索引的存储部署策略，以下说正确的有？（多选）A. 利用 HDFS 数据存储可靠性和易于扩容的特点优先选择索引存储于 HDFSB. 不论 Solt 索引存储在 HDFS 上还是存储在本地磁盘，在同一节点上部署 5 个 Solt 实例，根据 IP 和不同的端口号来区分不同的 Solt 实例C. 当对实时索引录入速度要求较高时，可选择索引存放于本地磁盘D. 当索引数据存放在 HDFS 上时， SolrServer 实例与 DataNode 实例部署在同一节点上。 QUESTION 279Fusionlnsight HD 系统部署时，如果 Solr 索引默认存放在 HDFS 上时，以下理解正确的有？A. 不需要考虑各 SolrServer 实例上创建了多少 SharedB. 为保证数据可靠性，创建索引时必须创建 ReplicaC. 通过 HDFS 读取索引时占用磁盘 IO，因此不建议 Solr 实例与 DataNode 部署在同一节点上D. 当 Solr 服务参考 INDEX_ON_HDFS 值为 HDFS 时，创建 Collection 的索引默认存储在HDFS 上。 QUESTION 284-m以下关于 Hadoop 的 HDFS 描述正确的有？（多选）A. HDFS 由 NameNode， DataNode， Client 组成B. HDFS 备 NameNode 上的元数据是由主 NameNode 同步过去的C. HDFS 采用就近的机架节点进行数据的第一副本存储D. HDFS 适合一次写入，多次读取的读写任务 QUESTION 286客户 IT 系统中 Fusionlnsight HD 集群有 150 个节点，每个节点 12 块磁盘（不包括 OS盘），每块磁盘大小 IT,只安装 HDFS，按照默认配置最大可存储多少数据？A. 1764TBB. 1800TBC. 600TBD. 588TB MapReduceQUESTION 34Hadoop 中 MapReduce 组件擅长处理哪种场景的计算任务？A. 迭代计算B. 离线计算C. 实时交互计算D. 流式计算 QUESTION 35Hadoop 系统中，如果 HDFS 文件系统的备份因子是 3，那么 MapReduce 每次运行 Task 都要从 3 个有副本的机器上传输需要处理的文件段。A. 对B. 错 QUESTION 62Hadoop 系 统 中 YARN 分 配 给 Container 的 内 存 大 小 ， 可 以 通 过 参 数yarn.app.mapreduce.am.resource.mb 来设置A. 对B. 错 QUESTION 80以下哪个不属于 Hadoop 中 MapReduce 组件的特点？A. 易于编程B. 良好的拓展性C. 实时计算D. 高容错性 QUESTION 246Hadoop 中 MapReduce 组件擅长处理哪种场景的计算任务？A. 迭代计算B. 离线计算C. 实时交互计算D. 流式计算 Yarn关于loader的都不是很懂 QUESTION 20-mYARN 服务中，如果要给队列 QuqueA 设置容量为 30%，应该配置哪个参数？A. yarn.scheduler.capacity.root.QueueA.user-limit-factorB. yarn.scheduler.capacity.root.QueueA.minimum-user-limit-percentC. yarn.scheduler.capacity.root.QueueA.capacityD. yarn.scheduler.capacity.root.QueueA.state QUESTION 36YARN 调度器分配资源的顺序，下面哪一个描述是正确的？A. 任意机器 -&gt; 同机架 -&gt; 本地资源B. 任意机器 -&gt; 本地资源 -&gt; 同机架C. 本地资源 -&gt; 同机架 -&gt; 任意机器D. 同机架 -&gt; 任意机器 -&gt; 本地资源 QUESTION 45如果 YARN 集群中只有 Default、 QueueA 和 QueueB 子队列，那么允许将他们的容量分别设置为 60%、 25%、 22%。A. 对B. 错 QUESTION 63-mYARN 通过 ResourceManager 对集群资源进行管理，它的主要功能有？A. 集群资源调度B. 应用程序管理C. 日志管理D. 以上说法都不对 QUESTION 88YARN 上有两个同级队列 Q1 与 Q2，容量都是 50%， Q1 上已经有 10 个任务共占用了 40 的容量， Q2 上有两个任务共占用了 30 的容量，那么由于 Q1 的任务数多，调度器会优先将资源分配给 Q1。A. 对B. 错 QUESTION 91FusionInsight HD 中 Loader 作业提交到 YARN 后，作业不能手动停止。A. 对B. 错 QUESTION 97Hadoop 平台中启用 YARN 组件的日志聚集功能，需要配置哪个参数？A. yarn.nodemanager.local-dirB. yarn.nodemanager.log-dirsC. yarn.acl.enableD. yarn.log-aggregation-enable QUESTION 104YARN 中设置队列 QueueA 最大使用资源量，需要配置哪个参数？A. yarn,scheduler.capacity.root,QueueA.user-limit-factorB. yarn,scheduler.capacity.root,QueueA.minimum-user-limit-fpercentC. yarn,scheduler.capacity.root,QueueA.stateD. yarn,scheduler.capacity.root,QueueA.maximum-capacity QUESTION 124Fusionlnsight HD Loader 作业前后，需要哪些节点与外部数据源通讯？A. loader 服务主节点B. 运行 Yarn 服务作业的节点C. 前面两个都需要D. 前面两个都不需要 QUESTION 143-mFusionlnsight HD LLD 配置规划工具可以生成哪些配置文件？A. 监控告警阈值配置文件B. 集群的安装模板文件C. HDFS 和 YARN 的配置文件D. 执行 Precheck 所需要的配置文件 CheckNodes.Config QUESTION 153Fusionlnsight HD 系统中 ，下列哪个方法不能查看到 Loader 作业执行的结果？A. 通过 Yarn 任务管理查看B. 通过 Loader UI 界面查看C. 通过 Manager 的告警查看D. 通过 NodeManager 查看 QUESTION 169关于 Fusionlnsight HD 中 Loader 作业描述正确的是？A. Loader 将作业提交到 Yarn 执行后，如果此时 Loader 服务异常，则此作业执行失败。B. Loader 将作业提交到 Yarn 执行后，如果某个 Mapper 任务执行失败，能够自动进行重试C. Loader 将作业执行失败后将会产生垃圾数据，需要用户手动清除D. Loader 将一个作业提交至 Yarn 执行后，该作业执行完成前，不能再提交其他作业 QUESTION 254Hadoop 平台中，要查看 YARN 服务中的一个 application 信息，通常需要使用什么命令？A. containerB. applicationatternptC. iarD. application QUESTION 261-mHadoop 系统中， YARN 支持哪些资源类型的管理？（多选）A. 内存B. CPUC. 网络D. 磁盘空间 #Spark QUESTION 9 Spark On Yarn 模式下的 driver 只能运行在客户端。A. 对B. 错 Spark QUESTION 21Spark 和 Hadoop 都不适用于迭代计算的场景。A. 对B. 错 QUESTION 37Spark SQL 表中，经常会存在很多小文件（大小远小于 HDFS 块大小），在这种情况下，Spark 会启动更多的 Task 来处理这些小文件，当 SQL 逻辑中存在 Shuffle 操作时，会大大增加 hash 分桶数，从而严重影响性能。A. 对B. 错 QUESTION 40Spark 应用运行时，如果某个 Task 运行失败则导致整个 app 运行失败。A. 对B. 错 QUESTION 54Spark On YARN 模式下，没有部署 NodeManager 的节点不能启动 executor 执行 TaskA. 对B. 错 QUESTION 79Spark 是基于内存的计算引擎，所有 Spark 程序运行过程中，的数据只能存储在内存中A. 对B. 错 QUESTION 95Spark 应用在运行时， Stage 划分的依据是什么？A. taskB. taskSetC. actionD. shuffle QUESTION 98Spark 任务的一个 Executor 同时可以运行多个 taskA. 对B. 错 QUESTION 101Fusionlnsight Manager 不能够管理哪个对象？A. SparkB. 主机 OSC. YARND. HDFS QUESTION 121Spark 任务的每个 stage 可划分为 job, 划分的标记是 shuffleA. 正确B. 错误 QUESTION 147下列哪些是 Spark 可以提供的功能？A. 分布式内存计算引擎B. 分布式文件系统C. 集群资源的统一调度D. 流处理功能 QUESTION 238Spark 组建中哪个选项不属于 transformation 操作？A. joinB. distinctC. reduceBykeyD. reduce QUESTION 278以下哪些是 Spark 服务的常驻进程？（多选）A. JobHistoryB. JDBCServerC. SparkResourceD. Nodemanager QUESTION 299华为 Fusionlnsight HD 集群中， Spark 服务可以从以下哪些服务读取数据？（多选）A. YARNB. HDFSC. HiveD. HBase HBaseQUESTION 3-mHBase 集群定时执行 Compaction 的目的是什么？A. 减少同一 Region，同一 ColumnFamily 下的文件数目B. 提升数据读取性能C. 减少同一 ColumnFamily 的文件数据D. 减少同一 Region 的文件数目 QUESTION 8FusionInsight HD HBase 默认使用什么作为其底层文件存储系统？A. HDFSB. HadoopC. MemoryD. MapReduce QUESTION 14-mHBase 的主要特点有哪些？A. 高可靠性B. 高性能C. 面向列D. 可伸缩 QUESTION 18下列哪些组件必须依赖于 Zookeeper 才能运行？A. HDFSB. HBaseC. SparkD. YARN QUESTION 24HBase 中 Region 的物理存储单元是什么A. RegionB. ColumnFamilyC. ColumnD. Row QUESTION 32FusionInsight HD 中使用 HBase 进行数据读取服务时需要连接 HMasterA. 对B. 错 QUESTION 33HBase 中一个 Region 进行 Split 操作时，将一个 HFile 文件真正分开到两个 Region 的过程发生在以下什么阶段？A. Split 过程中B. Flush 过程中C. Compaction 过程中D. HFile 分开过程中 QUESTION 43-mHBase 的数据文件 HFile 中一个 KeyValue 格式包含哪些信息？A. KeyB. ValueC. TimestampD. KeyType QUESTION 48HBase 的某张表的 RowKey 划分 SplitKey 为 9， E， a， z，请问该表有几个 Region？A. 3B. 4C. 5D. 6 QUESTION 52FusionInsight HD 平台中， HBase 暂不支持二级索引A. 对B. 错 QUESTION 56下列关于 HBase 的 BloomFilter 特性理解正确的是？A. 用来过滤数据B. 用来优化随机读取的性能C. 会增加存储的消耗D. 可以准确判断某条数据不存在 QUESTION 60Loader 仅支持关系型数据库与 HBase 之间的数据导入导出。A. 对B. 错 QUESTION 71HBase 的最小处理单元是 Region， User Region 和 Region Server 之间的路由信息是保存在哪里的？A. ZookeeperB. HDFSC. MasterD. meta 表 QUESTION 83HBase 元数据 Meta Region 路由信息保存在哪里？A. Root 表B. ZookeeperC. HMasterD. Meta 表 QUESTION 87执行 HBase 数据读取业务时，需要读取哪几部分数据？A. HFileB. HLogC. MemStoreD. HMaster QUESTION 89FusionInsight HD 的 HBase 服务包含哪些进程？A. HMasterB. SlaveC. HRegionServerD. DataNode QUESTION 92HBase 的主 Master 是如何选举的？A. 随机选取B. 由 RegionServer 进行裁决C. 通过 Zookeeper 进行裁决D. HMaster 为双主模式，不需要进行裁 QUESTION 109Fusionlnsight HD HBase 的管理进程是如何选择主节点的？A. 随机选取B. 由 RegionServer 进行裁决C. 通过 ZooKeeper 进行裁决D. HMaster 为双主模式，不需要进行裁决 QUESTION 142Fusionlnsight HD 系统中 HBase 元数据 Meta region 路由信息保存在哪？A. Root 表B. ZooKeeperC. HMasterD. Mata 表 QUESTION 155Fusionlnsight HD 中,如果需要查看当前登录 HBase 的用户和权限组，可以在 HBase shell中执行什么命令？A. use-permissionB. whoamiC. whoD. get-user QUESTION 162Hadoop 的 HBase 不适合哪些数据类型的应用场景？A. 大文件应用场景B. 海量数据应用场景C. 高吞吐率应用场景D. 半结构化数据应用场景 QUESTION 165Hadoop 平台中 HBase 的 Region 是由哪个服务进程来管理？A. HMasterB. DatanodeC. RegionServerD. Zookeeper QUESTION 173-m基于 Hadoop 开源大数据平台主要提供了针对数据分布式计算和存储能力，如下属于分布式存储组件的有？（多选）A. MRB. SparkC. HDFSD. HBase QUESTION 190Fusionlnsight HD 使用 HBase 客户端批量写入 10 条数据，某个 RegionServer 节点上包含该表的 2 个 Region，分别 A 和 B， 10 条数据中有两条属于 A， 4 条属于 B，请问写入这 10 条数据需要向该 RegionServer 发送几次 RPC 请求？A. 1B. 2C. 3D. 4 QUESTION 210Fusionlnsight HD 中使用 HBase 进行数据读写服务时，需要连接 HManager。A. 正确B. 错误 QUESTION 214Fusionlnsight HD 系统中，以下选项哪一个不是 HBase 写数据流程涉及的角色或服务？A. ZookeeperB. HDFSC. HMasterD. RegionServer QUESTION 225关于 HBase 中 HFile 的描述不正确的是？A. 一个 HFile 属于一个 RegionB. 一个 HFile 包含多个列族的数据C. 一个 HFile 包含多列数据D. 一个 HFile 包含多行数据 QUESTION 252Fusionlnsight HD 的 HBase 中会保存一张用户信息表 meg-table,Rowkey 为用户 ID ,其 中 一 列 为 用 户 昵 称 ， 现 在 按 先 后 顺 序 往 这 列 写 入 三 个KeyValue:001:Li,001:Mary,001:Lily，请问 scan’meg-table’,{ERSIONS=&gt;2}会返回哪几条数据？A. 001:LiB. 001:LilyC. 001:Li,001:Mary,001:LilyD. 001:Mary,001:Lily QUESTION 253关于 HBase 的 Region 分裂流程 Split 的描述不正确的是？A. Split 过程中并没有真正的将文件分开，仅仅是创建了引用文件B. Split 为了减少 Region 中数据大小，从而将一个 Region 分裂成两个 RegionC. Split 过程中该表会暂停服务D. Split 过程中分裂的 Region 会暂停服务 QUESTION 255Fusionlnsight HD 系统中执行 HBase 写数据时候，数据被写入内存 Memstore、日志 HLog和 HDP 中，请问哪一步写入成功后才会最终返回客户端写数据成功？A. MemstoreB. HLogC. HDFSD. Menmory QUESTION 265HBase 的物理存储单元是什么？A. RegionB. ColumnFamilyC. ColumnD. ROW QUESTION 270HBase 中数据存储的文件格式是什么？A. FlieB. SequenceFileC. LogD. TXTflie QUESTION 292Fusionlnsight HD 系统中 HBase 支持动态扩展列。A. 正确B. 错误 HiveQUESTION 4-mFusionInsight HD 系统中 Hive 支持的存储格式包括？A. HFileB. TextFileC. SequenceFileD. RCFile QUESTION 6-mFusionInsight Manager 界面显示 Hive 服务状态为 Bad 时，可能原因有哪些？A. DBService 不可用B. HDFS 服务不可用C. MetaStore 实例不可用D. HBase 服务不可用 QUESTION 31在 FusionInsight HD 中，以下哪一项不属于 Hive 的流控特性A. 支持对已建立的总连接数做阈值控制B. 支持对每个用户已经建立的连接数做阈值控制C. 支持对某个特定用户已建立的连接数做阈值控制D. 支持对单位时间内所建立的连接数做阈值控制 QUESTION 50关于 Hive 中普通表和外部表的描述不正确的是？A. 默认创建普通表B. 外部表实质是将已经存在 HDFS 上的文件路径跟表关联起来C. 删除普通表时，元数据和数据同时被删除D. 删除外部表时，只删除外部表数据而不删除元数据 QUESTION 68关于 Hive 建表的基本操作，描述正确的是？A. 创建外部表的时需要指定 external 关键字B. 一旦表创建好，不可再修改表名C. 一旦表创建好，不可再修改列名D. 一旦表创建好，不可再增加新列 QUESTION 137加载数据到 Hive 表，哪种方式不正确？A. 直接将本地路径的文件 load 到 Hive 表中B. 将 HDFS 上的额文件 load 到 Hive 表中C. Hive 支持 insert into 单条记录的方法，所以可以直接在命令行插入单条记录D. 将其他表的结果集 insert into 到 Hive 表中 QUESTION 185Fusionlnsight HD Manager 界面 Hive 日志收集，哪个选项不正确？A. 可指定实例进行日志收集，比如指定单独收集 MetaStore 的日志B. 可指定时间段进行日志收集，比如只收集 2016-1-10 的日志C. 可指定节点 IP 进行日志收集，例如仅下载某个 IP 的日志D. 可指定特定用户进行日志收集，例如仅下载 userA 用户产生的日志 QUESTION 202关于 Hive 与 Hadoop 其他组件的关系，以下描述错误的是？A. Hive 最终将数据存储在 HDFS 中B. Hive SQL 其本质是执行 MapReduce 任务C. Hive 是 Hadoop 平台的数据仓库工具D. Hive 对 HBase 有强依赖 QUESTION 297关于 Hive 在 Fusionlnsight HD 中的架构描述错误的是？A. 只要有一个 HiveServer 不可用，整个 Hive 集群便不可用B. Hiveserver 负责接受客户端请求、解析、执行 HQL 命令并返回查询结果C. MetaStore 用于提供数据服务，依赖于 DBServerD. 在同一个时间点 Hiveserve 只有一个处于 Active 状态，另一个则处于 Standby 状态 StreamingQUESTION 2FusionInsight HD 的 Streaming 对于 Zookeeper 弱依赖，即使 Zookeeper 故障 Streaming可以正常提供服务。A. 对B. 错 QUESTION 28关于 FusionInsight HD Streaming 的 Supervisor 描述正确的是？A. Supervisor 负责资源的分配和任务的调度B. Supervisor 负责接受 Nimbus 分配的任务，启动停止属于自己管理的 Worker 进程C. Supervisor 是运行具体处理逻辑的进程D. Supervisor 是在 Topology 中接收数据然后执行处理的组件 QUESTION 74-mFusionInsight HD 系统中使用 Streaming 客户端 Shell 命令查看拓扑或提交拓扑失败，以下哪些定位手段是正确的？A. 查看客户端异常堆栈，判断是否客户端使用问题B. 查看主 Nimbus 的运行日志，判断是否 Nimbus 服务端异常C. 查看 Supervisor 运行日志，判断是否 Supervisor 异常D. 查看 Worker 运行日志 QUESTION 207-mFusionlnsight HD 系统中 使用 Streaming 客户端 Shell 命令提交了拓扑之后，使用Storm UI 查看发现该拓扑长时间没有处理数据，可能原因有？A. 拓扑结构过于复杂或者并发太大，导致 worker 启动时间过长，超过 supervisor 的等待时间B. supervisor 的 slots 资源被耗尽，拓扑提交上去后分不到 slot 去启动 worker 进程C. 拓扑业务存在扩及错误，提交之后无法正常运行D. 当数据量较大时，拓扑处理速度较慢 QUESTION 263安装 Fusionlnsight HD 的 Streaming 组件时， Nimbus 角色要求安装几个节点？A. 1B. 2C. 3D. 4]]></content>
      <tags>
        <tag>HCIA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop各组件URL]]></title>
    <url>%2F2020%2F01%2F06%2Fhadoop%E5%90%84%E7%BB%84%E4%BB%B6URL%2F</url>
    <content type="text"><![CDATA[1、HDFS页面：50070 2、YARN的管理界面：8088 3、HistoryServer的管理界面：19888 4、Zookeeper的服务端口号：2181 5、Mysql的服务端口号：3306 6、Hive.server1=10000 7、Kafka的服务端口号：9092 8、azkaban界面：8443 9、Hbase界面：16010,60010 10、Spark的界面：8080 11、Spark的URL：7077]]></content>
  </entry>
  <entry>
    <title><![CDATA[leetcode刷题]]></title>
    <url>%2F2020%2F01%2F01%2Fleetcode%E5%88%B7%E9%A2%98%2F</url>
    <content type="text"><![CDATA[leetcode理论知识学习笔记collections模块https://blog.csdn.net/weixin_41644993/article/details/96498297 栈压栈 push()出栈 pop()返回栈顶元素 peek() 123456789101112131415161718192021class Stack(object): def __init__(self): self.__li = [] def push(self, item): self.__li.append(item) def pop(self): return self.__li.pop(-1) def peek(self): if self.__li: return self.__li[-1] else: return None def is_e(self): return self.__li == [] def size(self): return len(self.__li) 迷宫问题栈空的时候是迷宫走完了(此时没走出去) 可以通过在列表或者元祖前加*,来展开列表中的元素 队列排序和刹闸坡(对比)[https://images2015.cnblogs.com/blog/975503/201702/975503-20170214211234550-1109833343.png] 冒泡排序 O(n^2) 12345678910111213def bubble_sort(li): # 冒泡排序，两两进行比较 for j in range(len(li)): # 0 ~ n-1 for i in range(j): # 0 ~ j-1 if li[i] &gt; li[i+1]: li[i], li[i+1] = li[i+1], li[i] return lia = [2, 5, 1, 6]print(bubble_sort(a)) 选择排序 O(n^2) (主要是分成两部分，前面那部分是排好序的，后面那部分是未排序的) 1234567891011121314151617def select_sort(li): # 选择排序是找到后j个里面最小的一个，并把它放在li[]上 for j in range(len(li)): # 0 ~ n-1 min = li[j] min_index = j for i in range(j, len(li)): # 0 ~ j-1 if li[i] &lt; min: min = li[i] min_index = i li[j], li[min_index] = li[min_index], li[j] return lia = [2, 5, 1, 6]print(select_sort(a)) 插入排序 O(n^2) 1234567891011121314def insert_sort(li): # 思路: 每次对第i个元素之前的进行排序 for i in range(1, len(li)): while i &gt; 0: if li[i] &lt; li[i-1]: li[i], li[i-1] = li[i-1], li[i] i -= 1 else: break return lia = [2, 5, 1, 6, 2]print(insert_sort(a)) 希尔排序 123456789101112131415161718def shell_sort(li): # 思路: 本质上是在插入排序的基础上进行的改进 # 主要是把原来的数组按gap划分 n = len(li) gap = n // 2 while gap &gt; 0: for i in range(gap, n): while i &gt; 0: if li[i] &lt; li[i-gap]: li[i], li[i-gap] = li[i-gap], li[i] i -= 1 else: break gap = gap//2 return lia = [2, 5, 1, 6, 2]print(shell_sort(a)) 快速排序 123456789101112131415161718192021222324def quick_sort(li, start, final): if start &gt;= final: return low = start high = final minvalue = li[low] while low &lt; high: # 如果high处的值大于 minvalue high的值减一，然后继续比较 while low &lt; high and minvalue &lt;= li[high]: high -= 1 # 直到比较到high处的值小，这是把high处的值交换到low处 li[low] = li[high] while low &lt; high and li[low] &lt; minvalue: low += 1 li[high] = li[low] li[low] = minvalue quick_sort(li, start, low-1) quick_sort(li, low+1, final)a = [2, 5, 1, 6,7,7,7,7,9]quick_sort(a, 0, 3)print(a) 归并排序 123456789101112131415161718192021222324252627def Merge_Sort(li): n = len(li) if n &lt;= 1: return li # 这是其内部只有一个元素 m = n//2 left_li = Merge_Sort(li[:m]) right_li = Merge_Sort(li[m:]) # 先写当分成每个数组都是一个元素的时候： left_pointer, right_poiter = 0, 0 result = [] while left_pointer &lt; len(left_li) and right_poiter &lt; len(right_li): if left_li[left_pointer] &lt;= right_li[right_poiter]: result.append(left_li[left_pointer]) left_pointer += 1 else: result.append(right_li[right_poiter]) right_poiter += 1 result += left_li[left_pointer:] result += right_li[right_poiter:] return resulta = [2, 5, 1, 6, 7, 7, 7, 7, 9]b = Merge_Sort(a)print(a)print(b) 查找二分查找两个条件：1. 有序 2.列表(顺序表) 计算：头跟尾坐标相加除二 树注意一点 bool([])是False; bool[None]是True。 因为这本质上是对列表进行判断。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class Node(object): def __init__(self, item): self.elem = item self.lchild = None self.rchild = Noneclass Tree(object): def __init__(self): self.root = None def add(self, item): node = Node(item) if self.root is None: self.root = node return queue = [] queue.append(self.root) while queue: cur_node = queue.pop(0) # 弹出来的是一个node？ if cur_node.lchild is None: cur_node.lchild = node return else: queue.append(cur_node.lchild) if cur_node.rchild is None: cur_node.rchild = node return else: queue.append(cur_node.rchild) def B_travel(self): if self.root is None: return queue = [] queue.append(self.root) while queue: cur_node = queue.pop(0) print(cur_node.elem, end=' ') # 其实每一个元素都是一个节点，直接在队列里把他们都显示出来 if cur_node.lchild is not None: queue.append(cur_node.lchild) if cur_node.rchild is not None: queue.append(cur_node.rchild) def before_order(self, root): if root is None: return node = root print(node.elem, end=' ') self.before_order(node.lchild) self.before_order(node.rchild) def in_order(self, root): if root is None: return node = root self.in_order(node.lchild) print(node.elem, end=' ') self.in_order(node.rchild) def pos_order(self, root): if root is None: return node = root self.pos_order(node.lchild) self.pos_order(node.rchild) print(node.elem, end=' ')tree = Tree()tree.add(0)tree.add(1)tree.add(2)tree.add(3)tree.add(4)tree.add(5)tree.add(6)tree.add(7)tree.add(8)tree.add(9)tree.B_travel()print('\n')tree.before_order(tree.root)print('')tree.in_order(tree.root)print('')tree.pos_order(tree.root) 图DAG：有向无圈图 图论中BFS、DFS[Python] BFS和DFS算法（第1讲）、DijkstraBFS（一层一层的来） BFS的算法思想： 1.选择一个根 ，并放入队列 2.将队列位于head的 节点拿出来，并把与目前head节点相连的节点放到队列中 3.重复2 python实现： 1234567891011121314151617181920212223graph1 = &#123; 'A': ['B', 'C'], 'B': ['A', 'C', 'D'], 'C': ['A', 'B', 'D'], 'D': ['B', 'C', 'E'], 'E': ['C', 'D'], 'F': ['D']&#125;def BFS(graph, s): queue = [s] # 1.把根进入队列 seen = set() seen.add(s) while len(queue) &gt; 0: head = queue.pop(0) # 2. head pop出来 nodes = graph[head] # 并把与head相连的节点遍历出来 for w in nodes: if w not in seen: queue.append(w) # 放入队列中 seen.add(w) print(head)BFS(graph1, 'A') 对BFS进行扩展，如何找到路径 12345678910111213141516171819202122232425262728293031graph1 = &#123; 'A': ['B', 'C'], 'B': ['A', 'C', 'D'], 'C': ['A', 'B', 'D'], 'D': ['B', 'C', 'E'], 'E': ['C', 'D'], 'F': ['D']&#125;def BFS(graph, s): queue = [s] # 1.把根进入队列 seen = set() seen.add(s) parent = &#123;s: None&#125; # 创建一个父亲节点 while len(queue) &gt; 0: head = queue.pop(0) # 2. head pop出来 nodes = graph[head] # 并把与head相连的节点遍历出来 for w in nodes: if w not in seen: queue.append(w) # 放入队列中 seen.add(w) parent[w] = head # 记录节点的父亲节点(父亲节点是head，w是与父亲节点相连接的点) # print(head) return parentparent1 = BFS(graph1, 'E')v = 'B' # 从B开始print(parent1)while v is not None: # 只要v不是空，就一直去打印它的父亲节点 print(v) v = parent1[v] DFS（一条路走到黑）（回溯法）DFS的算法思想： 1.选择一个根 ，并放入堆栈中 2.将堆栈位于top的 节点拿出来，并把与目前top节点相连的节点压栈 3.重复2 python实现： 123456789101112def DFS(graph, s): stack = [s] seen = set() seen.add(s) while len(stack) &gt; 0: top = stack.pop() # 2. top(栈顶) pop出来 nodes = graph[top] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(top) Dijkstra 讲解利用priority queue (优先队列) 思路：1.先确定一个起点，并初始化一个parent字典用以记录父节点 ​ 2.把与起点(或pop出的点)相连接的节点放入priority queue中，放入后会自动根据距离排序(此时节点中要包含与起点的距离) ​ 3.在priority queue中pop出距离最短的节点(其余节点均保留在队列之中)（若pop出的节点已经输出则舍去不用），并记录父节点 ​ 4.重复2 3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import heapqimport mathgraph1 = &#123; 'A': &#123;'B': 5, 'C': 1&#125;, 'B': &#123;'A': 5, 'C': 2, 'D': 1&#125;, 'C': &#123;'A': 1, 'B': 2, 'D': 4, 'E': 8&#125;, 'D': &#123;'B': 1, 'C': 4, 'E': 3, 'F': 6&#125;, 'E': &#123;'C': 8, 'D': 3&#125;, 'F': &#123;'D': 6&#125;&#125;# 初始化distance 把除了根节点以外的节点到根节点的距离设置为无穷def init_distnace(graph, s): distance = &#123;s: 0&#125; for node in graph: if node != s: distance[node] = math.inf return distancedef dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue, (0, s)) # 1.把根进入优先队列 seen = set() parent = &#123;s: None&#125; distance = init_distnace(graph, s) while len(pqueue) &gt; 0: # 2. head pop出来(注意pop出来的东西是一对的) head_pair = heapq.heappop(pqueue) dist = head_pair[0] head = head_pair[1] seen.add(head) # 这里的seen与bfs中的位置不同，这里只有当点被弹出的时候才能认为这个点被看到 # 并把与head相连的节点遍历出来(要注意此时是.keys()因为graph里面的值此时也是一个字典) nodes = graph[head].keys() for w in nodes: if w not in seen: if dist + graph[head][w] &lt; distance[w]: # distance存的是根到w的最短距离 graph[head][w]为两点之间的距离 heapq.heappush(pqueue, (dist + graph[head][w], w)) # 下面两组数据即为要输出的结果 parent[w] = head distance[w] = dist + graph[head][w] return parent, distanceparent1, distance1 = dijkstra(graph1, 'A')print(parent1)print(distance1) 有关return的一个小陷阱https://blog.csdn.net/csdniter/article/details/90694394 由于return返回值会传递给上一层函数，而上一层函数没有return命令，故会返回None值给最外层，所以结果是None找到原因之后，再加一个return命令即可 测试函数编写链表创建和遍历 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = Nonedef BuildListNode(numbers): # convert that list into linked list dummyRoot = ListNode(0) ptr = dummyRoot for number in numbers: ptr.next = ListNode(number) ptr = ptr.next ptr = dummyRoot.next return ptrdef TrvalListNode(node): if not node: return "[]" result = "" while node: result += str(node.val) + ", " node = node.next return "[" + result[:-2] + "]"if __name__ == '__main__': numbers = [1,2,3,4] s = BuildListNode(numbers) print(TrvalListNode(s)) 使用举例 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Nonefrom ListNodeBuild import *class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: res = [] while l1: res.append(l1.val) l1 = l1.next while l2: res.append(l2.val) l2 = l2.next res.sort() root = ListNode(0) ptr = root for number in res: ptr.next = ListNode(number) ptr = ptr.next ptr = root.next return ptrif __name__ == '__main__': l1 = BuildListNode([1,2,4]) l2 = BuildListNode([1,3,4]) s = Solution().mergeTwoLists(l1,l2) print(TrvalListNode(s)) 二叉树树的创建和遍历 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Definition for a binary tree node.class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None# 创建树def build(data): if not data: return None root = TreeNode(data) nodeQueue = [root] front = 0 index = 1 while index &lt; len(data): node = nodeQueue[front] front = front + 1 item = data[index] index = index + 1 if item: leftNumber = item node.left = TreeNode(leftNumber) nodeQueue.append(node.left) if index &gt;= len(data): break item = data[index] index = index + 1 if item: rightNumber = item node.right = TreeNode(rightNumber) nodeQueue.append(node.right) return rootdef travelTree(root): if not root: return "[]" output = "" queue = [root] current = 0 while current != len(queue): node = queue[current] current = current + 1 if not node: output += "null, " continue output += str(node.val) + ", " queue.append(node.left) queue.append(node.right) return "[" + output[:-2] + "]" 使用举例 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Nonefrom typing import Listfrom BuildTree import *class Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if len(preorder) == 0: return None # 先把根找到 (注意根root是个节点不是一个数) root = TreeNode(preorder[0]) mid = inorder.index(preorder[0]) root.left = self.buildTree(preorder[1:mid+1], inorder[:mid]) root.right = self.buildTree(preorder[mid+1:], inorder[mid+1:]) return rootif __name__ == "__main__": preo = [3,9,20,15,7] ino = [9,3,15,20,7] root = Solution().buildTree(preo, ino) s = travelTree(root) print(s) 列表 sort() 无返回值 l=list(s) 把s转换成列表9.回文数 20-1-4 12345678910111213141516class Solution: def isPalindrome(self, x: int) -&gt; bool: if x &gt;=0 : num = 0 a = abs(x) while(a != 0): temp = a % 10 #保存余数 a = a//10 num = num * 10 + temp if num == x: return True else: return False else: return False 总结：与上一题类似，需要注意在a除10的时候是整除，即a = int(a/10)ora = a // 10 11.盛最多水的容器123456789101112131415class Solution: def maxArea(self, height: List[int]) -&gt; int: left = 0 right = len(height) - 1 result = 0 while left &lt; right: area = (right - left) * min(height[left],height[right]) if result &lt; area: result = area if height[left] &gt; height[right]: right -= 1 else: left += 1 return result 总结：1.min()可以找到最小值； ​ 2.一开始就已经把指针定义在两端，如果短指针不动，而把长指针向着另一端移动，两者的距离已经变小了，无论会不会遇到更高的指针，结果都只是以短的指针来进行计算。 故移动长指针是无意义的。 22. 括号生成123456789101112131415161718192021class Solution: def generateParenthesis(self, n: int) -&gt; List[str]: if n == 0: return [] result = [] self.helpler(n, n, '', result) return result def helpler(self, l, r, item, result1): # l: 左边剩余括号数量 # r: 右边剩余括号数量 # item: 现在输出的结果(不一定是对的) # result1: 最后要return的结果 if l &gt; r: return if l == 0 and r == 0: result1.append(item) if l &gt; 0: self.helpler(l-1, r, item + '(', result1) if r &gt; 0: self.helpler(l, r-1, item + ')', result1) 总结: 1.搞懂实例中的变量和函数 注意self.的用法 26. 删除排序数组中的重复项123456789class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: j=len(nums) k=list(set(nums)) k.sort() for i in k: nums.append(i) del nums[0:j] return len(nums) 总结: 1.为了避开原地 用了把新的加进去，再删除前面的j个； 2.注意set(nums)是把nums这个数组变成一个无序的集合，list()又将其转化为数组； 3.k.sort()可以对数组进行排序。 122. 买卖股票的最佳时机 II123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class Solution: def maxProfit(self, prices: List[int]) -&gt; int: profit = 0 for i in range(len(prices)-1): if prices[i] &lt; prices[i+1]: profit += (prices[i+1] - prices[i]) return profit``` 总结: 一句话，涨就买，跌就不管，把所有涨价的日子都加进来！## 189. 旋转数组```pythonclass Solution: def rotate(self, nums: List[int], k: int) -&gt; None: """ Do not return anything, modify nums in-place instead. """ j = len(nums) if k &gt; j: for i in range(j-k%j): nums.append(nums[i]) del nums[0:j-k%j] else: for i in range(j-k): nums.append(nums[i]) del nums[0:j-k]``` ## 217. 存在重复元素```pythonclass Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: nums2 = set(nums) if len(nums) == len(nums2): return False else: return True``` ---# 字符串1. 字符串替换 `s = s.replace`(旧, 新, 个数)2. 只保留字符串中的字母和数字，并且变成小写 `s_n = [*filter(str.isalnum, s.lower())]` `filter(function, iterable)`## 3.无重复字符的最长子串```pythonclass Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: dist = &#123;&#125; start = -1 max = 0 #先做一个遍历 for i in range(len(s)): # range（0， 5） 是[0, 1, 2, 3, 4]没有5 #分两种情况 #1.要判断的字符已经在字典中，and是为了保证是在新的子串中，和之前计算过的子串区分开 if s[i] in dist and dist[s[i]] &gt; start: start = dist[s[i]] # 主要是后面i-start 其中的start是代表的重复的那个字母之前的字母 dist[s[i]] = i #2.要判断的字符未在字典中,如果未在判断是否需要更新max else: if i-start &gt; max: max = i - start dist[s[i]] = i return max 总结：1.start要设置成-1，作为只有一个值的时候用i-start（0-1） ​ 2.start = dist[s[i]] 保证star是从前一个值开始，就像-1一样 7.整数反转方法一： 12345678910111213141516class Solution: def reverse(self, x: int) -&gt; int: num = 0 x_abs = abs(x) while(x_abs != 0): tmp = x_abs % 10 num = tmp + num*10 x_abs = int(x_abs/10) if x &gt; 0 and num &lt;= 2**31-1: return num elif x &lt; 0 and num &lt;= 2**31: return -num else: return 0 方法二： 12345678910class Solution: def reverse(self, x: int) -&gt; int: if x &lt; 0: x = -1*x x = int("-" + str(x)[::-1]) else: x = int(str(x)[::-1]) if x &gt; 2**31-1 or x &lt; -2**31: return 0 return x 总结：1.方法一中注意是 x_abs = int(x_abs/10)否则x_abs的数值将一直保留小数，或者改成x_abs//10 ​ 2.方法二中是把输入结果转换成字符串，再通过str(x)[::-1]就可以直接把字符串反向输入 ​ 3. ** 代表平方 ## 123456789101112class Solution: def isPalindrome(self, s: str) -&gt; bool: s_n = [*filter(str.isalnum, s.lower())] n = len(s_n) i=0 while i &lt; n-i-1: if s_n[i] == s_n[n-i-1]: i += 1 continue else: return False return True 242. 有效的字母异位词方法一: 123456789class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: sl, tl = list(s), list(t) sl.sort() tl.sort() if sl == tl: return True else: return False 方法二: 1234567891011class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: k = len(t) if k != len(s): return False for i in range(k): if s[i] not in t: return False else: t = t.replace(s[i], '', 1) return True 344. 反转字符串12345678910111213141516171819202122232425262728class Solution: def reverseString(self, s: List[str]) -&gt; None: """ Do not return anything, modify s in-place instead. """ if len(s)==0 or len(s)==1: return s j = len(s)-1 i = 0 while i&lt;j: tmp = s[i] s[i] = s[j] s[j] = tmp j -= 1 i += 1 return s``` 总结: 利用双字符串，设置中间量。## 387. 字符串中的第一个唯一字符```pythonclass Solution: def firstUniqChar(self, s: str) -&gt; int: for index, c in enumerate(s): if c not in s[:index] and c not in s[index+1:]: return index return -1 38. 外观数列1234567891011121314151617181920212223242526272829class Solution: def countAndSay(self, n: int) -&gt; str: # 设置一个函数来每次计算新的seq if n == 1: return '1' seq = '1' for i in range(n-1): seq = self.helper(seq) return seq def helper(self, seq): count = 1 i = 0 res = '' # 注意下面两个循环的设置，第一个while保证了可以循环的最后一位 # 第二个while保证了循环不会出界(两个判断条件不可调换) while i &lt; len(seq): count = 1 while i &lt; len(seq)-1 and seq[i] == seq[i+1]: count += 1 i += 1 res += str(count) + seq[i] i += 1 return resif __name__ == '__main__': s = 6 print(Solution().countAndSay(s)) 字典1.两个数的和12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dist=&#123;&#125; for i in range(len(nums)): if (target - nums[i]) not in dist: #对健判断 dist[nums[i]] = i # num[i]是键 i是值 else: return [dist[target - nums[i]],i] #返回的是键的=值 总结：Python 字典 in 操作符用于判断键是否存在于字典中 20. 有效的括号12345678910111213class Solution: def isValid(self, s: str) -&gt; bool: stack = [] lookup = &#123;'(': ')', '[': ']', '&#123;': '&#125;'&#125; for ss in s: if ss in lookup: stack.append(ss) elif len(stack) == 0 or ss != lookup[stack.pop()]: return False return len(stack) == 0 总结： 1. 最后的return 要考虑到{ （此时最后一个return为Falsee） 17.电话号码的字母组合方法一： 123456789101112131415161718class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: if digits == '': return [] d = &#123;'2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz'&#125; result = [''] for digit in digits: # 先遍历出来输入的所有字符串 tmp_list = [] for ch in d[digit]: # 再把第一个字符串内的字符遍历出来 for ch2 in result: # 将上面遍历出来的字符串和result内的字符串进行串联 tmp_list.append(ch2 + ch) result = tmp_list return result 总结: 1.此方法需要三个循环遍历，需要注意的是 把新遍历出的字符串与之前存在result里的字符串进行串联 方法二： 12345678910111213141516171819class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: phone = &#123;'2': ['a', 'b', 'c'], '3': ['d', 'e', 'f'], '4': ['g', 'h', 'i'], '5': ['j', 'k', 'l'], '6': ['m', 'n', 'o'], '7': ['p', 'q', 'r', 's'], '8': ['t', 'u', 'v'], '9': ['w', 'x', 'y', 'z']&#125; res = [] def dfs(combination, index = 0): if len(digits) == index: res.append(combination) return for letter in phone[digits[index]]: dfs(combination + letter, index + 1) if digits: dfs('', 0) return res 总结：1.回溯是一种通过穷举所有可能情况来找到所有解的算法。如果一个候选解最后被发现并不是可行解，回溯算法会舍弃它，并在前面的一些步骤做出一些修改，并重新尝试找到可行解; ​ 2.digits[index]的意思的digits这个字符串的第几个字符; ​ 3.特别注意这里的return的位置(详细解释见前面，不过这里的return是没有返回值的，不会出现return None的结果)：return的作用是将函数结果返回，即退出def函数模块。 12.整数转罗马数字123456789101112131415161718class Solution: def intToRoman(self, num: int) -&gt; str: # values = [1000, 900, 500, 400, 100...] # numberals = ['M', 'CM', 'D', 'CD', 'C'...] # 1994 &gt;? 1000 =&gt; M # 994 &gt;? 1000 X # 994 &gt;? 900 =&gt; MCM # 94 &gt;? 900 X # 94 &gt;? 500 X .... values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1] numberals = ['M', 'CM', 'D', 'CD','C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I'] result = '' for i in range(0, len(values)): while num &gt;= values[i]: num -= values[i] result += numberals[i] return result 1234567891011121314151617181920212223242526272829class Solution: def intToRoman(self, num: int) -&gt; str: # values = [1000, 900, 500, 400, 100...] # numberals = ['M', 'CM', 'D', 'CD', 'C'...] # 1994 &gt;? 1000 =&gt; M # 994 &gt;? 1000 X # 994 &gt;? 900 =&gt; MCM # 94 &gt;? 900 X # 94 &gt;? 500 X .... num_dict=&#123;1:'I', 4:'IV', 5:'V', 9:'IX', 10:'X', 40:'XL', 50:'L', 90:'XC', 100:'C', 400:'CD', 500:'D', 900:'CM', 1000:'M' &#125; result = '' for i in sorted(num_dict.keys())[::-1]: while num &gt;= i: num -= i result += num_dict[i] return result 总结：1.尽可能多列出所有可能性，然后从大向减 ​ 2.若要使用字典应该注意，字典是没有索引的，先使用sorted(num_dict.keys())[::-1]排好序，再遍历其key， 这时候i就是key num_dict[i就是其值 ｛i : ‘num_dict[i]’｝ 13.罗马数字转整数123456789101112131415161718class Solution: def romanToInt(self, s: str) -&gt; int: numberal_map = &#123;'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000&#125; result = 0 for i in range(len(s)): if i == 0: result += numberal_map[s[i]] elif i &gt; 0 and numberal_map[s[i]] &lt;= numberal_map[s[i-1]]: result += numberal_map[s[i]] else: result += numberal_map[s[i]] - 2* numberal_map[s[i-1]] return result 总结：1.当numberal_map[s[i]] &gt; numberal_map[s[i-1]]时，此时result内已经加上了i-1时的值，所以这时候要减去2倍的i-1的值;(eg:IV 在第一个字符传进来的时候result内值为I，再第二个值传进来的时候本来是I+V，但是我们需要的是V-I所以要减去二倍的之前传过来的值。) 要注意i == 0的情况。(是＝=哦) 哈希表705. 设计哈希集合123456789101112131415161718192021222324252627282930313233343536class MyHashSet: def __init__(self): self.data = [None for _ in range(1000)] # 相当于创建1000个None def add(self, key: int) -&gt; None: k = key % 1000 # data[k] 里存的可能也是一个列表 if self.data[k] == None: self.data[k] = [key] # 这边是赋的一个list # # 可能这个data[k]里已经存了多个key值了 for val in self.data[k]: if val == key: return # 如果data[k] != None self.data[k].append(key) # 注意这里，一般情况下是使用data.append()加在列表的最后。而这里面 data[k]内存的就是一个列表，相当于这里的data[k]是个list。 def remove(self, key: int) -&gt; None: k = key % 1000 if self.data[k]: for i, val in enumerate(self.data[k]): # 这里的i是起到一个计数的作用 if val == key: self.data[k].remove(key) # list[].remove() break def contains(self, key: int) -&gt; bool: k = key % 1000 if self.data[k]: for val in self.data[k]: if val == key: return True return False 706. 设计哈希映射12345678910111213141516171819202122232425262728293031323334353637383940414243444546class MyHashMap: def __init__(self): """ Initialize your data structure here. """ self.keys = [] self.values = [] def put(self, key: int, value: int) -&gt; None: """ value will always be non-negative. """ if key in self.keys: # self.keys.index(key) --&gt;找到key所在的list keys的索引值，因为一对键值对对应的索引值是相同的 self.values[self.keys.index(key)] = value # Upgrade else: self.keys.append(key) self.values.append(value) def get(self, key: int) -&gt; int: """ Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key """ if key in self.keys: return self.values[self.keys.index(key)] else: return -1 def remove(self, key: int) -&gt; None: """ Removes the mapping of the specified value key if this map contains a mapping for the key """ if key in self.keys: idx = self.keys.index(key) self.keys.pop(idx) self.values.pop(idx)# # Your MyHashMap object will be instantiated and called as such:# # obj = MyHashMap()# # obj.put(key,value)# # param_2 = obj.get(key)# # obj.remove(key) 771. 宝石与石头12345678class Solution: def numJewelsInStones(self, J: str, S: str) -&gt; int: count = 0 Jset = set(J) for s in S: if s in Jset: count += 1 return count 总结: 这道题用哈希集合可以简化思维过程，即记下在哈希集合中的宝石数目。 链表2.两数相加1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: carry = 0 d=ListNode(0) #初始化一个预先指针，目的为了return时是从开头 开始 p = d while l1 and l2 : #l1,2 不为None p.next = ListNode((l1.val + l2.val + carry) % 10) # p.next是链表处的值 carry = (l1.val + l2.val + carry) // 10 l1 = l1.next l2 = l2.next p = p.next if l1: # 防止出现位数不匹配 while l1: p.next = ListNode((l1.val + carry) % 10) carry = (l1.val + carry) // 10 l1 = l1.next p = p.next if l2: while l2: p.next = ListNode(( l2.val + carry) % 10) carry = (l2.val + carry) // 10 l2 = l2.next p = p.next if carry: #防止出现最后还有一个进位 p.next = ListNode(1) return d.next 总结：1.对于链表问题，返回结果为头结点时，通常需要先初始化一个预先指针 dummy，该指针的下一个节点指向真正的头结点head (即第一次用p.next=…的地方)。使用预先指针的目的在于链表初始化时无可用节点值，而且链表构造过程需要指针移动，进而会导致头指针丢失，无法返回结果。 206. 反转链表12345678910111213141516# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: cur = head prv = None while cur: tmp = cur.next cur.next = prv prv = cur cur = tmp return prv 总结： 1.我们可以申请两个指针，第一个指针叫 prv，最初是指向 None 的。 第二个指针 cur 指向 head，然后不断遍历 cur。 每次迭代到 cur，都将 cur 的 next 指向 pre，然后 prv 和 cur 前进一位。 都迭代完了(cur 变成 null 了)，prv 就是最后一个节点了。 234. 回文链表1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def isPalindrome(self, head: ListNode) -&gt; bool: res = [] while head: res.append(head.val) head = head.next i = 0 k = len(res)-1 while i &lt; k: if res[i] != res[k]: return False i += 1 k -= 1 return True 24.两两交换链表中的节点1234567891011class Solution: def swapPairs(self, head: ListNode) -&gt; ListNode: thead = ListNode(-1) thead.next = head c = thead while c.next and c.next.next: a, b=c.next, c.next.next c.next, a.next = b, b.next b.next = a c = c.next.next return thead.next https://pic.leetcode-cn.com/43254846f029b4814a6c9a139e4f9f89833ac54803ea50b24feb35210631f88b-a.jpg 总结： 1.参考的此方法，通过建立一个额外的空头节点c 21. 合并两个有序链表123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: res = [] while l1: res.append(l1.val) l1 = l1.next while l2: res.append(l2.val) l2 = l2.next res.sort() root = ListNode(0) ptr = root for number in res: ptr.next = ListNode(number) ptr = ptr.next ptr = root.next return ptr 237. 删除链表中的节点1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 总结：阅读理解 19. 删除链表的倒数第N个节点12345678910111213141516171819202122232425# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def removeNthFromEnd(self, head: ListNode, n: int) -&gt; ListNode: if head.next == None: return None p1= head i = 0 while p1: i += 1 p1 = p1.next p2= head while i-n-1 &gt; 0: i -= 1 p2 = p2.next if i-n-1 == -1: head = head.next else: p2.next = p2.next.next return head 206. 反转链表12345678910111213141516# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: pre = None cur = head while cur: tmp = cur.next cur.next = pre pre = cur cur = tmp return pre 141. 环形链表12345678910111213class Solution(object): def hasCycle(self, head): if(head == None or head.next == None): return False node1 = head node2 = head.next while(node1 != node2): if(node2 == None or node2.next == None): return False node1 = node1.next node2 = node2.next.next return True 总结：通过两个指针 一快一慢。注意什么时候判断为False 方法二：哈希值法, 空间复杂度O(n) 这个很好考虑, 把遍历过的节点记录,当发现遍历的节点下一个节点遍历过, 说明有环 12345678910111213def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ lookup = set() p = head while p: lookup.add(p) if p.next in lookup: return True p = p.next return False 队列和栈622. 设计循环队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class MyCircularQueue: def __init__(self, k: int): """ Initialize your data structure here. Set the size of the queue to be k. """ self.f,self.r = 0,0 self.k = k + 1 self.__li = [None for _ in range(self.k)] def enQueue(self, value: int) -&gt; bool: """ Insert an element into the circular queue. Return true if the operation is successful. """ if not self.isFull(): self.__li[self.r] = value self.r = (self.r + 1) % self.k return True else: return False def deQueue(self) -&gt; bool: """ Delete an element from the circular queue. Return true if the operation is successful. """ if not self.isEmpty(): self.f = (self.f + 1) % self.k return True else: return False def Front(self) -&gt; int: """ Get the front item from the queue. """ if not self.isEmpty(): return self.__li[self.f] else: return -1 def Rear(self) -&gt; int: """ Get the last item from the queue. """ if not self.isEmpty(): return self.__li[(self.r - 1 + self.k) % self.k] # 防止减完一变成负的 else: return -1 def isEmpty(self) -&gt; bool: """ Checks whether the circular queue is empty or not. """ return self.r==self.f def isFull(self) -&gt; bool: """ Checks whether the circular queue is full or not. """ return (self.r + 1) % self.k == self.f# Your MyCircularQueue object will be instantiated and called as such:# obj = MyCircularQueue(k)# param_1 = obj.enQueue(value)# param_2 = obj.deQueue()# param_3 = obj.Front()# param_4 = obj.Rear()# param_5 = obj.isEmpty()# param_6 = obj.isFull() 排序和搜索88. 合并两个有序数组123456789class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: """ Do not return anything, modify nums1 in-place instead. """ del nums1[m:] nums1 += nums2 nums1.sort() return nums1 总结：sort 与 sorted 区别： sort 是应用在 list 上的方法，属于列表的成员方法，sorted 可以对所有可迭代的对象进行排序操作。 list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。 sort使用方法为ls.sort()，而sorted使用方法为sorted(ls) 278. 第一个错误的版本 12345678910111213141516171819202122232425262728293031 # The isBadVersion API is already defined for you.# @param version, an integer# @return a bool# def isBadVersion(version):class Solution: def firstBadVersion(self, n): """ :type n: int :rtype: int """ # 找到最小的那个True # 下面两句话就相当于，只要是True就继续往左走，只要是False就继续往右走 # 判断 isBadVersion(mid): # True: 更新right = mid (因为此时的right可能就是想要的结果) # False: 更新left = mid + 1(因为第一个True一定在mid右边)) left, right = 1, n while right - left &gt; 1: # 将left和right分别锁定成两个临近值，最后对着两个值进行单独判断 if isBadVersion((left + right)//2) : right = (left + right)//2 else: left = (left + right)//2 + 1 if isBadVersion(left): return left else: return right 二叉树144. 二叉树的前序遍历方法一：递归 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.result.append(root.val) self.helper(root.left) self.helper(root.right) 总结: 1.我们针对之前所学习的print的那个版本进行修改，通过建立一个数组(result)和一个helper函数。注意这里的result数组的值不能被清零。 方法二：(迭代) 123456789101112131415161718class Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: if not root: return [] stack = [root] res = [] while stack: node = stack.pop() res.append(node.val) if node.right: stack.append(node.right) if node.left: stack.append(node.left) return res 94. 二叉树的中序遍历12345678910111213141516171819# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.helper(root.left) self.result.append(root.val) self.helper(root.right) 145. 二叉树的后序遍历123456789101112131415161718# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def postorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.helper(root.left) self.helper(root.right) self.result.append(root.val) 102. 二叉树的层次遍历123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if root is None: return [] que = [root] result = [] tmp = [] while que: n= len(que) for i in range(n): node = que.pop(0) tmp.append(node.val) if node.left is not None and node.right is not None: que.append(node.left) que.append(node.right) elif node.left is None and node.right is not None: que.append(node.right) elif node.left is not None and node.right is None: que.append(node.left) result.append(tmp) tmp=[] return result 总结: 1.首先看答案的结果，是一层当作一个列表进行输出，而不是一个一个输出，所以这里用了一个tmp列表，并且每层都清零一次 2. 我这个迭代的方法是每层把队列里的值一次性都清空，这样就变成一层一层的了。 104. 二叉树的最大深度123456789101112131415# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def maxDepth(self, root: TreeNode) -&gt; int: if root is None: return 0 # 运用递归的方法，先看假如是一个子树如何处理 left_d = self.maxDepth(root.left) right_d = self.maxDepth(root.right) return max(left_d, right_d)+1 101. 对称二叉树1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: if root is None: return True que = [root, root] while que: node1 = que.pop(0) node2 = que.pop(0) # 如果两个节点都是空就继续循环，有一个是空就False if not (node1 or node2): continue if not (node1 and node2): return False if node1.val != node2.val: return False # 将左节点的左孩子和右节点的右孩子入队 que.append(node1.left) que.append(node2.right) # 将左节点的右孩子和右节点的左孩子入队 que.append(node1.right) que.append(node2.left) return True 总结: 与普通的迭代遍历不同，这里要注意压入的左右孩子。 105. 从前序与中序遍历序列构造二叉树1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Nonefrom typing import Listfrom BuildTree import *class Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if len(preorder) == 0: return None # 先把根找到 (注意根root是个节点不是一个数) root = TreeNode(preorder[0]) mid = inorder.index(preorder[0]) root.left = self.buildTree(preorder[1:mid+1], inorder[:mid]) root.right = self.buildTree(preorder[mid+1:], inorder[mid+1:]) return rootif __name__ == "__main__": preo = [3,9,20,15,7] ino = [9,3,15,20,7] root = Solution().buildTree(preo, ino) s = travelTree(root) print(s) 136. 只出现一次的数字1234567891011121314from typing import Listclass Solution: def singleNumber(self, nums: List[int]) -&gt; int: hash_table = &#123;&#125; for i in nums: try: hash_table.pop(i) except: hash_table[i] = 1 return hash_table.popitem()[0]if __name__ == "__main__": nums = [4, 1, 2, 1, 2] print(Solution().singleNumber(nums)) 总结: 1. try except语句是当try中的语句发生错误时执行except内的语句。 2. 也可以使用异或，因为 相同数字 的异或 就是0 350. 两个数组的交集 II12345678class Solution: def intersect(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: result = [] for i in nums1: if i in nums2: result.append(i) nums2.remove(i) return result 方法二： 123class Solution: def intersect(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: return [*(collections.Counter(nums1) &amp; collections.Counter(nums2)).elements()] ## 123456789101112131415161718class Solution: def plusOne(self, digits): d=0 for x in digits: d = d*10 + int(x) # d = "" # for i in digits: # d =d+str(i) # d = int(d) d = d+1 res = [] for i in str(d): res.append(int(i)) return resif __name__ == '__main__': s = [9,9,9] print(Solution().plusOne(s)) 总结:此方法是把 str()–&gt;int()–&gt;str() 283. 移动零1234567891011121314class Solution: def moveZeroes(self, nums: List[int]) -&gt; None: """ Do not return anything, modify nums in-place instead. """ n = len(nums) i, j = 0, 0 while (i + j &lt; n): if nums[i] == 0: nums.pop(i) nums.append(0) j += 1 continue i += 1]]></content>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3实战Spark大数据分析及调度]]></title>
    <url>%2F2019%2F12%2F17%2FPython3%E5%AE%9E%E6%88%98Spark%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%8F%8A%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[spark的运行1.启动hadoop (要在root用户下运行) 123cd app/hadoopxxxxxxx/sbinstart-dfs.sh**可在 http://localhost:50070/ 中查看自己上传的文件** 2.启动spark 12cd app/sparkxxxxxxx/bin./spark-shell spark core核心 RDDSpark源码所在的地方：https://github.com/apache/spark 官网：xxxx.apache.org源码：https://github.com/apache/xxxx spark url查看 http://hadoop000:4040 什么是RDDabstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]]) extends Serializable with Logging 1）RDD是一个抽象类2）带泛型的，可以支持多种类型： String、Person、User RDD：Resilient Distributed Dataset 弹性 分布式 数据集 弹性：遇到故障以后可以进行修复或者跟踪 Represents an immutable：不可变(RDD不可变) partitioned collection of elements ：分区 Array(1,2,3,4,5,6,7,8,9,10) 3个分区： (1,2,3) (4,5,6) (7,8,9,10) that can be operated on in parallel： 并行计算的问题 单机存储/计算==&gt;分布式存储/计算1）数据的存储: 切割 HDFS的Block2）数据的计算: 切割(分布式并行计算) MapReduce/Spark3）存储+计算 : HDFS/S3+MapReduce/Spark ==&gt; OK RDD的五大特性：Internally, each RDD is characterized by five main properties: *1. *A list of partitions 一系列的分区/分片 *2. *A function for computing each split(切片)/partition y = f(x) rdd.map(_+1) :对RDD进行函数就是对RDD里所有的分区进行一个函数 3. A list of dependencies on other RDDs rdd1 ==&gt; rdd2 ==&gt; rdd3 ==&gt; rdd4 存在依赖关系(依赖其他RDD) dependencies:*\* rdda = 5个partition ==&gt;map操作 rddb = 5个partition 当遇到rdda内第三个分区丢失时，spark会通过dependency重新计算数据(从源文件重新读取) *4. *Optionally(可选的), a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) *5. *Optionally, a list of preferred locations(最佳的位置) to compute each split on (e.g. block locations for an HDFS file) 数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算 为什么location有s？ 答: spark会尽可能地将任务分配到数据块所存储的位置。Hadoop中读取数据生成RDD时，preferredLocation[s]返回每一个数据块所在的机器名或者IP地址，如果每一块数据是多份存储的，那么就会返回多个机器地址,以便后续调度的程序根据这个地址更加有效地分配任务. 五大特性源码体现：def compute(split: Partition, context: TaskContext): Iterator[T] 特性二def getPartitions: Array[Partition] 特性一def getDependencies: Seq[Dependency[_]] = deps 特性三def getPreferredLocations(split: Partition): Seq[String] = Nil 特性五val partitioner: Option[Partitioner] = None 特性四 图解RDD SparkContext&amp;SparkConf第一要务：创建SparkContext 连接到Spark“集群”：local、standalone、yarn、mesos 通过SparkContext来创建RDD、广播变量到集群 在创建SparkContext之前还需要创建一个SparkConf对象(conf-&gt;配置) RDD创建方式 Parallelized Collections External Datasets If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes 1）我们上课是在单节点上的：一个节点， hello.txt只要在这台机器上有就行了 2）standalone: Spark集群： 3个节点 local path 都是从节点的本地读取数据 不建议 IDE开发开发pyspark应用程序 1) IDE(Integrated Development Environment ): IDEA pycharm 2) 设置基本参数: python interceptor PYTHONPATH SPARK_HOME 2zip包 3）开发 4）使用local进行本地测试 提交pyspark应用程序($SPARK_HOME) ./spark-submit –master local[2] –name spark0301 /home/hadoop/script/spark0301.py 具体提交的详细说明参见：http://spark.apache.org/docs/latest/submitting-applications.html Spark任务提交全流程 1.Driver端启动SparkSubmit进程，启动后开始向Master进行通信，此时创建了一个对象（SparkContext），接着向Master发送任务消息2.Master接收到任务信息后，开始资源调度，此时会和所有的Worker进行通信，找到空闲的Worker，并通知Worker来拿取任务和启动相应的Executor3.Executor启动后，开始与Driver进行反向注册，接下来Driver开始把任务发送给相应的Executor，Executor开始计算任务 全流程: https://imgchr.com/i/ljBytg 1.调用SparkSubmit类，内部执行submit –&gt; doRunMain -&gt; 通过反射获取应用程序的主类对象 –&gt; 执行主类的main方法。2.构建SparkConf和SparkContext对象，在SparkContext入口做了三件事，创建了SparkEnv对象（创建了ActorSystem对象），TaskScheduler（用来生成并发送task给Executor），DAGScheduler（用来划分Stage）。3.ClientActor将任务信息封装到ApplicationDescription对象里并且提交给Master。4.Master收到ClientActor提交的任务信息后，把任务信息存在内存中，然后又将任务信息放到队列中。5.当开始执行这个任务信息的时候，调用scheduler方法，进行资源的调度。6.将调度好的资源封装到LaunchExecutor并发送给对应的Worker。7.Worker接收到Master发送过来的调度信息（LaunchExecutor）后，将信息封装成一个ExecutorRunner对象。8.封装成ExecutorRunner后，调用ExecutorRunner的start方法，开始启动 CoarseGrainedExecutorBackend对象。9.Executor启动后向DriverActor进行反向注册。10.与DriverActor注册成功后，创建一个线程池（ThreadPool），用来执行任务。11.当所有的Executor注册完成后，意味着作业环境准备好了，Driver端会结束与SparkContext对象的初始化。12.当Driver初始化完成后（创建了sc实例），会继续执行我们提交的App的代码，当触发了Action的RDD算子时，就触发了一个job，这时就会调用DAGScheduler对象进行Stage划分。13.DAGScheduler开始进行Stage划分。14.将划分好的Stage按照区域生成一个一个的task，并且封装到TaskSet对象，然后TaskSet提交到TaskScheduler。15.TaskScheduler接收到提交过来的TaskSet，拿到一个序列化器，对TaskSet序列化，将序列化好的TaskSet封装到LaunchExecutor并提交到DriverActor。16.把LaunchExecutor发送到Executor上。17.Executor接收到DriverActor发送过来的任务（LaunchExecutor），会将其封装成TaskRunner，然后从线程池中获取线程来执行TaskRunner。18.TaskRunner拿到反序列化器，反序列化TaskSet，然后执行App代码，也就是对RDD分区上执行的算子和自定义函数。 a example about SparkContext&amp;SparkConf123456789101112131415from pyspark import SparkConf,SparkContext# 创建SparkConf：设置的是Spark相关的参数信息conf = SparkConf().setMaster("local[2]").setAppName("spark0301")# 创建SparkContextsc = SparkContext(conf=conf)# 业务逻辑data = [1,2,3,4,5]distData = sc.parallelize(data) # parallelize的目的是为了把data转换成RDDprint(distData.collect())# 好的习惯sc.stop() RDD编程官网参考文件http://spark.apache.org/docs/latest/rdd-programming-guide.html RDD Programming Guide RDD Operation transformations: create a new dataset from an existing one RDDA —transformation–&gt; RDDB y = f(x) rddb = rdda.map(....) lazy(*****) (它不会立刻计算，相反，它仅仅记住作用到数据集中---直到遇到action( collect() )) rdda.map().filter()......collect map/filter/group by/distinct/..... actions: return a value to the driver program after running a computation on the dataset count/reduce/collect...... 1) transformation are lazy, nothing actually happens until an action is called; 2) action triggers the computation; 3) action returns values to driver or writes data to external storage;常用的RDD函数map: map(func) 将func函数作用到数据集的每一个元素上，生成一个新的分布式的数据集返回 word =&gt; (word,1)map()是将传入的函数依次作用到序列的每个元素，每个元素都是独自被函数“作用”一次 。 reduce()是将传人的函数作用在序列的第一个元素得到结果后，把这个结果继续与下一个元素作用（累积计算）。 简单说，map是对每个元素进行操作-返回一个列表；reduce是所有元素操作-返回一个结果。 filter: filter(func) 选出所有func返回值为true的元素，生成一个新的分布式的数据集返回 flatMap —-&gt;(压扁以后做map 先拆成多个部分) flatMap(func) 输入的item能够被map到0或者多个items输出，返回值是一个Sequence groupByKey： ​ 把相同的key的数据分发到一起​ [‘hello’, ‘spark’, ‘hello’, ‘world’, ‘hello’, ‘world’]​ (‘hello’,1) (‘spark’,1)…….. *reduceByKey: * ​ reduceByKey的作用对像是(key, value)形式的rdd，而reduce有减少、压缩之意，reduceByKey的作用就是对相同key的数据进行处理，最终每个key只保留一条记录。 ​ 把相同的key的数据分发到一起并进行相应的计算​ mapRdd.reduceByKey(lambda a,b:a+b)​ [1,1] 1+1​ [1,1,1] 1+1=2+1=3​ [1] 1 ​ 需求: 请按wc结果中出现的次数降序排列 sortByKey 按照Key进行排序​ (‘hello’, 3), (‘world’, 2), (‘spark’, 1) 答：完成上面的方法就是key和value交换 即lambda x：x[1],x[0] 后进行sortByKey， 最后再做一步map再交换回来 union： *join： * inner join outer join:left/right/full action: rdd.take(3) 显示前3个 rdd.sum()求和 rdd.max()最大值 rdd.count()计数 saveAsTextFile()以文件的形式保存 如何在本地运行spark(不用IDE，提交到本地)，并写到文件系统中 1.把在IDE中已经测试好的代码建立 .py（/mycode） 2.在到./spark中 执行—&gt;此时可以在控制台上显示 ./spark-submit --master local[2] --name spark04cp /home/hadoop/mycode/spark04cp.py(执行文件) file:///home/hadoop/data/hello.txt(要处理的数据) 3.在代码中加入 saveAsTextFile(sys.argv[2]) 在执行是最后再加入第三个参数，即为保存的目录 ./spark-submit --master local[2] --name spark04cp /home/hadoop/mycode/spark04cp.py(执行程序) file:///home/hadoop/data/hello*.txt(要处理的数据) file:///home/hadoop/data/tmp/wc(保存的处理后数据的位置) 算子综合实例：一、词频案例:wc 1) input: 1/n文件 文件夹 后缀名 hello spark hello hadoop hello welcome 2) 开发步骤分析 文本内容的每一行转成一个个的单词 : flatMap 单词 ==&gt; (单词, 1): map 把所有相同单词的计数相加得到最终的结果: reduceByKey 最后按照个数的升序排列 123456789101112131415161718192021222324252627282930import sysfrom pyspark import SparkConf, SparkContextif __name__ == '__main__': if len(sys.argv) != 3: # 等于1代表只有这个.py运行了，没有别的外部参数 print('Usage:wordcount &lt;input&gt;', file=sys.stderr) sys.exit(-1) # 如果没有文件输入就直接退出 conf = SparkConf() sc = SparkContext(conf=conf) # sys.argv[0] 代表运行的本程序名称 sys.argv[1]代表输入 print(sys.argv[0]) print(sys.argv[1]) def printResult(): counts = sc.textFile(sys.argv[1]) \ .flatMap(lambda line: line.split("\t")) \ .map(lambda x: (x, 1)) \ .reduceByKey(lambda a, b: (a + b))\ .map(lambda x: (x[1], x[0]))\ .sortByKey()\ .map(lambda x: (x[1], x[0])).saveAsTextFile(sys.argv[2]) printResult() sc.stop() 二、TopN 1) input : 1/n文件 文件夹 后缀名 2) 求某个维度的topn 3）开发步骤分析 文本内容的每一行根据需求提取出你所需要的字段： map 单词 ==&gt; (单词, 1): map 把所有相同单词的计数相加得到最终的结果: reduceByKey 取最多出现次数的降序： sortByKey 平均数：统计平均年龄id age3 964 445 676 47 98 开发步骤分析： 1) 取出年龄 map 2）计算年龄综合 reduce 3）计算记录总数 count 4）求平均数 spark运行模式 Local模式： 开发 123--master --name # 应用程序名称--py-files # 上传文件 ./spark-submit --master local[2] --name spark-local /home/hadoop/script/spark0402.py file:///home/hadoop/data/hello.txt file:///home/hadoop/wc/output standalone hdfs: NameNode DataNode yarn: ResourceManager NodeManager 1234567891011121314151617181920master: (主)worker: (从)$SPARK_HOME/conf/slaves hadoop000 假设你有5台机器，就应该进行如下slaves的配置 hadoop000 hadoop001 hadoop002 hadoop003 hadoop004 如果是多台机器，那么每台机器都在相同的路径下部署spark启动spark集群 $SPARK_HOME/sbin/start-all.sh ps: 要在spark-env.sh中添加JAVA_HOME，否则会报错 检查： jps： Master和Worker进程，就说明我们的standalone模式安装成功 webui： ./spark-submit --master spark://hadoop000:7077 --name spark-standalone /home/hadoop/script/spark0402.py hdfs://hadoop000:8020/wc.txt hdfs://hadoop000:8020/wc/output 如果使用standalone模式，而且你的节点个数大于1的时候，如果你使用本地文件测试，必须要保证每个节点上都有本地测试文件 (或者选择在hdfs内的文件)yarn mapreduce yarn spark on yarn 70% spark作为客户端而已，他需要做的事情就是提交作业到yarn上去执行 yarn vs standalone yarn： 你只需要一个节点，然后提交作业即可 这个是不需要spark集群的（不需要启动master和worker的） standalone：你的spark集群上每个节点都需要部署spark，然后需要启动spark集群（需要master和worker） ./spark-submit --master yarn --name spark-yarn /home/hadoop/script/spark0402.py hdfs://hadoop000:8020/wc.txt hdfs://hadoop000:8020/wc/output When running with master ‘yarn’ either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment 作业：试想：为什么需要指定HADOOP_CONF_DIR或者YARN_CONF_DIR 如何使得这个信息规避掉Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME yarn支持client和cluster模式：driver运行在哪里 client：提交作业的进程是不能停止的，否则作业就挂了 cluster：提交完作业，那么提交作业端就可以断开了，因为driver是运行在am里面的 Error: Cluster deploy mode is not applicable to Spark shells 12pyspark/spark-shell : 交互式运行程序 clientspark-sql 如何查看已经运行完的yarn的日志信息： yarn logs -applicationId (此ID在hadoop000:8088/cluster中可以查看)Log aggregation has not completed or is not enabled.参见：https://coding.imooc.com/class/chapter/128.html#Anchor JobHistory使用 不管你的spark应用程序运行在哪里，你的spark代码都是一样的，不需要做任何的修改和调整，所以spark使用起来是非常方便的！！！！！！ ##spark core 进阶 Spark核心概述 ​ ​ 用户提交Job后会生成SparkContext对象，SparkContext向Cluster Manager（在Standalone模式下是Spark Master）申请Executor资源，并将Job分解成一系列可并行处理的task，然后将task分发到不同的Executor上运行，Executor在task执行完后将结果返回到SparkContext。 (上图SparkContext 与 executor为什么是双向的是因为 当连接的时候， SparkContext acquires executors on nodes in the cluster, which are processes that run computations and store data for your application，同时 executor返回心跳信息，当一个exceutor挂掉的时候可以开启另外一个exceutor http://spark.apache.org/docs/latest/cluster-overview.html) 1.Application ：基于Spark的应用程序 = 1 driver + executors User program built on Spark. Consists of a driver program and executors on the cluster. spark0402.py （这就是一个应用程序） pyspark/spark-shell（这也是） 2.Driver program The process running the main() function of the application creating the SparkContext 3.Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) spark-submit –master local[2]/spark://hadoop000:7077/yarn(后面这些东西就是集群管理) 4.Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Yarn-cluster VS Yarn-client*5.Worker node * Any node that can run application code in the cluster standalone: slave节点 slaves配置文件 yarn: nodemanager *6.Executor * A process launched for an application on a worker node runs tasks keeps data in memory or disk storage across them Each application has its own executors. *7.Task * A unit of work that will be sent to one executor 8.Job ** A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. //一个action对应一个job，理解：job里面多个task，运行在executor里**// 9.Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. 一个stage的边界往往是从某个地方取数据开始，到shuffle的结束 另外的解释方式 job定义：我们都知道，在spark rdd中，有action、transform操作，当真正触发action时，才真正执行计算，此时产生一个job任务。 stage定义：以shuffle为界，当在一个job任务中涉及shuffle操作时，会进行stage划分，产生一个或多个stage。 task定义： 一个stage可能包含一个或者多个task任务，task任务与partition、executor息息相关，即并行度。 partition定义： partition个数即rdd的分区数，不同的数据源读进来的数据分区数默认不同，可以通过repartition进行重分区操作。 executor定义： executor运行在work上，一个work可以运行一个或多个executor，一个executor可以运行一个或者多个task（取决于executor的core个数，默认是一个task占用一个core，即有多少个core就可以启动多少个task任务） 各个之间的关系图： ​ 一个work有一个或多个executor​ 一个executor有一个或者多个task（取决于executor的core个数）​ 一个task对应一个partition分区，即并行度（官网推荐partition分区数设置是task个数的2~3倍，充分利用资源）​ 一个stage有一个或多个task任务​ 一个job有一个或多个stage 理解：job里面多个task，运行在executor里 hadoop与spark对比 10.Spark Cache rdd.cache(): StorageLevel ​ cache它和tranformation: lazy 没有遇到action是不会提交作业到spark上运行的​ 如果一个RDD在后续的计算中可能会被使用到，那么建议cache​​ cache底层调用的是persist方法，传入的参数是：StorageLevel.MEMORY_ONLY​ cache=persist​​ unpersist: 立即执行的 11.Spark Lineage ​ 是一种RDD之间的依赖关系，可以有效的帮助容错。(当一个里面的RDD内的paritition丢失时，因为Lineage记录paritition的血缘关系) 12.Spark Dependency(依赖) ​ 窄依赖：一个父RDD的partition至多被子RDD的某个partition使用一次 (pipiline流水线) ​ 宽依赖：一个父RDD的partition会被子RDD的partition使用多次，有shuffle (宽依赖的容错效果更差) hello,1 hello,1 hello(相同的放在一起处理) world,1 hello,1 world world,1RDD的shuffle以及依赖关系 正如上图，hdfs在三台机器上运行，左边前3个操作都是窄依赖(lines、words、pairs), 当遇到reduceByKey(宽依赖)时，会进行shuffle操作，同时也会变成两个stage。 sc.textFile(“file:///home/hadoop/data/hello.txt”).flatMap(lambda line: line.split(“\t”)).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).collect() Spark Core调优HistorySever 优化12$SPARK_HOME/sbin/./start-history-server.sh 可以在./start-history-server.sh参看历史信息 序列化优化1.java 速度慢，简单，默认 2.kryo速度快，麻烦 内存管理优化两大类：执行和存储 (一般各占一半) 广播变量优化一个机器一个副本，可以减小副本的大小 ###数据本地性优化 如果数据和节点在同一台机器上时，运行速度最快 移动计算而不是移动数据！！ Spark SQLSpark SQL SQL: MySQL、Oracle、DB2、SQLServer 很多小伙伴熟悉SQL语言 数据量越来越大 ==&gt; 大数据(Hive、Spark Core-基于RDD) 直接使用SQL语句来对大数据进行分析：这是大家所追逐的梦想 person.txt ==&gt; 存放在HDFS 1,zhangsan,30 2,lisi,31 3,wangwu,32 hive表：person id:int name:string age:int 导入数据： load ..... 统计分析： select ... from person SQL on Hadoop Hive Impala: Cloudera Presto Drill ….. 12345678Hive: on MapReduce (性能差) SQL =(翻译)=&gt; MapReduce =(提交到)=&gt; Hadoop ClusterSpark SQL: on SparkHive on Spark (与上面的不同)共同点： metastore mysql Spark SQL不仅仅是SQL这么简单的事情，它还能做更多的事情 Hive: SQL Spark SQL: SQL Spark SQL提供的操作数据的方式 SQL DataFrame API Dataset API ===&gt; 一个用于处理结构化数据的Spark组件，强调的是“结构化数据”，而非“SQL” Spark RDD VS MapReduceR/Pandas : one machine (单机的) ==&gt; DataFrame：让小伙伴们感觉像开发单机版应用程序一样来开发分布式应用程序 A DataFrame is a Dataset organized into named columns (可以简单的把DataFrame理解成关系型数据库中的一个表，这是表层，底层其实做了更深的优化)以列(列名、列类型、列值)的形式构成分布式的数据集 面试题：RDD与DataFrame的区别12345 schema: 在SQL环境下，schema就是数据库对象的集合 区别 RDD与DataFrame方法一： Inferring the Schema Using Reflection 方法二： 下面的已运行过 8_8.py 123456789101112131415161718192021222324252627282930313233343536# Import data typesfrom pyspark.sql.types import *from pyspark.sql import SparkSessionspark = SparkSession.builder.appName('spark0801').getOrCreate()sc = spark.sparkContext# Load a text file and convert each line to a Row.lines = sc.textFile("file:///home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.txt")parts = lines.map(lambda l: l.split(","))# Each line is converted to a tuple.people = parts.map(lambda p: (p[0], p[1].strip()))# The schema is encoded in a string.schemaString = "name age"fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]schema = StructType(fields)# Apply the schema to the RDD.schemaPeople = spark.createDataFrame(people, schema)# Creates a temporary view using the DataFrameschemaPeople.createOrReplaceTempView("people")print(schemaPeople.printSchema())# SQL can be run over DataFrames that have been registered as a table.# results = spark.sql("SELECT name FROM people")print(schemaPeople.show())sc.stop()# +-------+# | name|# +-------+# |Michael|# | Andy|# | Justin|# +-------+ Spark Streamingis an extension of the core Spark API enables scalable, high-throughput, fault-tolerant stream processing of live data streams流： Java SE IO 输入: 山沟沟、下水道… Kafka, Flume, Kinesis, or TCP sockets // TODO… 业务逻辑处理 输出: 痛、瓶子…. filesystems, databases, and live dashboards 在线机器学习 Q:安装完Spark之后能否直接使用Spark Streaming?A:YES 常用实时流处理框架对比 Storm：真正的实时流处理 Tuple Java Spark Streaming：并不是真正的实时流处理，而是一个mini batch操作(批处理) Scala、Java、Python 使用Spark一栈式解决问题 Flink: 底层是流处理，可以做到批处理(和spark相反) Kafka Stream Spark Streaming它的职责所在 receives live input data streams divides the data into batches batches are then processed by the Spark engine to generate the final stream of results in batches. Spark Core的核心抽象叫做：RDD 5大特性、对应源码中的5个方法是什么Spark Streaming的核心抽象叫做：DStream represents a continuous stream of data DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs. Azkaban基础篇(工作流插件)（必须在/bin/az..启动，否则报错）工作流概述 请假 OA 1 ：部门经理审批 3 ：部门经理审批 ==&gt; HR 5 ：部门经理审批 ==&gt; HR ==&gt; 老大 10：….. 借款： 涉及金额 Spark SQL/Hadoop用于做离线统计处理ETL(ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。)1) 数据抽取： Sqoop把RDBMS中的数据抽取到Hadoop Flume进行日志、文本数据的采集，采集到Hadoop2) 数据处理 Hive/MapReduce/Spark/……3) 统计结果入库 数据就存放到HDFS(Hive/Spark SQL/文件) 启动一个Server: HiveServer2 / ThriftServer jdbc的方式去访问统计结果 使用Sqoop把结果导出到RDBMS中 这些作业之间是存在时间先后依赖关系的Step A ==&gt; Step B ==&gt; Step C crontab定时调度为了更好的组织起这样的复杂执行计算的关系===&gt; 这就需要一个工作流调度系统来进行依赖关系作业的调度 Linux crontab + shell 优点：简单、易用 缺点： 维护 依赖 step a: 01:30 30分钟 step b: 02:10 30分钟 step c: 02:50 30分钟 ….. 资源利用率 集群在0130压力非常大，资源没有申请到 常用的调度框架 Azkaban：轻量级 Oozie：重量级 cm hue xml 宙斯(Zeus) Azkaban概述 Open-source Workflow Manager 批处理工作流，用于跑Hadoop的job 提供了一个易于使用的用户界面来维护和跟踪你的工作流程 Azkaban架构 Relational Database (MySQL) AzkabanWebServer AzkabanExecutorServer Azkaban运行模式 solo-server 数据信息存储在H2==&gt;MySQL webserver和execserver是运行在同一个进程中婢女 the heavier weight two server mode 数据信息存储在MySQL，在生产上一定要做主备 webserver和execserver是运行在不同的进程中的 distributed multiple-executor mode Azkaban编译：万世开头难，务必要保证你的网络速度不错(这里的编译是在对源码进行修改以后进行的) 1） 去github上下载源码包 2） ./gradlew build installDist 3） 建议搭建先去下载gradle-4.1-all.zip 然后整合azkaban源码中来，避免在编译的过程中去网络上下载，导致编译速度非常慢 4） 编译成功之后，去对应的目录下找到对应模式的安装包即可 Azkaban环境搭建 1) 解压编译后的安装包到~/app 2）启动azkaban $AZKABAN_HOME/bin/azkaban-solo-start.sh 验证：jps AzkabanSingleServer ip:8081 ES使用1234# bin目录下./elasticsearch# 查看192.168.211.4:9200 kibana使用123bin/kibana# 查看192.168.211.4:5601 实战大数据项目开发流程1) 调研 业务2) 需求分析 项目的需求 显示 隐式 甘特图：项目周期管理3) 方案设计 概要设计 详细设计 基本要求 系统要求：扩展性、容错性、高可用(HDFS YARN HA???)、定制化4) 功能开发 开发 单元测试 junit5) 测试 测试环境 QA 功能、性能、压力 用户测试6) 部署上线 试运行 DIFF “双活” 正式上线7) 运维 7*248) 后期迭代开发 大数据企业级应用1) 数据分析 商业 自研2）搜索/引擎 Lucene/Solr/ELK3）机器学习4) 精准营销5) 人工智能 企业级大数据分析平台1) 商业 2) 自研 Apache CDH HDP 数据量预估及集群规划Q: 一条日志多大、多少个字段、一天多少数据300500字节 * 1000W * 5 * 5 = 100GHDFS 3副本 * 100G * (23年) 服务器一台：磁盘多少？ ==&gt; Node数量 集群规模：数据量 + 存储周期 集群机器规模：DN: 数据量大小/每个Node的磁盘大小 NN: 2 RM: 2 NM: DN ZK: 3/5/7/9 GATEWAY: 资源设置：cpu/memory/disk/network 作业规划：MapReduce/Hive/Spark Server: ***** 调度：AZ、OOZIE数据来源：http://stateair.net/web/historical/1/1.html 根据北京的数据进行统计分析 同时间：北京 vs 广州 vs 成都 空气质量指数 pm2.5 健康建议0-50 健康51-100 中等101-150 对敏感人群不健康151-200 不健康201-300 非常不健康301-500 危险 500 爆表 数据分析==&gt;es==&gt;kibana data2017 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2017_HourlyPM25_created20170803.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2016 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2016_HourlyPM25_created20170201.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2015 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2015_HourlyPM25_created20160201.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2017.show()data2016.show()data2015.show()def get_grade(value): if value &lt;=50 and value &gt;=0: return “健康” elif value &lt;= 100: return “中等” elif value &lt;= 150: return “对敏感人群不健康” elif value &lt;= 200: return “不健康” elif value &lt;= 300: return “非常不健康” elif value &lt;= 500: return “危险” elif value &gt; 500: return “爆表” else: return None # 进来一个Value，出去一个Gradegroup2017 = data2017.withColumn(“Grade”,grade_function_udf(data2017[‘Value’])).groupBy(“Grade”).count()group2016 = data2016.withColumn(“Grade”,grade_function_udf(data2016[‘Value’])).groupBy(“Grade”).count()group2015 = data2015.withColumn(“Grade”,grade_function_udf(data2015[‘Value’])).groupBy(“Grade”).count() group2017.show()group2016.show()group2015.show() 使用SparkSQL将统计结果写入到ES中去 from pyspark.sql.functions import *from pyspark.sql.types import * def get_grade(value): if value &lt;= 50: return “健康” elif value &lt;= 100: return “中等” elif value &lt;= 150: return “对敏感人群不健康” elif value &lt;= 200: return “不健康” elif value &lt;= 300: return “非常不健康” elif value &lt;= 500: return “危险” elif value &gt; 500: return “爆表” else: return None data2017 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“/data/Beijing_2017_HourlyPM25_created20170803.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)grade_function_udf = udf(get_grade, StringType())group2017 = data2017.withColumn(“Grade”, grade_function_udf(data2017[‘Value’])).groupBy(“Grade”).count()result2017_2 = group2017.select(“Grade”, “count”, group2017[‘count’] / data2017.count()*100) result2017_2=group2017.select(“Grade”, “count”).withColumn(“precent”,group2017[‘count’] / data2017.count()*100) result2017_2.selectExpr(“Grade as grade”, “count”, “precent”).write.format(“org.elasticsearch.spark.sql”).option(“es.nodes”,”192.168.199.102:9200”).mode(“overwrite”).save(“weaes/weather”) 练习：1) 同一个城市不同年份的对比2）相同年份的不同城市的对比 3) 月份为统计维度：3-1 3-24) 小时为统计维度 curl -XPOST ‘http://hadoop000:9200/imooc_es/student/1&#39; -H ‘Content-Type: application/json’ -d ‘{“name”:”imooc”,“age”:5,“interests”:[“Spark”,”Hadoop”]}’ ek后台启动： nohup …. &amp;]]></content>
  </entry>
  <entry>
    <title><![CDATA[克隆虚拟机]]></title>
    <url>%2F2019%2F12%2F17%2F%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181.克隆虚拟机2.克隆后的虚拟机配置sudo -i修改网卡信息vi /etc/udev/rules.d/70-persistent-net.rules 修改主机名vi /etc/sysconfig/network修改ip信息(改UUID的一个数字、ipaddr末尾改一个、HWADDR换成网卡的MAC地址)vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改映射vi /etc/hosts重启网卡service network restart]]></content>
  </entry>
  <entry>
    <title><![CDATA[大数据技术原理与应用]]></title>
    <url>%2F2019%2F11%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hadoop的启动12cd /usr/local/hadoop./sbin/start-dfs.sh #启动hadoop HDFSHDFS的启动 12cd /usr/local/hadoop../bin/hdfs dfs +命令 用shell命令将本地文件上传到HDFS中 1. 目录操作需要注意的是，Hadoop系统安装好以后，第一次使用HDFS时，需要首先在HDFS中创建用户目录。本教程全部采用hadoop用户登录Linux系统，因此，需要在HDFS中为hadoop用户创建一个用户目录，命令如下： 1cd /usr/local/hadoop./bin/hdfs dfs –mkdir –p /user/hadoop Shell 命令 该命令中表示在HDFS中创建一个“/user/hadoop”目录，“–mkdir”是创建目录的操作，“-p”表示如果是多级目录，则父目录和子目录一起创建，这里“/user/hadoop”就是一个多级目录，因此必须使用参数“-p”，否则会出错。“/user/hadoop”目录就成为hadoop用户对应的用户目录，可以使用如下命令显示HDFS中与当前用户hadoop对应的用户目录下的内容： 1./bin/hdfs dfs –ls . Shell 命令 该命令中，“-ls”表示列出HDFS某个目录下的所有内容，“.”表示HDFS中的当前用户目录，也就是“/user/hadoop”目录，因此，上面的命令和下面的命令是等价的： 1./bin/hdfs dfs –ls /user/hadoop Shell 命令 如果要列出HDFS上的所有目录，可以使用如下命令： 1./bin/hdfs dfs –ls Shell 命令 下面，可以使用如下命令创建一个input目录： 1./bin/hdfs dfs –mkdir input Shell 命令 在创建个input目录时，采用了相对路径形式，实际上，这个input目录创建成功以后，它在HDFS中的完整路径是“/user/hadoop/input”。如果要在HDFS的根目录下创建一个名称为input的目录，则需要使用如下命令： 1./bin/hdfs dfs –mkdir /input Shell 命令 可以使用rm命令删除一个目录，比如，可以使用如下命令删除刚才在HDFS中创建的“/input”目录（不是“/user/hadoop/input”目录）： 1./bin/hdfs dfs –rm –r /input Shell 命令 上面命令中，“-r”参数表示如果删除“/input”目录及其子目录下的所有内容，如果要删除的一个目录包含了子目录，则必须使用“-r”参数，否则会执行失败。 2. 文件操作12345在实际应用中，经常需要从本地文件系统向HDFS中上传文件，或者把HDFS中的文件下载到本地文件系统中。首先，使用vim编辑器，在本地Linux文件系统的“/home/hadoop/”目录下创建一个文件myLocalFile.txt，里面可以随意输入一些单词，比如，输入如下三行：HadoopSparkXMU DBLAB 然后，可以使用如下命令把本地文件系统的“/home/hadoop/myLocalFile.txt”上传到HDFS中的当前用户目录的input目录下，也就是上传到HDFS的“/user/hadoop/input/”目录下： 1./bin/hdfs dfs -put /home/hadoop/myLocalFile.txt input 可以使用ls命令查看一下文件是否成功上传到HDFS中，具体如下： 1./bin/hdfs dfs –ls input1 该命令执行后会显示类似如下的信息： 12Found 1 items -rw-r--r-- 1 hadoop supergroup 36 2017-01-02 23:55 input/ myLocalFile.txt 下面使用如下命令查看HDFS中的myLocalFile.txt这个文件的内容： 1./bin/hdfs dfs –cat input1/myLocalFile.txt 下面把HDFS中的myLocalFile.txt文件下载到本地文件系统中的“/home/hadoop/下载/”这个目录下，命令如下： 1./bin/hdfs dfs -get input1/myLocalFile.txt /home/hadoop/下载 可以使用如下命令，到本地文件系统查看下载下来的文件myLocalFile.txt： 1cd ~cd 下载lscat myLocalFile.txt 最后，了解一下如何把文件从HDFS中的一个目录拷贝到HDFS中的另外一个目录。比如，如果要把HDFS的“/user/hadoop/input/myLocalFile.txt”文件，拷贝到HDFS的另外一个目录“/input”中（注意，这个input目录位于HDFS根目录下），可以使用如下命令： 1./bin/hdfs dfs -cp input/myLocalFile.txt /input 可在 http://localhost:50070/ 中查看自己上传的文件 ###3.用pyhdfs对文件进行操作：(先启动hadoop) 1234567891011121314import pyhdfsclient = pyhdfs.HdfsClient(hosts="localhost,50070",user_name="hadoop")print(client.listdir("/user/hadoop/input")) # 返回指定目录下的所有文件response = client.open("/user/hadoop/input/test.txt") # 打开文件 并读取print(response.read())print(client.exists('/user/hadoop/input/test.txt')) # 判断文件是否存在print(client.get_home_directory()) # 返回 用户的根目录print(client.get_active_namenode()) # 返回可用的namenode节点 HBase启动HBase 1234567891.启动hadoop (启动后输入jps后可以看大盘Name(data/secondary)Node)ssh localhostcd /usr/local/hadoop./sbin/start-dfs.sh2.启动HBasecd /usr/local/hbasebin/start-hbase.sh进入shell界面bin/hbase shell HBase Shell 基本使用 MAPREDUCEMapReduce理解 HIVE采用MySQL数据库保存Hive的元数据，而不是采用Hive自带的derby来存储元数据。 hive的启动(先启动hadoop)： 1234cd /usr/local/hadoop./sbin/start-all.shcd /usr/local/hive./bin/hive hive基本命令 (sql语言) Hive简单编程实践1.先上传本地文件到HDFS中(参考前面的) 2.进入HIVE 1234567create table docs(line string);load data inpath '文件夹或者文件目录' overwrite into table docs;create table word_count as select word, count(1) as count from(select explode(split(line,' '))as word from docs) wgroup by wordorder by word; 执行后，用select语句查看，结果如下：]]></content>
      <tags>
        <tag>BD</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习]]></title>
    <url>%2F2019%2F10%2F14%2FMySQL%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[名词 VIM一些命令 退出vim的快捷键，不需要进入命令编辑模式 按住shift CTRL+Z 退出 zz 保存退出 zq 不保存退出，q表示放弃 之所以按住shift，其实是切换大小写 在命令编辑模式下： :q 不保存退出 :q! 不保存强制退出 :wq 保存退出，w表示写入，不论是否修改，都会更改时间戳 :x 保存退出，如果内容未改，不会更改时间戳 DB(database) DBMS(DB Management System)数据库管理系统，C/S 客户端/服务端 SQL(Structure Query L)语言 SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。 \1. 数据查询语言DQL数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：SELECT &lt;字段名表&gt;FROM &lt;表或视图名&gt;WHERE &lt;查询条件&gt; 2 .数据操纵语言DML数据操纵语言DML主要有三种形式：1) 插入：INSERT2) 更新：UPDATE3) 删除：DELETE \3. 数据定义语言DDL数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：CREATE TABLE/VIEW/INDEX/SYN/CLUSTER| | | | |表 视图 索引 同义词 簇 DDL操作是隐性提交的！不能rollback \4. 数据控制语言DCL数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如：1) GRANT：授权。 2) ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK; 3) COMMIT [WORK]：提交。 列—&gt;字段 行–&gt;记录 DBA数据库管理员(职务) cmd进sql C:\Windows\system32&gt;mysql -h localhost -P3306 -u root -p 连接本机可以简化为 C:\Windows\system32&gt;mysql -u root -p sql命令后面加; mysql中+只作为 数值加法 CONCAT( , )是拼接 DESC 表名 ； 显示表的结构 MySQL的常见命令12345678910111213141516171.查看当前所有的数据库show databases;2.打开指定的库use 库名;3.查看当前库的所有表show tables;4.查看其它库的所有表show tables from 库名;5.创建表create table 表名( 列名 列类型, 列名 列类型， 。。。);6.查看表结构desc 表名; 12345677.查看服务器的版本方式一：登录到mysql服务端select version();方式二：没有登录到mysql服务端mysql --version或mysql --V MySQL的语法规范12345671.不区分大小写,但建议关键字大写， 表名、列名小写2.每条命令最好用分号结尾3.每条命令根据需要，可以进行缩进 或换行4.注释 单行注释：#注释文字 单行注释：-- 注释文字 多行注释：/* 注释文字 */ SQL的常见命令1234567891011show databases； 查看所有的数据库use 库名； 打开指定 的库show tables ; 显示库中的所有表show tables from 库名;显示指定库中的所有表create table 表名( 字段名 字段类型, 字段名 字段类型); 创建表desc 表名; 查看指定表的结构select * from 表名;显示表中的所有数据 DQL语言的学习进阶1：基础查询123456789语法：SELECT 要查询的东西【FROM 表名】;特点：①通过select查询完的结果 ，是一个虚拟的表格，不是真实存在② 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数可以直接双击表 不用打字了!F12可以整理格式 进阶2：条件查询1234567891011121314151617181920212223242526272829条件查询：根据条件过滤原始表的数据，查询到想要的数据语法：select 要查询的字段|表达式|常量值|函数from 表where 条件 ;分类：一、条件表达式 示例：salary&gt;10000 条件运算符： &gt; &lt; &gt;= &lt;= = != &lt;&gt;二、逻辑表达式示例：salary&gt;10000 &amp;&amp; salary&lt;20000逻辑运算符： and（&amp;&amp;）:两个条件如果同时成立，结果为true，否则为false or(||)：两个条件只要有一个成立，结果为true，否则为false not(!)：如果条件成立，则not后为false，否则为true三、模糊查询示例：last_name like &apos;a%&apos;IS NULL IS NOT NULLIN安全等于&lt;=&gt; 进阶3：排序查询123456789语法：select 要查询的东西from 表where 条件order by 排序的字段|表达式|函数|别名 【asc|desc】 进阶4：常见函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849一、单行函数1、字符函数 concat拼接 substr截取子串 upper转换成大写 lower转换成小写 trim去前后指定的空格和字符 ltrim去左边空格 rtrim去右边空格 replace替换 lpad左填充 rpad右填充 instr返回子串第一次出现的索引 length 获取字节个数 2、数学函数 round 四舍五入 rand 随机数 floor向下取整 ceil向上取整 mod取余 truncate截断3、日期函数 now当前系统日期+时间 curdate当前系统日期 curtime当前系统时间 str_to_date 将字符转换成日期 date_format将日期转换成字符 datadiff(&apos;2019-02-01&apos;,&apos;2019-1-1&apos;)前面的日期减后面的 4、流程控制函数 if(判断条件，1，0) case 要判断的字段和表达式 when 常量1 then 要显示的值1或语句1 when 常量2 then 要显示的值2或语句2 else 要显示的值n end (在最后一条后面加；) 多重if case when 条件1 then 要显示的值1或语句1 when 条件2 then 要显示的值2或语句2 else 要显示的值n或语句n end 5、其他函数 version版本 database当前库 user当前连接用户 ​ 二、分组函数 1234567891011121314151617181920sum 求和max 最大值min 最小值avg 平均值count 计数特点：1、以上五个分组函数都忽略null值，除了count(*)2、sum和avg一般用于处理数值型 max、min、count可以处理任何数据类型 3、都可以搭配distinct使用，用于统计去重后的结果(只能去重一个)4、count的参数可以支持： 字段、*、常量值，一般放1 建议使用 count(*) -----可以用来统计个数 eg：查询部门编号为90的员工个数 SELECT COUNT(*) 个数 FROM employees WHERE department_id = 90;6、和分组函数一同查询的字段有限制（意义上的）要求是 group by 后面的字段 进阶5：分组查询1234语法：select 查询的字段，分组函数from 表group by 分组的字段 ​ 特点：​ 1、可以按单个字段分组​ 2、和分组函数一同查询的字段最好是分组后的字段​ 3、分组筛选​ 针对的表 位置 关键字​ 分组前筛选： 原始表 group by的前面 where​ 分组后筛选： 分组后的结果集 group by的后面 having​​ 4、可以按多个字段分组，字段之间用逗号隔开​ 5、可以支持排序​ 6、having后可以支持别名 进阶6：多表连接查询12笛卡尔乘积：如果连接条件省略或无效则会出现解决办法：添加上连接条件 一、传统模式下的连接 ：等值连接——非等值连接 12341.等值连接的结果 = 多个表的交集2.n表连接，至少需要n-1个连接条件3.多个表不分主次，没有顺序要求4.一般为表起别名，提高阅读性和性能 二、sql99语法：通过join关键字实现连接 123456789101112131415161718含义：1999年推出的sql语法支持：等值连接、非等值连接 （内连接）外连接交叉连接语法：select 字段，...from 表1【inner|left outer|right outer|cross】join 表2 on 连接条件【inner|left outer|right outer|cross】join 表3 on 连接条件【where 筛选条件】【group by 分组字段】【having 分组后的筛选条件】【order by 排序的字段或表达式】好处：语句上，连接条件和筛选条件实现了分离，简洁明了！ ​三、自连接 案例：查询员工名和直接上级的名称 sql99 123SELECT e.last_name,m.last_nameFROM employees eJOIN employees m ON e.`manager_id`=m.`employee_id`; sql92 123SELECT e.last_name,m.last_nameFROM employees e,employees m WHERE e.`manager_id`=m.`employee_id`; 进阶8：分页查询应用场景： 1实际的web项目中需要根据用户的需求提交对应的分页查询的sql语句 语法： 1234567select 字段|表达式,...from 表【where 条件】【group by 分组字段】【having 条件】【order by 排序的字段】limit 【起始的条目索引，】条目数; 特点： 123456781.起始条目索引从0开始2.limit子句放在查询语句的最后3.公式：select * from 表 limit （page-1）*sizePerPage,sizePerPage假如:每页显示条目数sizePerPage要显示的页数 page 进阶9：联合查询引入： union 联合、合并 语法： 12345select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】.....select 字段|常量|表达式|函数 【from 表】 【where 条件】 特点： 1231、多条查询语句的查询的列数必须是一致的2、多条查询语句的查询的列的类型几乎相同3、union代表去重，union all代表不去重 黑马程序员123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595-- 数据库的操作 -- 链接数据库 mysql -uroot -p mysql -uroot -pmysql -- 退出数据库 exit/quit/ctrl+d -- sql语句最后需要有分号;结尾 -- 显示数据库版本 select version(); -- 显示时间 select now(); -- 查看所有数据库 show databases; -- 创建数据库 -- create database 数据库名 charset=utf8; create database python04; create database python04new charset=utf8; -- 查看创建数据库的语句 -- show crate database .... show create database python04; -- 查看当前使用的数据库 select database(); -- 使用数据库 -- use 数据库的名字 use python04new; -- 删除数据库 -- drop database 数据库名; drop database python04;-- 数据表的操作 -- 查看当前数据库中所有表 show tables; -- 创建表 -- auto_increment表示自动增长 -- not null 表示不能为空 -- primary key 表示主键 -- default 默认值 -- create table 数据表名字 (字段 类型 约束[, 字段 类型 约束]); create table xxxxx(id int, name varchar(30)); create table yyyyy(id int primary key not null auto_increment, name varchar(30)); create table zzzzz( id int primary key not null auto_increment, name varchar(30) ); -- 查看表结构 -- desc 数据表的名字; desc xxxxx; -- 创建students表(id、name、age、high、gender、cls_id) create table students( id int unsigned not null auto_increment primary key, name varchar(30), age tinyint unsigned default 0, high decimal(5,2), gender enum(&quot;男&quot;, &quot;女&quot;, &quot;中性&quot;, &quot;保密&quot;) default &quot;保密&quot;, cls_id int unsigned ); insert into students values(0, &quot;老王&quot;, 18, 188.88, &quot;男&quot;, 0); select * from students; -- 创建classes表(id、name) create table classes( id int unsigned not null auto_increment primary key, name varchar(30) ); insert into classes values(0, &quot;python04大神&quot;); select * from classes; -- 查看表的创建语句 -- show create table 表名字; show create table students; -- 修改表-添加字段 -- alter table 表名 add 列名 类型; alter table students add birthday datetime; -- 修改表-修改字段：不重命名版 -- alter table 表名 modify 列名 类型及约束; alter table students modify birthday date; -- 修改表-修改字段：重命名版 -- alter table 表名 change 原名 新名 类型及约束; alter table students change birthday birth date default &quot;2000-01-01&quot;; -- 修改表-删除字段 -- alter table 表名 drop 列名; alter table students drop high; -- 删除表 -- drop table 表名; -- drop database 数据库; -- drop table 数据表; drop table xxxxx; -- 增删改查(curd) -- 增加 -- 全列插入 -- insert [into] 表名 values(...) -- 主键字段 可以用 0 null default 来占位 -- 向classes表中插入 一个班级 insert into classes values(0, &quot;菜鸟班&quot;); +--------+-------------------------------------+------+-----+------------+----------------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------------------------------+------+-----+------------+----------------+ | id | int(10) unsigned | NO | PRI | NULL | auto_increment | | name | varchar(30) | YES | | NULL | | | age | tinyint(3) unsigned | YES | | 0 | | | gender | enum(&apos;男&apos;,&apos;女&apos;,&apos;中性&apos;,&apos;保密&apos;) | YES | | 保密 | | | cls_id | int(10) unsigned | YES | | NULL | | | birth | date | YES | | 2000-01-01 | | +--------+-------------------------------------+------+-----+------------+----------------+ -- 向students表插入 一个学生信息 insert into students values(0, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); insert into students values(null, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); insert into students values(default, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); -- 失败 -- insert into students values(default, &quot;小李飞刀&quot;, 20, &quot;第4性别&quot;, 1, &quot;1990-02-01&quot;); -- 枚举中 的 下标从1 开始 1---“男” 2---&gt;&quot;女&quot;.... insert into students values(default, &quot;小李飞刀&quot;, 20, 1, 1, &quot;1990-02-01&quot;); -- 部分插入 -- insert into 表名(列1,...) values(值1,...) insert into students (name, gender) values (&quot;小乔&quot;, 2); -- 多行插入 insert into students (name, gender) values (&quot;大乔&quot;, 2),(&quot;貂蝉&quot;, 2); insert into students values(default, &quot;西施&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;), (default, &quot;王昭君&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); -- 修改 -- update 表名 set 列1=值1,列2=值2... where 条件; update students set gender=1; -- 全部都改 update students set gender=1 where name=&quot;小李飞刀&quot;; -- 只要name是小李飞刀的 全部的修改 update students set gender=1 where id=3; -- 只要id为3的 进行修改 update students set age=22, gender=1 where id=3; -- 只要id为3的 进行修改 -- 查询基本使用 -- 查询所有列 -- select * from 表名; select * from students; ---定条件查询 select * from students where name=&quot;小李飞刀&quot;; -- 查询 name为小李飞刀的所有信息 select * from students where id&gt;3; -- 查询 name为小李飞刀的所有信息 -- 查询指定列 -- select 列1,列2,... from 表名; select name,gender from students; -- 可以使用as为列或表指定别名 -- select 字段[as 别名] , 字段[as 别名] from 数据表 where ....; select name as 姓名,gender as 性别 from students; -- 字段的顺序 select id as 序号, gender as 性别, name as 姓名 from students; -- 删除 -- 物理删除 -- delete from 表名 where 条件 delete from students; -- 整个数据表中的所有数据全部删除 delete from students where name=&quot;小李飞刀&quot;; -- 逻辑删除 -- 用一个字段来表示 这条信息是否已经不能再使用了 -- 给students表添加一个is_delete字段 bit 类型 alter table students add is_delete bit default 0; update students set is_delete=1 where id=6;-- 数据的准备 -- 创建一个数据库 create database python_test charset=utf8; -- 使用一个数据库 use python_test; -- 显示使用的当前数据是哪个? select database(); -- 创建一个数据表 -- students表 create table students( id int unsigned primary key auto_increment not null, name varchar(20) default &apos;&apos;, age tinyint unsigned default 0, height decimal(5,2), gender enum(&apos;男&apos;,&apos;女&apos;,&apos;中性&apos;,&apos;保密&apos;) default &apos;保密&apos;, cls_id int unsigned default 0, is_delete bit default 0 ); -- classes表 create table classes ( id int unsigned auto_increment primary key not null, name varchar(30) not null );-- 查询 -- 查询所有字段 -- select * from 表名; select * from students; select * from classes; select id, name from classes; -- 查询指定字段 -- select 列1,列2,... from 表名; select name, age from students; -- 使用 as 给字段起别名 -- select 字段 as 名字.... from 表名; select name as 姓名, age as 年龄 from students; -- select 表名.字段 .... from 表名; select students.name, students.age from students; -- 可以通过 as 给表起别名 -- select 别名.字段 .... from 表名 as 别名; select students.name, students.age from students; select s.name, s.age from students as s; -- 失败的select students.name, students.age from students as s; -- 消除重复行 -- distinct 字段 select distinct gender from students;-- 条件查询 -- 比较运算符 -- select .... from 表名 where ..... -- &gt; -- 查询大于18岁的信息 select * from students where age&gt;18; select id,name,gender from students where age&gt;18; -- &lt; -- 查询小于18岁的信息 select * from students where age&lt;18; -- &gt;= -- &lt;= -- 查询小于或者等于18岁的信息 -- = -- 查询年龄为18岁的所有学生的名字 select * from students where age=18; -- != 或者 &lt;&gt; -- 逻辑运算符 -- and -- 18到28之间的所以学生信息 select * from students where age&gt;18 and age&lt;28; -- 失败select * from students where age&gt;18 and &lt;28; -- 18岁以上的女性 select * from students where age&gt;18 and gender=&quot;女&quot;; select * from students where age&gt;18 and gender=2; -- or -- 18以上或者身高查过180(包含)以上 select * from students where age&gt;18 or height&gt;=180; -- not -- 不在 18岁以上的女性 这个范围内的信息 -- select * from students where not age&gt;18 and gender=2; select * from students where not (age&gt;18 and gender=2); -- 年龄不是小于或者等于18 并且是女性 select * from students where (not age&lt;=18) and gender=2; -- 模糊查询 -- like -- % 替换1个或者多个 -- _ 替换1个 -- 查询姓名中 以 &quot;小&quot; 开始的名字 select name from students where name=&quot;小&quot;; select name from students where name like &quot;小%&quot;; -- 查询姓名中 有 &quot;小&quot; 所有的名字 select name from students where name like &quot;%小%&quot;; -- 查询有2个字的名字 select name from students where name like &quot;__&quot;; -- 查询有3个字的名字 select name from students where name like &quot;__&quot;; -- 查询至少有2个字的名字 select name from students where name like &quot;__%&quot;; -- rlike 正则 -- 查询以 周开始的姓名 select name from students where name rlike &quot;^周.*&quot;; -- 查询以 周开始、伦结尾的姓名 select name from students where name rlike &quot;^周.*伦$&quot;; -- 范围查询 -- in (1, 3, 8)表示在一个非连续的范围内 -- 查询 年龄为18、34的姓名 select name,age from students where age=18 or age=34; select name,age from students where age=18 or age=34 or age=12; select name,age from students where age in (12, 18, 34); -- not in 不非连续的范围之内 -- 年龄不是 18、34岁之间的信息 select name,age from students where age not in (12, 18, 34); -- between ... and ...表示在一个连续的范围内 -- 查询 年龄在18到34之间的的信息 select name, age from students where age between 18 and 34; -- not between ... and ...表示不在一个连续的范围内 -- 查询 年龄不在在18到34之间的的信息 select * from students where age not between 18 and 34; select * from students where not age between 18 and 34; -- 失败的select * from students where age not (between 18 and 34); -- 空判断 -- 判空is null -- 查询身高为空的信息 select * from students where height is null; select * from students where height is NULL; select * from students where height is Null; -- 判非空is not null select * from students where height is not null;-- 排序 -- order by 字段 -- asc从小到大排列，即升序 -- desc从大到小排序，即降序 -- 查询年龄在18到34岁之间的男性，按照年龄从小到到排序 select * from students where (age between 18 and 34) and gender=1; select * from students where (age between 18 and 34) and gender=1 order by age; select * from students where (age between 18 and 34) and gender=1 order by age asc; -- 查询年龄在18到34岁之间的女性，身高从高到矮排序 select * from students where (age between 18 and 34) and gender=2 order by height desc; -- order by 多个字段 -- 查询年龄在18到34岁之间的女性，身高从高到矮排序, 如果身高相同的情况下按照年龄从小到大排序 select * from students where (age between 18 and 34) and gender=2 order by height desc,id desc; -- 查询年龄在18到34岁之间的女性，身高从高到矮排序, 如果身高相同的情况下按照年龄从小到大排序, -- 如果年龄也相同那么按照id从大到小排序 select * from students where (age between 18 and 34) and gender=2 order by height desc,age asc,id desc; -- 按照年龄从小到大、身高从高到矮的排序 select * from students order by age asc, height desc;-- 聚合函数 -- 总数 -- count -- 查询男性有多少人，女性有多少人 select * from students where gender=1; select count(*) from students where gender=1; select count(*) as 男性人数 from students where gender=1; select count(*) as 女性人数 from students where gender=2; -- 最大值 -- max -- 查询最大的年龄 select age from students; select max(age) from students; -- 查询女性的最高 身高 select max(height) from students where gender=2; -- 最小值 -- min -- 求和 -- sum -- 计算所有人的年龄总和 select sum(age) from students; -- 平均值 -- avg -- 计算平均年龄 select avg(age) from students; -- 计算平均年龄 sum(age)/count(*) select sum(age)/count(*) from students; -- 四舍五入 round(123.23 , 1) 保留1位小数 -- 计算所有人的平均年龄，保留2位小数 select round(sum(age)/count(*), 2) from students; select round(sum(age)/count(*), 3) from students; -- 计算男性的平均身高 保留2位小数 select round(avg(height), 2) from students where gender=1; -- select name, round(avg(height), 2) from students where gender=1;-- 分组 -- group by -- 按照性别分组,查询所有的性别 select name from students group by gender; select * from students group by gender; select gender from students group by gender; -- 失败select * from students group by gender; -- 计算每种性别中的人数 select gender,count(*) from students group by gender; -- 计算男性的人数 select gender,count(*) from students where gender=1 group by gender; -- group_concat(...) -- 查询同种性别中的姓名 select gender,group_concat(name) from students where gender=1 group by gender; select gender,group_concat(name, age, id) from students where gender=1 group by gender; select gender,group_concat(name, &quot;_&quot;, age, &quot; &quot;, id) from students where gender=1 group by gender; -- having -- 查询平均年龄超过30岁的性别，以及姓名 having avg(age) &gt; 30 select gender, group_concat(name),avg(age) from students group by gender having avg(age)&gt;30; -- 查询每种性别中的人数多于2个的信息 select gender, group_concat(name) from students group by gender having count(*)&gt;2;-- 分页 -- limit start, count -- 限制查询出来的数据个数 select * from students where gender=1 limit 2; -- 查询前5个数据 select * from students limit 0, 5; -- 查询id6-10（包含）的书序 select * from students limit 5, 5; -- 每页显示2个，第1个页面 select * from students limit 0,2; -- 每页显示2个，第2个页面 select * from students limit 2,2; -- 每页显示2个，第3个页面 select * from students limit 4,2; -- 每页显示2个，第4个页面 select * from students limit 6,2; -- -----&gt; limit (第N页-1)*每个的个数, 每页的个数; -- 每页显示2个，显示第6页的信息, 按照年龄从小到大排序 -- 失败select * from students limit 2*(6-1),2; -- 失败select * from students limit 10,2 order by age asc; select * from students order by age asc limit 10,2; select * from students where gender=2 order by height desc limit 0,2;-- 连接查询 -- inner join ... on -- select ... from 表A inner join 表B; select * from students inner join classes; -- 查询 有能够对应班级的学生以及班级信息 select * from students inner join classes on students.cls_id=classes.id; -- 按照要求显示姓名、班级 select students.*, classes.name from students inner join classes on students.cls_id=classes.id; select students.name, classes.name from students inner join classes on students.cls_id=classes.id; -- 给数据表起名字 select s.name, c.name from students as s inner join classes as c on s.cls_id=c.id; -- 查询 有能够对应班级的学生以及班级信息，显示学生的所有信息，只显示班级名称 select s.*, c.name from students as s inner join classes as c on s.cls_id=c.id; -- 在以上的查询中，将班级姓名显示在第1列 select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id; -- 查询 有能够对应班级的学生以及班级信息, 按照班级进行排序 -- select c.xxx s.xxx from student as s inner join clssses as c on .... order by ....; select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id order by c.name; -- 当时同一个班级的时候，按照学生的id进行从小到大排序 select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id order by c.name,s.id; -- left join -- 查询每位学生对应的班级信息 select * from students as s left join classes as c on s.cls_id=c.id; -- 查询没有对应班级信息的学生 -- select ... from xxx as s left join xxx as c on..... where ..... -- select ... from xxx as s left join xxx as c on..... having ..... select * from students as s left join classes as c on s.cls_id=c.id having c.id is null; select * from students as s left join classes as c on s.cls_id=c.id where c.id is null; -- right join on -- 将数据表名字互换位置，用left join完成-- 自关联 -- 省级联动 url:http://demo.lanrenzhijia.com/2014/city0605/ -- 查询所有省份 select * from areas where pid is null; -- 查询出山东省有哪些市 select * from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;山东省&quot;; select province.atitle, city.atitle from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;山东省&quot;; -- 查询出青岛市有哪些县城 select province.atitle, city.atitle from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;青岛市&quot;; select * from areas where pid=(select aid from areas where atitle=&quot;青岛市&quot;)-- 子查询 -- 标量子查询 -- 查询出高于平均身高的信息 -- 查询最高的男生信息 select * from students where height = 188; select * from students where height = (select max(height) from students); -- 列级子查询 -- 查询学生的班级号能够对应的学生信息 -- select * from students where cls_id in (select id from classes); Python 中操作 MySQL 步骤 引入模块 在py文件中引入pymysql模块 1from pymysql import * Connection 对象 用于建立与数据库的连接 创建对象：调用connect()方法 1conn=connect(参数列表) 参数host：连接的mysql主机，如果本机是’localhost’ 参数port：连接的mysql主机的端口，默认是3306 参数database：数据库的名称 参数user：连接的用户名 参数password：连接的密码 参数charset：通信采用的编码方式，推荐使用utf8 对象的方法 close()关闭连接 commit()提交 cursor()返回Cursor对象，用于执行sql语句并获得结果 Cursor对象 用于执行sql语句，使用频度最高的语句为select、insert、update、delete 获取Cursor对象：调用Connection对象的cursor()方法 1cs1=conn.cursor() 对象的方法 close()关闭 execute(operation [, parameters ])执行语句，返回受影响的行数，主要用于执行insert、update、delete语句，也可以执行create、alter、drop等语句 fetchone()执行查询语句时，获取查询结果集的第一个行数据，返回一个元组 fetchall()执行查询时，获取结果集的所有行，一行构成一个元组，再将这些元组装入一个元组返回 对象的属性 rowcount只读属性，表示最近一次execute()执行后受影响的行数 connection获得当前连接对象 MySQL常用操作注意：MySQL中每个命令后都要以英文分号；结尾。1、显示数据库mysql&gt; show databases;MySql刚安装完有两个数据库：mysql和test。mysql库非常重要，它里面有MySQL的系统信息，我们改密码和新增用户，实际上就是用这个库中的相关表进行操作。 2、显示数据库中的表mysql&gt; use mysql; （打开库，对每个库进行操作就要打开此库）Database changedmysql&gt; show tables; 3、显示数据表的结构：describe 表名; 4、显示表中的记录：select * from 表名;例如：显示mysql库中user表中的纪录。所有能对MySQL用户操作的用户都在此表中。select * from user; 5、建库：create database 库名;例如：创建一个名字位aaa的库mysql&gt; create database aaa; 6、建表：use 库名；create table 表名 (字段设定列表)；例如：在刚创建的aaa库中建立表person,表中有id(序号，自动增长)，xm（姓名）,xb（性别）,csny（出身年月）四个字段use aaa;mysql&gt; create table person (id int(3) auto_increment not null primary key, xm varchar(10),xb varchar(2),csny date);可以用describe命令察看刚建立的表结构。mysql&gt; describe person; 7、增加记录例如：增加几条相关纪录。mysql&gt;insert into person values(null,’张三’,’男’,’1997-01-02′);mysql&gt;insert into person values(null,’李四’,’女’,’1996-12-02′);注意，字段的值（’张三’,’男’,’1997-01-02’）是使用两个英文的单撇号包围起来，后面也是如此。因为在创建表时设置了id自增，因此无需插入id字段，用null代替即可。可用select命令来验证结果。mysql&gt; select * from person; 8、修改纪录例如：将张三的出生年月改为1971-01-10mysql&gt; update person set csny=’1971-01-10′ where xm=’张三’; 9、删除纪录例如：删除张三的纪录。mysql&gt; delete from person where xm=’张三’; 10、删库和删表drop database 库名;drop table 表名； 11、查看mysql版本在mysql5.0中命令如下：show variables like ‘version’;或者：select version();]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题目python版]]></title>
    <url>%2F2019%2F09%2F15%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84python-%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[leetcode两个数的和12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dist=&#123;&#125; for i in range(len(nums)): if (target - nums[i]) not in dist: #对健判断 dist[nums[i]] = i # num[i]是键 i是值 else: return [dist[target - nums[i]],i] #返回的是键的=值 总结：Python 字典 in 操作符用于判断键是否存在于字典中 1. 最大子列和问题 给定K个整数组成的序列{ N1, N2, …, N**K }，“连续子列”被定义为{ N**i, N**i+1, …, N**j }，其中 1≤i≤j≤K。“最大子列和”则被定义为所有连续子列元素的和中最大者。例如给定序列{ -2, 11, -4, 13, -5, -2 }，其连续子列{ 11, -4, 13 }有最大的和20。现要求你编写程序，计算给定整数序列的最大子列和。 本题旨在测试各种不同的算法在各种数据情况下的表现。各组测试数据特点如下： 数据1：与样例等价，测试基本正确性； 数据2：102个随机整数； 数据3：103个随机整数； 数据4：104个随机整数； 数据5：105个随机整数； 输入格式: 输入第1行给出正整数K (≤100000)；第2行给出K个整数，其间以空格分隔。 输出格式: 在一行中输出最大子列和。如果序列中所有整数皆为负数，则输出0。 输入样例: 126-2 11 -4 13 -5 -2 程序： 123456789101112num = int(input('请给出正整数K'))N = [int(x) for x in input('请给出K个数').split()]print(N)ThisSum = MaxSum = 0for i in range(num): ThisSum += N[i] if ThisSum &gt; MaxSum: MaxSum = ThisSum elif(ThisSum &lt; 0): ThisSum = 0print(MaxSum) 2.两个有序链表序列的合并 已知两个非降序链表序列S1与S2，设计函数构造出S1与S2合并后的新的非降序链表S3。 输入格式: 输入分两行，分别在每行给出由若干个正整数构成的非降序序列，用−1表示序列的结尾（−1不属于这个序列）。数字用空格间隔。 输出格式: 在一行中输出合并后新的非降序链表，数字间用空格分开，结尾不能有多余空格；若新链表为空，输出NULL。 输入样例: 121 3 5 -12 4 6 8 10 -1 输出样例: 11 2 3 4 5 6 8 10 12345678910111213141516171819202122232425262728293031323334353637class Solution: def Merge(self, pHead1, pHead2): if not pHead1: return pHead2 if not pHead2: return pHead1 if pHead1.val &lt;= pHead2.val: pHead1.next = self.Merge(pHead1.next, pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1, pHead2.next) return pHead2 def getNewChart(self, list): if list: node = ListNode(list.pop(0)) # pop(0)是移除你的words中的第一个元素，并返回 # 被移除的元素的值，也就是说返回的是你words中的第一个元素。 node.next = self.getNewChart(list) return nodeclass ListNode: def __init__(self, x): self.val = x self.next = Noneif __name__ == '__main__': list1 = [1, 3, 5] list2 = [0, 1, 4] testList1 = Solution().getNewChart(list1) testList2 = Solution().getNewChart(list2) final = Solution().Merge(testList1, testList2) while final: print(final.val, end=" ") final = final.next]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法python版]]></title>
    <url>%2F2019%2F09%2F15%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84python%2F</url>
    <content type="text"><![CDATA[1. 基础概念 split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串 split() 方法语法： 1str.split(str="", num=string.count(str)) 链表是一种常见的基础数据结构，结构体指针在这里得到了充分的利用。链表可以动态的进行存储分配，也就是说，链表是一个功能极为强大的数组，他可以在节点中定义多种数据类型，还可以根据需要随意增添，删除，插入节点。链表都有一个头指针，一般以head来表示，存放的是一个地址。链表中的节点分为两类，头结点和一般节点，头结点是没有数据域的。链表中每个节点都分为两部分，一个数据域，一个是指针域。说到这里你应该就明白了，链表就如同车链子一样，head指向第一个元素：第一个元素又指向第二个元素；……，直到最后一个元素，该元素不再指向其它元素，它称为“表尾”，它的地址部分放一个“NULL”（表示“空地址”），链表到此结束。 作为有强大功能的链表，对他的操作当然有许多，比如：链表的创建，修改，删除，插入，输出，排序，反序，清空链表的元素，求链表的长度等等。 初学链表，一般从单向链表开始 12---&gt;NULLhead 这是一个空链表。 12 ----&gt;[p1]----&gt;[p2]...----&gt;[pn]----&gt;[NULL]head p1-&gt;next p2-&gt;next pn-&gt;next 有n个节点的链表。 创建链表 1234typedef struct student&#123; int score; struct student *next;&#125; LinkList; 一般创建链表我们都用typedef struct，因为这样定义结构体变量时，我们就可以直接可以用LinkList *a;定义结构体类型变量了。 初始化一个链表，n为链表节点个数。 12345678910111213LinkList *creat(int n)&#123; LinkList *head, *node, *end;//定义头节点，普通节点，尾部节点； head = (LinkList*)malloc(sizeof(LinkList));//分配地址 end = head; //若是空链表则头尾节点一样 for (int i = 0; i &lt; n; i++) &#123; node = (LinkList*)malloc(sizeof(LinkList)); //动态内存申请，将结构指针变成结构体变量 scanf("%d", &amp;node-&gt;score); end-&gt;next = node; end = node; &#125; end-&gt;next = NULL;//结束创建 return head;&#125; 修改链表节点值 修改链表节点值很简单。下面是一个传入链表和要修改的节点，来修改值的函数。 123456789101112131415void change(LinkList *list,int n) &#123;//n为第n个节点 LinkList *t = list; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; puts("输入要修改的值"); scanf("%d", &amp;t-&gt;score); &#125; else &#123; puts("节点不存在"); &#125;&#125; 删除链表节点 删除链表的元素也就是把前节点的指针域越过要删除的节点指向下下个节点。即：p-&gt;next = q-&gt;next;然后放出q节点的空间，即free(q); 12345678910111213141516void delet(LinkList *list, int n) &#123; LinkList *t = list, *in; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; in = t; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; in-&gt;next = t-&gt;next; free(t); &#125; else &#123; puts("节点不存在"); &#125;&#125; 插入链表节点 我们可以看出来，插入节点就是用插入前节点的指针域链接上插入节点的数据域，再把插入节点的指针域链接上插入后节点的数据域。根据图，插入节点也就是：e-&gt;next = head-&gt;next; head-&gt;next = e; 增加链表节点用到了两个结构体指针和一个int数据。 123456789101112131415161718void insert(LinkList *list, int n) &#123; LinkList *t = list, *in; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; in = (LinkList*)malloc(sizeof(LinkList)); puts("输入要插入的值"); scanf("%d", &amp;in-&gt;score); in-&gt;next = t-&gt;next;//填充in节点的指针域，也就是说把in的指针域指向t的下一个节点 t-&gt;next = in;//填充t节点的指针域，把t的指针域重新指向in &#125; else &#123; puts("节点不存在"); &#125;&#125; 输出链表 输出链表很简单，边遍历边输出就行了。 123 while (h-&gt;next != NULL) &#123;h = h-&gt;next;printf("%d ", h-&gt;score); python 初始化链表/列表输入 1.只有一个整数：a = int(input) 2.一行多个整数并用空格分开：a,b = map(int,input().split()) 3.数据较多时可用 列表存储：num = list(map(int,input().split())) 4.关于初始化链表： 12345678910class Node: def __init__(self,x): self.val = x self.next = Nonenum = list(map(int,input().split(','))) #假设输入的每个元素按逗号隔开node = Node(-1)tep = nodefor i in num: tep.next = Node(i) tep = tep.next 栈 栈的存储结构通常由一个一维数组和一个记录栈顶元素位置的变量组成。LIFO 栈的顺序存储结构通常由一个一维数组和一个记录栈顶元素位置的变量组成 队列 队列：具有一定操作约束的线性表，一端插入，另一端删除.FIFO 队列的顺序存储结构通常由一个一维数组和一个记录队列头元素位置的变量front以及一个记录队列尾元素位置的变量rear组成。 树 儿子-兄弟表示法 旋转45后又叫二叉树 每个都是两个指针域 满二叉树： 除最后一层无任何子节点外，每一层上的所有结点都有两个子结点二叉树。 国内教程定义：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为K，且结点总数是(2^k) -1 ，则它就是满二叉树。 节点： 就是一个图中的0、1、2~~14，这些就叫节点。 叶子节点： 就是没有子节点的节点，比如图中的7、8、9~~14这些，0、1、2、3这些就不是叶子节点。 拓展：二叉树相关术语 树的结点（node）：包含一个数据元素及若干指向子树的分支； 孩子结点（child node）：结点的子树的根称为该结点的孩子； 双亲结点：B 结点是A 结点的孩子，则A结点是B 结点的双亲； 兄弟结点：同一双亲的孩子结点； 堂兄结点：同一层上结点； 祖先结点: 从根到该结点的所经分支上的所有结点子孙结点：以某结点为根的子树中任一结点都称为该结点的子孙 结点层：根结点的层定义为1；根的孩子为第二层结点，依此类推； 树的深度：树中最大的结点层 结点的度：结点子树的个数 树的度： 树中最大的结点度。 叶子结点：也叫终端结点，是度为 0 的结点； 分枝结点：度不为0的结点； 有序树：子树有序的树，如：家族树； 无序树：不考虑子树的顺序； 注意在POP后T为栈顶值。 二叉搜索树（BST， Binary Search Tree），也称二叉排序树或二叉查找树 二叉搜索树：一棵二叉树，可以为空；如果不为空，满足以下性质： 非空左子树的所有键值小于其根结点的键值。 非空右子树的所有键值大于其根结点的键值。 左、右子树都是二叉搜索树。 平衡二叉树 平均查找长度ASL “平衡因子（ Balance Factor，简称BF） : BF(T) = hL-hR，其中hL和hR分别为T的左、右子树的高度。 平衡二叉树（ Balanced Binary Tree）（ AVL树）空树，或者任一结点左、右子树高度差的绝对值不超过1，即|BF(T) |≤ 1 图​ 个人以为，BFS就像是再画一个半径为R++的圆，每画一次，这个圆就一点点的扩大，这样的好处在于他能够巨细无遗地扫描到你想要的元素；DFS就像是我们在画阴影的时候的方法，先沿对角线画一条斜线，然后在他的左边或者右边不断地画斜线填充，直到斜线接触到你想要的点；。 ​ 综上而言，在空间效率问题上，小范围而言BFS由于是采用队列的方式二优于DFS的递归方式，但是如果数据量扩大，这个真的不好说，这其中应该会有一个临界值，让两者的效率逆转。 ​ 而在时间效率上的话，一定程度上DFS的时间效率优于BFS。个人认为，BFS的主要作用在于扫描与找方向，DFS的作用主要在于在找到方向之后的建立最短 路径。 1.dfs(深度优先搜索)是两个搜索中先理解并使用的，其实就是暴力把所有的路径都搜索出来，它运用了回溯，保存这次的位置，深入搜索，都搜索完了便回溯回来，搜下一个位置，直到把所有最深位置都搜一遍，要注意的一点是，搜索的时候有记录走过的位置，标记完后可能要改回来； 回溯法是一种搜索法，按条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法； 例如这张图，从1开始到2，之后到5，5不能再走了，退回2，到6，退回2退回1，到3，一直进行； 理解这种方法比较简单，难的是要怎么用 123456789101112131415void dfs(int deep)&#123; int x=deep/n,y=deep%n; if(符合某种要求||已经不能在搜了) &#123; 做一些操作； return ; &#125; if(符合某种条件且有地方可以继续搜索的)//这里可能会有多种条件，可能要循环什么的 &#123; a[x][y]='x';//可能要改变条件，这个是瞎写的 dfs(deep+1,sum+1);//搜索下一层 a[x][y]='.';//可能要改回条件，有些可能不用改比如搜地图上有多少块连续的东西 &#125;&#125; 2.bfs(宽度/广度优先搜索)，这个一直理解了思想，不会用，后面才会的，思想，从某点开始，走四面可以走的路，然后在从这些路，在找可以走的路，直到最先找到符合条件的，这个运用需要用到队列(queue)，需要稍微掌握这个才能用bfs. 还是这张图，从1开始搜，有2，3，4几个点，存起来，从2开始有5，6，存起来，搜3，有7，8，存起来，搜4，没有了；现在开始搜刚刚存的点，从5开始，没有，然后搜6.。。一直进行，直到找到；]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib]]></title>
    <url>%2F2019%2F09%2F05%2Fmatplotlib%2F</url>
    <content type="text"><![CDATA[GUI编程–matplotlib绘图 numpyarray 12345678910111213141516import numpy as npt1 = np.array([1, 2, 3]) # 存放数组t2 = np.array(range(1,4),dtype = float) # 指定数组类型t6 = t5.astype('int8') # 调整数据类型#random.random() 取小数np.round(t7,2) # 取t7里小数的前两位t1.shape # 返回一个元祖 返回元祖的含义注意！ 第一个是0轴，第二个是1轴...t1.reshape((3, 4)) # 转入一个元祖 把1*12 变成3*4t5.flatte() # 把数据转换成一维的 轴 二维数组里 axis=0 代表行 1代表列 numpy读取数据(.csv) np.loadtxt(fram.dtype = np.float,delimiter = None, skipprows = 0, usecoles = None, unpack=False) fram 文件、字符或者生成器，可以是.gz (文件名) delimiter 分隔字符串，默认空格，可改成， skiprows 跳过前x行，一般跳过第一行表头 usecols 读取指定的列，索引，元祖类型 unpack 如True读入属性分别写入不同数组变量，False 只写入一个数组变量，默认False。(类似于把矩阵旋转了) t1.transpose()数组转置 或者t1.T 索引 切片 t2[2]取第二行 , 取多行t2[2,6,9] 取行t2[1,:] 取列t2[:,[0,2,7]] numpy中数值修改 t2[t2&lt;10] = 3 np.where(t&lt;10,0,10) 小于10是0，大于10是10 t.clip(10,18) 小于10 是10 大于18是18 数组的拼接 np.vstack((t1,t2)) 竖直拼接 np.hstack((t1,t2)) 水平拼接 行列交换t[[1,2],:] = t[[2,1],:] 构造全为0的数组 np.zero((us_data.shape[0],1)) 构造全为1的数组 np.ones((us_data.shape[0],1)) 获取最大值最小值的位置np.argmax(t,axis=0) /np.argmin(t,axis=1) 创建一个对角线为1的正方形数组(方阵)：np.eye(3) numpy的注意点copy和view a=b 完全不复制，a和b相互影响 a = b[:],视图的操作，一种切片，会创建新的对象a，但是a的数据完全由b保管，他们两个的数据变化是一致的， a = b.copy(),复制，a和b互不影响 常用的统计函数 求和：t.sum(axis=None) 指定一个轴 均值：t.mean(a,axis=None) 受离群点的影响较大 中值：np.median(t,axis=None) 最大值：t.max(axis=None) 最小值：t.min(axis=None) 极值：np.ptp(t,axis=None) 即最大值和最小值只差 标准差：t.std(axis=None) FuncAnimation动态绘图1、函数FuncAnimation(fig,func,frames,init_func,interval,blit)是绘制动图的主要函数，其参数如下： a.fig 绘制动图的画布名称 b.func自定义动画函数，即下边程序定义的函数update c.frames动画长度，一次循环包含的帧数，在函数运行时，其值会传递给函数update(n)的形参“n” d.init_func自定义开始帧，即传入刚定义的函数init,初始化函数 e.interval更新频率，以ms计 f.blit选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显 12345678910111213141516171819202122import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.animation import FuncAnimationfig, ax = plt.subplots() #生成子图，相当于fig = plt.figure(),ax = fig.add_subplot(),其中ax的函数参数表示把当前画布进行分割，例：fig.add_subplot(2,2,2).表示将画布分割为两行两列 #ax在第2个子图中绘制，其中行优先，xdata, ydata = [], [] #初始化两个数组ln, = ax.plot([], [], 'r-', animated=False) #第三个参数表示画曲线的颜色和线型，具体参见：https://blog.csdn.net/tengqingyong/article/details/78829596def init(): ax.set_xlim(0, 2*np.pi) #设置x轴的范围pi代表3.14...圆周率， ax.set_ylim(-1, 1) #设置y轴的范围 return ln, #返回曲线def update(n): xdata.append(n) #将每次传过来的n追加到xdata中 ydata.append(np.sin(n)) ln.set_data(xdata, ydata) #重新设置曲线的值 return ln,ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 10), #这里的frames在调用update函数是会将frames作为实参传递给“n” init_func=init, blit=True)plt.show() PS:一般来说一个动图有两类函数，一类是初始化函数，另一类是需要更新的函数！！ Figure构造器参数说明 class matplotlib.figure.Figure( figsize=None, #Figure的大小，单位是英寸 dpi=None, #分辨率（每英寸的点数） facecolor=None, #修饰的颜色 edgecolor=None, #边界颜色 linewidth=0.0, #线条宽度 frameon=None, #布尔值，是否绘制框架（Frame） subplotpars=None, #子图的参数 tight_layout=None, #取值布尔或者字典，缺省自动布局，False 使用 subplotpars参数，True就使用tight_layout，如果是字典，则包含如下字段：pad, w_pad, h_pad, 与 rect constrained_layout=None) #True就使用constrained_layout，会自动调整plot的位置。]]></content>
      <tags>
        <tag>pyqt</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[柔性负载-杨明]]></title>
    <url>%2F2019%2F08%2F25%2F%E6%9F%94%E6%80%A7%E8%B4%9F%E8%BD%BD-%E6%9D%A8%E6%98%8E%2F</url>
    <content type="text"><![CDATA[关于杨明老师柔性负载的文献综述制作人：颜世伟 制作时间：2019.8 --- 一、机械谐振建模 [1] 电机和执行机构通过传动轴系联接，传动轴系具有一定的抗扭刚度 K 和阻尼系数 Cw。当传动轴系发生扭转形变时轴系将产生转矩 Tw，此转矩对于电机来说可看作是电机的负载转矩，而对于执行机构来说可看作是驱动转矩。伺服驱动器控制电机运行，为电机的转轴提供电磁转矩 Te。在电机端电磁转矩 Te和传动轴系转矩 Tw作用于转动惯量为 J1、阻尼系数为 C1的电机转轴。在执行机构端，执行机构具有大小为 J2的等效转动惯量以及阻尼系数 C2，传动轴系转矩 Tw与负载转矩Tl共同作用于执行机构最终决定了负载转速。 系统中的阻尼系数很小，可忽略阻尼系数从而对系统模型进行化简得到 传动装置的数学表达： 实际系统模型框图： 根据系统的传递函数画处Bode图(A: ω1/ ωref传递函数的幅频特性曲线. B：ω2/ ω1 . C为A+B) 可以看出闭环系统中存在一个谐振点，系统对于此频率点的响应比较强烈，存在机械谐振。并且谐振频率受系统中的机械谐振频率和振幅主要受到负载转动惯量 J2和传动轴扭转弹性系数 K 两个参数影响. 二、双惯量弹性伺服系统外部机械参数辨识方法综述[5] 三、谐振机理分析及谐振特征快速辨识[6]目的：通过辨识的谐振频率可以确定陷波滤波器参数 主要思路：首先对比连续系统开、闭环幅频特性与固有谐振特征的定量关系，确定谐振模式; 进一步针对离散系统，分析控制器刚度对离散闭环系统谐振的影响，确定离散系统持续振荡状态下谐振频率即为 NTF (共轭极点为自然振动频率点)频率。 谐振原理及模型 电机转速与电机电磁转矩之间的传递函数： 从上式看出，机械谐振点在传递函数上引入了一对共轭的零极点，共轭零点为抗谐振频率点 AＲF( anti-resonance frequency) ，共轭极点为自然振动频率点 NTF( natural torsional frequency)。 系统的谐振特征 1.开环系统的谐振(速度开环，电流闭环) 计算得系统的阶跃响应为 斜坡输出的基础上叠加 NTF 频率的振荡，振荡幅值与NTF 频率成反比。而且振荡频率值与负载转矩无关，所以在分析谐振频率时可以不考虑负载转矩对系统的影响。证明在速度开环情况下，系统会以NTF 谐振频率振荡。 2.闭环系统的振荡(电流环近似为1) 速度控制器的传递函数为下式 其特征方程可以化简为 系统的闭环带宽主要受到min(w1,w2),且min(w1,w2)&lt;wARF(共轭零点的谐振频率)。 进一步画出弹性系统 wm/ Te、开环系统及闭环系统的幅频曲线如下： 通过弹性虚脱的幅频特性可以看出：在速度闭环系统中，由于受到闭环控制作用的影响，NTF 频率大于 0 dB 的增益会被明显抑制，所以此时的谐振主要是以接近 AＲF 频率的振荡频率 fe在振动，而且该频率振动也会逐渐地趋于稳定。 带有速度输出限幅的闭环系统响应如下 可以看出柔性负载引入对系统的最显著影响就是降低了系统的带宽，使得系统无法进一步提高性能。振荡频率明显分为两段: 当速度调节器饱和，系统处于速度控制开环阶段，此时电机和负载侧都以 NTF 频率振荡; 当电机速度达到给定速度，速度调节器退饱和，进入速度控制闭环阶段。在对于大惯量系统控制器刚度随之较大时，该阶段就能以AＲF 谐振频率衰减振荡，直至转速达到给定。 谐振特征辨识 将信号(伪随机序列信号/Chirp信号)幅值变为一倍额定电流值输入作为 q 轴给定输入系统。 再通过计算每个频率处的给定信号与激励信号的幅值比和相位差就可以得到被测系统的幅频和相频特性。系统的幅频及相频特性可按下式计算。按上述方式就可以绘制出系统的频率特性 Bode 图。 结论:1) 闭环系统的带宽受到弹性系统的限制，加大连续系统刚度只会使系统带宽及谐振频率趋近ARF谐振频率。所以在大惯量伺服系统中，由于控制器刚度较高，可以将谐振频率近似为 ARF频率。2) 由于刚度的增加可能会使离散闭环系统的稳定裕度为负，进入发散状态。由于速度限幅的作用，使系统进入非线性振荡状态。此时振荡频率为NTF 频率叠加二分之一采样频率，经过采样滤波的实际系统体现的就为 NTF 谐振频率。 参考文献[1] 杨明, 胡浩, 徐殿国. 永磁交流伺服系统机械谐振成因及其抑制[J]. 电机与控制学报, 2012, 16(1):79-84. [2] 杨明, 郝亮, 徐殿国. 基于自适应陷波滤波器的在线机械谐振抑制[J]. 哈尔滨工业大学学报, 2014, 46(4):63-69. [3] 王璨, 杨明, 徐殿国. 基于PI控制的双惯量弹性系统机械谐振的抑制[J]. 电气传动, 2015(1). [4] 杨明, 王璨, 徐殿国. 基于轴矩限幅控制的机械谐振抑制技术[J]. 电机与控制学报, 2015, 19(4):58-64. [5] 王璨, 杨明, 栾添瑞. 双惯量弹性伺服系统外部机械参数辨识综述[J]. 中国电机工程学报, 2016, 36(3):804-817. [6] 杨明, 郝亮, 徐殿国. 双惯量弹性负载系统机械谐振机理分析及谐振特征快速辨识[J]. 电机与控制学报, 2016, 20(04):112-120. [7] 郎志, 杨明, 徐殿国. 双惯量弹性系统负载扰动观测器设计研究[J]. 电工技术学报, 2016(S2):90-97. [8] Beinke S，Wertz H，Schutte F，et al．Identification of nonlinear two-mass systems for self-commissioning speed control of electrical drives[C]//Proceedings of the 24th Annual Conference of the IEEE Industrial Electronics Society．Aachen：IEEE，1998：2251-2256． [9] GuoY J，HuangL P，Muramatsu M．Research on inertia identification and auto-tuning of speed controller for AC servo system[C]//Proceedings of the Power Conversion Conference．Osaka：IEEE，2002：896-901 [10] Östring M ， Gunnarsson S ， Norrlöf M ． Closed-loop identification of an industrial robot containing flexibilities [J]．Control Engineering Practice，2003，11(3)：291-300 [11] Östring M．Closed loop identification of the physical parameters of an industrial robot[C]//Proceedings of the 32nd International Symposium on Robotics ． Seoul ，Korea，2000． [12]Dhaouadi R，KuboK．Transfer function and parameters identification of a motor drive system using adaptive filtering[C]//Proceedings of the 4th International Workshop on Advanced Motion Control．Mie：IEEE，1996：588-593． [13] Eker I，Vural M．Experimental online identification of a three-mass mechanical system[C]//Proceedings of 2003 IEEE Conference on Control Applications．Istanbul，Turkey：IEEE，2003：60-65． [14] Landau I D，Karimi A．An extended output error recursive algorithm for identification in closed loop[C]//Proceedings of the 35th IEEE Conference on Decision and Control．Kobe：IEEE，1996：1405-1410． [15] Eker I，Vural M．Experimental online identification of a three-mass mechanical system[C]//Proceedings of 2003 IEEE Conference on Control Applications．Istanbul，Turkey：IEEE，2003：60-65． [16] Landau I D，Karimi A．An extended output error recursive algorithm for identification in closed loop[C]//Proceedings of the 35th IEEE Conference on Decision and Control．Kobe：IEEE，1996：1405-1410． [17] Zoubek H，Pacas M．A method for speed-sensorless identification of two-mass-systems[C]//Proceedings of the 2010 IEEE Energy Conversion Congress and Exposition．Atlanta，GA：IEEE，2010：4461-4468． [18] YoshiokaY ， HanamotoT ． Estimation of a multimass system using the LWTLS and a coefficient diagram for vibration-controller design[J] ． IEEE Transactions on Industry Applications，2008，44(2)：566-574． [19] Villwock S，Pacas M．Application of the Welch-method for the identification of two-and three-mass-systems [J]．IEEE Transactions on Industrial Electronics，2008，55(1)：457-466．]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>柔性负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyqt基础知识]]></title>
    <url>%2F2019%2F08%2F24%2Fpyqt5%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[pqqt基础知识汇总 第一个窗口1、if __name__ == “__main__“: 是代表如果这个文件是主程序这运行下面的代码，如果是被别的程序文件调用的话，则运行下面的代码。 2、__init __ 方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象做一些你希望的 初始化 。注意，这个名称的开始和结尾都是双下划线。 3、生成实例（对象）必须以类名()，别忘记了() 4、类中的函数（方法）必须有self，是代表属于这个实例（对象）本身持有的，而外部定义的函数则不需要。 5、QApplication相当于main函数，也就是整个程序（有很多文件）的主入口函数。 ​ 对于一个Gui程序必须至少有一个这样的一个实例来让程序运行。 6、最后一句是调用sys库的exit退出方法，退出条件（参数）是app.exec_()也就是整个窗口关闭。 Qt Designer1、记得在Qt Designer中窗体的layout层次可以通过对象查看器来查看，layout的一些设置可以通过属性编辑器来修该。 2、通常我们使用栅格布局作为顶层布局，将控件放置好之后可以通过右键–布局–栅格布局，将布局充满整个窗体。 3、我们可以先放入控件，然后ctrl选中多个控件，然后点击工具栏上快速布局工具进行布局。 4、在mianwindows中默认会有个centralwidget布局也是继承自QtWidgets.QWidget，表示窗口的中央部分。 信号和槽信号和槽是一种高级接口，应用于对象之间的通信，它是 QT 的核心特性，也是 QT 区别于其它工具包的重要地方。它为高层次的事件处理自动生成所需要的附加代码。在我们所熟知的很多 GUI 工具包中，窗口小部件 (widget) 都有一个回调函数用于响应它们能触发的每个动作，这个回调函数通常是一个指向某个函数的指针。但是，在 QT 中信号和槽取代了这些凌乱的函数指针，使得我们编写这些通信程序更为简洁明了。所有从 QObject 或其子类 ( 例如 Qwidget) 派生的类都能够包含信号和槽。当对象改变其状态时，信号就由该对象发射 (emit) 出去，这就是对象所要做的全部事情，它不知道另一端是谁在接收这个信号。这就是真正的信息封装，它确保对象被当作一个真正的软件组件来使用。槽用于接收信号，但它们是普通的对象成员函数。一个槽并不知道是否有任何信号与自己相连接。而且，对象并不了解具体的通信机制。你可以将很多信号与单个的槽进行连接，也可以将单个的信号与很多的槽进行连接，甚至于将一个信号与另外一个信号相连接也是可能的，这时无论第一个信号什么时候发射系统都将立刻发射第二个信号。总之，信号与槽构造了一个强大的部件编程机制。 说实话对于像我这样的新手来说看着就蛋疼，想学会它没办法，我们还是简化一下概念吧：所有QObject类都可以使用信号槽，换句话来说继承自pyqt中的类基本上都可以使用信号槽机制。当然非QObject也是可以通过其他一些办法来使用信号槽的。 仅仅有了信号和槽是不行的，我们还需要了解：信号(Signal)、槽(slot)、连接(connect)、动作事件(action)、发射(emit)、发送者、接受者等等一些列的知识。 在PyQt中接受者和发送者必须是个对象（实例）！ PyQt中的控件中提供了很多信号和槽方法，大家可以多多使用Qt Designer 设计参考！ 槽其实就个函数（方法），Qt5中的槽函数不在限定必须是slot，可以是普通的函数、类的普通成员函数、lambda函数等。编译期间就会检查信号与槽是否存在！ 信号的connect连接最好放在__init__析构函数里面，这样只会声明一次连接，如果在类方法（函数中）使用的话，要记得disconnect，否则connect会连接多次，导致程序异常。 信号槽函数不用加 ()，否则可能会导致连接异常。 PyQt信号和槽传递额外参数使用Pyqt编程过程中，经常会遇到给槽函数传递额外参数的情况。但是信号-槽机制只是指定信号如何连接到槽，信号定义的参数被传递给槽，而额外的参数（用户定义）不能直接传递。 而传递额外参数又是很有用处。你可能使用一个槽处理多个组件的信号，有时要传递额外的信息。 一种方法是使用lambda表达式。 1234567891011121314151617181920212223242526272829from PyQt4.QtCore import *from PyQt4.QtGui import * class MyForm(QMainWindow): def __init__(self, parent=None): super(MyForm, self).__init__(parent) button1 = QPushButton('Button 1') button2 = QPushButton('Button 1') button1.clicked.connect(lambda: self.on_button(1)) button2.clicked.connect(lambda: self.on_button(2)) layout = QHBoxLayout() layout.addWidget(button1) layout.addWidget(button2) main_frame = QWidget() main_frame.setLayout(layout) self.setCentralWidget(main_frame) def on_button(self, n): print('Button &#123;0&#125; clicked'.format(n)) if __name__ == "__main__": import sys app = QApplication(sys.argv) form = MyForm() form.show() app.exec_() 解释一下，on_button是怎样处理从两个按钮传来的信号。我们使用lambda传递按钮数字给槽，也可以传递任何其他东西—甚至是按钮组件本身（假如，槽打算把传递信号的按钮修改为不可用） 第2个方法是使用functools里的partial函数。 1button1.clicked.connect(partial(self.on_button, 1))button2.clicked.connect(partial(self.on_button, 2)) 《Rapid GUI Program with Python and QT》 P143例子。 自定义信号emit及传参12345678910111213141516171819202122232425262728293031from f1 import Ui_MainWindowfrom PyQt5 import QtWidgets,QtCoreimport sys,timeclass MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): # 继承QWidget和Ui_MainWindow _signal = QtCore.pyqtSignal() # 定义信号 def __init__(self): super(MyWindow, self).__init__() self.setupUi(self) #加载窗体 self.pushButton.clicked.connect(self.prn) # 按钮1链接到prn槽函数 # self.pushButton_2.clicked.connect(self.prn) self._signal.connect(self.mysignalslot) # 将信号连接到mysignalslot def prn(self): print('打印测试') time.sleep(1) print('延时1秒') self._signal.emit() #发射信号 def mysignalslot(self): # 自定义槽函数 print('我是slot')if __name__ == '__main__': import sys app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() # 创建实例 mywindow.show() # 使用QtWidgets的show()方法 sys.exit(app.exec_()) main.py的程序如上所示。实现功能： 注意：当信号与槽函数的参数数量相同时，它们参数类型要完全一致。信号与槽不能有缺省参数。 当信号的参数与槽函数的参数数量不同时，只能是信号的参数数量多于槽函数的参数数量，且前面相同数量的参数类型应一致，信号中多余的参数会被忽略。此外，在不进行参数传递时，信号槽绑定时也是要求信号的参数数量大于等于槽函数的参数数量。这种情况一般是一个带参数的信号去绑定一个无参数的槽函数。 当然可以出传递的参数类型有很多种：str、int、list、object、float、tuple、dict等等 单个文件打开 QFileDialog.getOpenFileName()多个文件打开 QFileDialog.getOpenFileNames() 文件夹选取 QFileDialog.getExistingDirectory() 文件保存 QFileDialog.getSaveFileName() 123456789101112131415161718192021222324252627282930313233343536373839404142from PyQt5 import QtWidgetsfrom PyQt5.QtWidgets import QFileDialog class MyWindow(QtWidgets.QWidget): def __init__(self): super(MyWindow,self).__init__() self.myButton = QtWidgets.QPushButton(self) self.myButton.setObjectName("myButton") self.myButton.setText("Test") self.myButton.clicked.connect(self.msg) def msg(self): directory1 = QFileDialog.getExistingDirectory(self, "选取文件夹", "C:/") #起始路径 print(directory1) fileName1, filetype = QFileDialog.getOpenFileName(self, "选取文件", "C:/", "All Files (*);;Text Files (*.txt)") #设置文件扩展名过滤,注意用双分号间隔 print(fileName1,filetype) files, ok1 = QFileDialog.getOpenFileNames(self, "多文件选择", "C:/", "All Files (*);;Text Files (*.txt)") print(files,ok1) fileName2, ok2 = QFileDialog.getSaveFileName(self, "文件保存", "C:/", "All Files (*);;Text Files (*.txt)") print(fileName2,ok2) if __name__=="__main__": import sys app=QtWidgets.QApplication(sys.argv) myshow=MyWindow() myshow.show() sys.exit(app.exec_()) 第一个参数parent，用于指定父组件。注意，很多Qt组件的构造函数都会有这么一个parent参数，并提供一个默认值0,这里一般填 self父类； 第三个参数dir，是对话框显示时默认打开的目录，”.”代表程序运行目录,”/“代表当前盘符下根目录，注意，这里跟平台有关，例如windows可填”C:\“等，Linux下填写”/“根目录 第四个参数Filter，是对话框后缀名过滤器，有Image File(.jpg *png)就让他只能显示后缀名是jpg或者是png的文件。Text Files(.txt)代表后缀名为.txt的文件。All Files()则代表是各种类型的文件。如果需要使用多个过滤器，使用&quot;;;&quot;分割，比如`”JPEG Files(.jpg);;PNG Files(*.png)”；` 各种对话框PyQt5提供了一系列标准的对话框，常见的有：消息对话框QMessageBox、颜色对话框QColorDialog、字体对话框QFontDialog、输入对话框QInputDialog以及文件对话框QFileDialog 1. 颜色对话框和字体对话框这两种对话框分别可以让用户进行颜色和字体选择。两者用法相似，所以就放在一起讲了： 123456789101112131415161718192021222324252627282930313233343536373839import sysfrom PyQt5.QtWidgets import QApplication, QWidget, QTextEdit, QColorDialog, QFontDialog, QPushButton, \ QHBoxLayout, QVBoxLayoutclass Demo(QWidget): def __init__(self): super(Demo, self).__init__() self.text_edit = QTextEdit(self) # 1 self.color_btn = QPushButton('Color', self) # 2 self.font_btn = QPushButton('Font', self) self.color_btn.clicked.connect(lambda: self.open_dialog_func(self.color_btn)) self.font_btn.clicked.connect(lambda: self.open_dialog_func(self.font_btn)) self.h_layout = QHBoxLayout() self.h_layout.addWidget(self.color_btn) self.h_layout.addWidget(self.font_btn) self.v_layout = QVBoxLayout() self.v_layout.addWidget(self.text_edit) self.v_layout.addLayout(self.h_layout) self.setLayout(self.v_layout) def open_dialog_func(self, btn): if btn == self.color_btn: # 3 color = QColorDialog.getColor() if color.isValid(): self.text_edit.setTextColor(color) else: # 4 font, ok = QFontDialog.getFont() if ok: self.text_edit.setFont(font)if __name__ == '__main__': app = QApplication(sys.argv) demo = Demo() demo.show() sys.exit(app.exec_()) QTextEdit控件用于显示文本颜色和字体变化； 实例化两个按钮分别用于打开颜色对话框和字体对话框，然后进行信号和槽的连接； 如果是color_btn被按下的话，则调用QColorDialog的getColor()方法显示颜色对话框，当选择一种颜色后其十六进制的值会保存在color变量中，但如果点击对话框中的取消(Cancel)按钮的话，则color为无效值。通过isValid()方法判断color是否有效，若有效的话则通过setTextColor()方法设置QTextEdit的文本颜色； 如果是font_btn被按下的话，则调用QFontDialog的getFont()方法显示字体对话框，该方法返回两个值，分别为QFont和bool值，如果用户选择了一种字体并按下确定(Ok)的话，则font保存所选择的QFont值，并且ok为True。若按下取消(Cancel)的话，则bool为False。当ok为True时，调用setFont()方法设置QTextEdit的文本字体。 2 输入对话框输入对话框一共有五种输入方法： 下面请看示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from zh22_2 import Ui_MainWindowfrom PyQt5 import QtWidgets,QtCorefrom PyQt5.QtWidgets import QApplication, QWidget, QInputDialog, QLineEdit, QTextEdit, QPushButton, \ QGridLayoutimport sys,timeclass MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): # 继承QWidget和Ui_MainWindow def __init__(self): super().__init__() self.setupUi(self) #加载窗体 self.name_btn.clicked.connect(self.name_f) self.gender_btn.clicked.connect(self.gender_f) self.age_btn.clicked.connect(self.age_f) self.score_btn.clicked.connect(self.score_f) self.info_btn.clicked.connect(self.info_f) def name_f(self): name, ok = QInputDialog.getText(self, 'Name Input', 'Please enter the name:') if ok: self.name_line.setText(name) def gender_f(self): gender_list = ["Man", "Woman"] gender, ok = QInputDialog.getItem(self, 'Geender Input', 'Please select the gender', gender_list) if ok: self.gender_line.setText(gender) def age_f(self): age, ok = QInputDialog.getInt(self, 'Age Input', 'Please enter the age:') if ok: self.age_line.setText(str(age)) def score_f(self): score, ok = QInputDialog.getDouble(self, 'Score Input', 'Please select the score:') if ok: self.score_line.setText(str(score)) def info_f(self): info, ok = QInputDialog.getMultiLineText(self, 'Info Input', 'Please enter the info:') if ok: self.info_line.insertPlainText(info)if __name__ == '__main__': app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() # 创建实例 mywindow.show() # 使用QtWidgets的show()方法 sys.exit(app.exec_()) 前面就是实例化按钮、单行输入框和文本编辑框并通过布局管理器进行排列，我们重点来看下槽函数： 如果是name_btn被点击的话，则调用QInputDialog的getText(parent, str, str)方法，第一个参数为指定的父类，第二个为输入框的标题，第三个为输入框提示。方法会返回一个字符串和一个布尔值，若点击输入框的ok按钮，则变量ok就为True，接着我们调用QLineEdit的setText()方法将其文本设为所输入的内容； getItem(parent, str, str, iterable, int, bool)方法需要多设置几个参数，前三个与getText()相同，第四个参数为要加入的选项内容，这里我们传入了item_list列表，可以让用户选择男性或女性。第五个参数为最初显示的选项，0代表刚开始显示第一个选项，即Female。最后一个参数是选项内容是否可编辑，这里设为False，不可编辑。 其他方法的使用都是类似的，这里就进行省略了。]]></content>
      <tags>
        <tag>pyqt</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础]]></title>
    <url>%2F2019%2F08%2F14%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[第一个python程序 在写代码之前，请千万不要用复制-粘贴把代码从页面粘贴到你自己的电脑上。写程序也讲究一个感觉，你需要一个字母一个字母地把代码自己敲进去，在敲代码的过程中，初学者经常会敲错代码：拼写不对，大小写不对，混用中英文标点，混用空格和Tab键，所以，你需要仔细地检查、对照，才能以最快的速度掌握如何写程序。 直接运行py 有同学问，能不能像.exe文件那样直接运行.py文件呢？在Windows上是不行的，但是，在Mac和Linux上是可以的，方法是在.py文件的第一行加上一个特殊的注释： #!/usr/bin/env python3 print(‘hello, world’) 然后，通过命令给hello.py以执行权限： $ chmod a+x hello.py 就可以直接运行hello.py了 python基础 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。此外，Python还提供了列表、字典等多种数据类型，还允许创建自定义数据类型，我们后面会继续讲到。 /除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数 //，称为地板除，两个整数的除法仍然是整数 %,取余 对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符 以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： ‘ABC’.encode(‘ascii’) b’ABC’ ‘中文’.encode(‘utf-8’) b’\xe4\xb8\xad\xe6\x96\x87’ 要计算str包含多少个字符，可以用len()函数 删除list的元素，用pop(i)方法,i为索引 classmates.pop(i) list = [] ; tuple = () ; dict = { ‘ xiaoming ‘: 15 ,} 要删除一个key，用pop(key)方法，对应的value也会从dict中删除 set set和dict类似，也是一组key的集合，但不存储value。 再议不可变对象 上面我们讲了，str是不变对象，而list是可变对象。 对于可变对象，比如list，对list进行操作，list内部的内容是会变化的，比如： 1234&gt;&gt;&gt; a = [&apos;c&apos;, &apos;b&apos;, &apos;a&apos;]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 而对于不可变对象，比如str，对str进行操作呢： 12345&gt;&gt;&gt; a = &apos;abc&apos;&gt;&gt;&gt; a.replace(&apos;a&apos;, &apos;A&apos;)&apos;Abc&apos;&gt;&gt;&gt; a&apos;abc&apos; 虽然字符串有个replace()方法，也确实变出了&#39;Abc&#39;，但变量a最后仍是&#39;abc&#39;，应该怎么理解呢？ 我们先把代码改成下面这样： 123456&gt;&gt;&gt; a = &apos;abc&apos;&gt;&gt;&gt; b = a.replace(&apos;a&apos;, &apos;A&apos;)&gt;&gt;&gt; b&apos;Abc&apos;&gt;&gt;&gt; a&apos;abc&apos; 要始终牢记的是，a是变量，而&#39;abc&#39;才是字符串对象！有些时候，我们经常说，对象a的内容是&#39;abc&#39;，但其实是指，a本身是一个变量，它指向的对象的内容才是&#39;abc&#39;： 123┌───┐ ┌───────┐│ a │─────────────────&gt;│ &apos;abc&apos; │└───┘ └───────┘ 当我们调用a.replace(&#39;a&#39;, &#39;A&#39;)时，实际上调用方法replace是作用在字符串对象&#39;abc&#39;上的，而这个方法虽然名字叫replace，但却没有改变字符串&#39;abc&#39;的内容。相反，replace方法创建了一个新字符串&#39;Abc&#39;并返回，如果我们用变量b指向该新字符串，就容易理解了，变量a仍指向原有的字符串&#39;abc&#39;，但变量b却指向新字符串&#39;Abc&#39;了： 123456┌───┐ ┌───────┐│ a │─────────────────&gt;│ &apos;abc&apos; │└───┘ └───────┘┌───┐ ┌───────┐│ b │─────────────────&gt;│ &apos;Abc&apos; │└───┘ └───────┘ 所以，对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。 Python的循环有两种，一种是for…in循环，依次把list或tuple中的每个元素迭代出来，看例子： 123names = ['Michael', 'Bob', 'Tracy']for name in names: print(name) 执行这段代码，会依次打印names的每一个元素： 123MichaelBobTracy 第二种循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环。比如我们要计算100以内所有奇数之和，可以用while循环实现： 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) 在循环内部变量n不断自减，直到变为-1时，不再满足while条件，循环退出。 另外的， 如果要提前结束循环，可以用break语句： 1234567n = 1while n &lt;= 100: if n &gt; 10: # 当n = 11时，条件满足，执行break语句 break # break语句会结束当前循环 print(n) n = n + 1print('END') 在循环过程中，也可以通过continue语句，跳过当前的这次循环，直接开始下一次循环。 123456n = 0while n &lt; 10: n = n + 1 if n % 2 == 0: # 如果n是偶数，执行continue语句 continue # continue语句会直接继续下一轮循环，后续的print()语句不会执行 print(n) 字符串方法及注释 capitalize() 把字符串的第一个字符改为大写 casefold() 把整个字符串的所有字符改为小写 center(width) 将字符串居中，并使用空格填充至长度 width 的新字符串 count(sub[, start[, end]]) 返回 sub 在字符串里边出现的次数，start 和 end 参数表示范围，可选。 encode(encoding=’utf-8’, errors=’strict’) 以 encoding 指定的编码格式对字符串进行编码。 endswith(sub[, start[, end]]) 检查字符串是否以 sub 子字符串结束，如果是返回 True，否则返回 False。start 和 end 参数表示范围，可选。 expandtabs([tabsize=8]) 把字符串中的 tab 符号（\t）转换为空格，如不指定参数，默认的空格数是 tabsize=8。 find(sub[, start[, end]]) 检测 sub 是否包含在字符串中，如果有则返回索引值，否则返回 -1，start 和 end 参数表示范围，可选。 index(sub[, start[, end]]) 跟 find 方法一样，不过如果 sub 不在 string 中会产生一个异常。 isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返回 True，否则返回 False。 isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True，否则返回 False。 isdecimal() 如果字符串只包含十进制数字则返回 True，否则返回 False。 isdigit() 如果字符串只包含数字则返回 True，否则返回 False。 islower() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是小写，则返回 True，否则返回 False。 isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。 isspace() 如果字符串中只包含空格，则返回 True，否则返回 False。 istitle() 如果字符串是标题化（所有的单词都是以大写开始，其余字母均小写），则返回 True，否则返回 False。 isupper() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是大写，则返回 True，否则返回 False。 join(sub) 以字符串作为分隔符，插入到 sub 中所有的字符之间。 ljust(width) 返回一个左对齐的字符串，并使用空格填充至长度为 width 的新字符串。 lower() 转换字符串中所有大写字符为小写。 lstrip() 去掉字符串左边的所有空格 partition(sub) 找到子字符串 sub，把字符串分成一个 3 元组 (pre_sub, sub, fol_sub)，如果字符串中不包含 sub 则返回 (‘原字符串’, ‘’, ‘’) replace(old, new[, count]) 把字符串中的 old 子字符串替换成 new 子字符串，如果 count 指定，则替换不超过 count 次。 rfind(sub[, start[, end]]) 类似于 find() 方法，不过是从右边开始查找。 rindex(sub[, start[, end]]) 类似于 index() 方法，不过是从右边开始。 rjust(width) 返回一个右对齐的字符串，并使用空格填充至长度为 width 的新字符串。 rpartition(sub) 类似于 partition() 方法，不过是从右边开始查找。 rstrip() 删除字符串末尾的空格。 split(sep=None, maxsplit=-1) 不带参数默认是以空格为分隔符切片字符串，如果 maxsplit 参数有设置，则仅分隔 maxsplit 个子字符串，返回切片后的子字符串拼接的列表。 splitlines(([keepends])) 在输出结果里是否去掉换行符，默认为 False，不包含换行符；如果为 True，则保留换行符。。 startswith(prefix[, start[, end]]) 检查字符串是否以 prefix 开头，是则返回 True，否则返回 False。start 和 end 参数可以指定范围检查，可选。 strip([chars]) 删除字符串前边和后边所有的空格，chars 参数可以定制删除的字符，可选。 swapcase() 翻转字符串中的大小写。 title() 返回标题化（所有的单词都是以大写开始，其余字母均小写）的字符串。 translate(table) 根据 table 的规则（可以由 str.maketrans(‘a’, ‘b’) 定制）转换字符串中的字符。 upper() 转换字符串中的所有小写字符为大写。 zfill(width) 返回长度为 width 的字符串，原字符串右对齐，前边用 0 填充。 函数 定义默认参数要牢记一点：默认参数必须指向不变对象！ 为什么要设计str、None这样的不变对象呢？因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。 文件文件打开模式 打开模式 执行操作 ‘r’ 以只读方式打开文件（默认） ‘w’ 以写入的方式打开文件，会覆盖已存在的文件 ‘x’ 如果文件已经存在，使用此模式打开将引发异常 ‘a’ 以写入模式打开，如果文件存在，则在末尾追加写入 ‘b’ 以二进制模式打开文件 ‘t’ 以文本模式打开（默认） ‘+’ 可读写模式（可添加到其他模式中使用） ‘U’ 通用换行符支持 ​ 文件对象方法 文件对象方法 执行操作 f.close() 关闭文件 f.read([size=-1]) 从文件读取size个字符，当未给定size或给定负值的时候，读取剩余的所有字符，然后作为字符串返回 f.readline([size=-1]) 从文件中读取并返回一行（包括行结束符），如果有size有定义则返回size个字符 f.write(str) 将字符串str写入文件 f.writelines(seq) 向文件写入字符串序列seq，seq应该是一个返回字符串的可迭代对象 f.seek(offset, from) 在文件中移动文件指针，从from（0代表文件起始位置，1代表当前位置，2代表文件末尾）偏移offset个字节 f.tell() 返回当前在文件中的位置 f.truncate([size=file.tell()]) 截取文件到size个字节，默认是截取到文件指针当前位置 OS模块 os模块中关于文件/目录常用的函数使用方法 函数名 使用方法 getcwd() 返回当前工作目录 chdir(path) 改变工作目录 listdir(path=’.’) 列举指定目录中的文件名（’.’表示当前目录，’..’表示上一级目录） mkdir(path) 创建单层目录，如该目录已存在抛出异常 makedirs(path) 递归创建多层目录，如该目录已存在抛出异常，注意：’E:\a\b’和’E:\a\c’并不会冲突 remove(path) 删除文件 rmdir(path) 删除单层目录，如该目录非空则抛出异常 removedirs(path) 递归删除目录，从子目录到父目录逐层尝试删除，遇到目录非空则抛出异常 rename(old, new) 将文件old重命名为new system(command) 运行系统的shell命令 walk(top) 遍历top路径以下所有的子目录，返回一个三元组：(路径, [包含目录], [包含文件])【具体实现方案请看：第30讲课后作业^_^】 以下是支持路径操作中常用到的一些定义，支持所有平台 os.curdir 指代当前目录（’.’） os.pardir 指代上一级目录（’..’） os.sep 输出操作系统特定的路径分隔符（Win下为’\‘，Linux下为’/‘） os.linesep 当前平台使用的行终止符（Win下为’\r\n’，Linux下为’\n’） os.name 指代当前使用的操作系统（包括：’posix’, ‘nt’, ‘mac’, ‘os2’, ‘ce’, ‘java’） os.path模块中关于路径常用的函数使用方法 函数名 使用方法 basename(path) 去掉目录路径，单独返回文件名 dirname(path) 去掉文件名，单独返回目录路径 join(path1[, path2[, …]]) 将path1, path2各部分组合成一个路径名 split(path) 分割文件名与路径，返回(f_path, f_name)元组。如果完全使用目录，它也会将最后一个目录作为文件名分离，且不会判断文件或者目录是否存在 splitext(path) 分离文件名与扩展名，返回(f_name, f_extension)元组 getsize(file) 返回指定文件的尺寸，单位是字节 getatime(file) 返回指定文件最近的访问时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） getctime(file) 返回指定文件的创建时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） getmtime(file) 返回指定文件最新的修改时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） 以下为函数返回 True 或 False exists(path) 判断指定路径（目录或文件）是否存在 isabs(path) 判断指定路径是否为绝对路径 isdir(path) 判断指定路径是否存在且是一个目录 isfile(path) 判断指定路径是否存在且是一个文件 islink(path) 判断指定路径是否存在且是一个符号链接 ismount(path) 判断指定路径是否存在且是一个挂载点 samefile(path1, paht2) 判断path1和path2两个路径是否指向同一个文件 标准异常Python标准异常总结 AssertionError 断言语句（assert）失败 AttributeError 尝试访问未知的对象属性 EOFError 用户输入文件末尾标志EOF（Ctrl+d） FloatingPointError 浮点计算错误 GeneratorExit generator.close()方法被调用的时候 ImportError 导入模块失败的时候 IndexError 索引超出序列的范围 KeyError 字典中查找一个不存在的关键字 KeyboardInterrupt 用户输入中断键（Ctrl+c） MemoryError 内存溢出（可通过删除对象释放内存） NameError 尝试访问一个不存在的变量 NotImplementedError 尚未实现的方法 OSError 操作系统产生的异常（例如打开一个不存在的文件） OverflowError 数值运算超出最大限制 ReferenceError 弱引用（weak reference）试图访问一个已经被垃圾回收机制回收了的对象 RuntimeError 一般的运行时错误 StopIteration 迭代器没有更多的值 SyntaxError Python的语法错误 IndentationError 缩进错误 TabError Tab和空格混合使用 SystemError Python编译器系统错误 SystemExit Python编译器进程被关闭 TypeError 不同类型间的无效操作 UnboundLocalError 访问一个未初始化的本地变量（NameError的子类） UnicodeError Unicode相关的错误（ValueError的子类） UnicodeEncodeError Unicode编码时的错误（UnicodeError的子类） UnicodeDecodeError Unicode解码时的错误（UnicodeError的子类） UnicodeTranslateError Unicode转换时的错误（UnicodeError的子类） ValueError 传入无效的参数 ZeroDivisionError 除数为零 以下是 Python 内置异常类的层次结构： BaseException+– SystemExit+– KeyboardInterrupt+– GeneratorExit+– Exception +– StopIteration +– ArithmeticError | +– FloatingPointError | +– OverflowError | +– ZeroDivisionError +– AssertionError +– AttributeError +– BufferError +– EOFError +– ImportError +– LookupError | +– IndexError | +– KeyError +– MemoryError +– NameError | +– UnboundLocalError +– OSError | +– BlockingIOError | +– ChildProcessError | +– ConnectionError | | +– BrokenPipeError | | +– ConnectionAbortedError | | +– ConnectionRefusedError | | +– ConnectionResetError | +– FileExistsError | +– FileNotFoundError | +– InterruptedError | +– IsADirectoryError | +– NotADirectoryError | +– PermissionError | +– ProcessLookupError | +– TimeoutError +– ReferenceError +– RuntimeError | +– NotImplementedError +– SyntaxError | +– IndentationError | +– TabError +– SystemError +– TypeError +– ValueError | +– UnicodeError | +– UnicodeDecodeError | +– UnicodeEncodeError | +– UnicodeTranslateError +– Warning +– DeprecationWarning +– PendingDeprecationWarning +– RuntimeWarning +– SyntaxWarning +– UserWarning +– FutureWarning +– ImportWarning +– UnicodeWarning +– BytesWarning +– ResourceWarning 对象对象 = 属性 + 方法 类是为了让对象实现量产. self指的是类实例对象本身(注意：不是类本身) 若子类会覆盖父类的__init__时，可采用(1)调用父类 父类.__init__(self) (2) supur().__init__() import在Python中，如果import的语句比较长，导致后续引用不方便，可以使用as语法，比如： import dir1.dir2.mod 那么，后续对mod的引用，都必须是dir1.dir2.mod dir1.dir2.mod.X那么，为了简化输入，可以使用as语法： import dir1.dir2.mod as m 那么，后续对mod的引用，可以直接使用m m. X 需要注意的是，使用as语法之后，只能通过as后面名字来访问导入的moudle import mod as m m.X # OK mod.X # Error 下面提供as的完整语法格式，import和from都支持： import modulename as name # 只能通过name来引用 from modulename import attrname as name # 只能通过name来引用 魔法方法(左右两边两个下划线) 魔法方法 含义 基本的魔法方法 new(cls[, …]) 1. new 是在一个对象实例化的时候所调用的第一个方法 2. 它的第一个参数是这个类，其他的参数是用来直接传递给 init 方法 3. new 决定是否要使用该 init 方法，因为 new 可以调用其他类的构造方法或者直接返回别的实例对象来作为本类的实例，如果 new 没有返回实例对象，则 init 不会被调用 4. new 主要是用于继承一个不可变的类型比如一个 tuple 或者 string init(self[, …]) 构造器，当一个实例被创建的时候调用的初始化方法 del(self) 析构器，当一个实例被销毁的时候调用的方法 call(self[, args…]) 允许一个类的实例像函数一样被调用：x(a, b) 调用 x.call(a, b) len(self) 定义当被 len() 调用时的行为 repr(self) 定义当被 repr() 调用时的行为 str(self) 定义当被 str() 调用时的行为 bytes(self) 定义当被 bytes() 调用时的行为 hash(self) 定义当被 hash() 调用时的行为 bool(self) 定义当被 bool() 调用时的行为，应该返回 True 或 False format(self, format_spec) 定义当被 format() 调用时的行为 有关属性 getattr(self, name) 定义当用户试图获取一个不存在的属性时的行为 getattribute(self, name) 定义当该类的属性被访问时的行为 setattr(self, name, value) 定义当一个属性被设置时的行为 delattr(self, name) 定义当一个属性被删除时的行为 dir(self) 定义当 dir() 被调用时的行为 get(self, instance, owner) 定义当描述符的值被取得时的行为 set(self, instance, value) 定义当描述符的值被改变时的行为 delete(self, instance) 定义当描述符的值被删除时的行为 比较操作符 lt(self, other) 定义小于号的行为：x &lt; y 调用 x.lt(y) le(self, other) 定义小于等于号的行为：x &lt;= y 调用 x.le(y) eq(self, other) 定义等于号的行为：x == y 调用 x.eq(y) ne(self, other) 定义不等号的行为：x != y 调用 x.ne(y) gt(self, other) 定义大于号的行为：x &gt; y 调用 x.gt(y) ge(self, other) 定义大于等于号的行为：x &gt;= y 调用 x.ge(y) 算数运算符 add(self, other) 定义加法的行为：+ sub(self, other) 定义减法的行为：- mul(self, other) 定义乘法的行为：* truediv(self, other) 定义真除法的行为：/ floordiv(self, other) 定义整数除法的行为：// mod(self, other) 定义取模算法的行为：% divmod(self, other) 定义当被 divmod() 调用时的行为 pow(self, other[, modulo]) 定义当被 power() 调用或 ** 运算时的行为 lshift(self, other) 定义按位左移位的行为：&lt;&lt; rshift(self, other) 定义按位右移位的行为：&gt;&gt; and(self, other) 定义按位与操作的行为：&amp; xor(self, other) 定义按位异或操作的行为：^ or(self, other) 定义按位或操作的行为：| 反运算 radd(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rsub(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rmul(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rtruediv(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rfloordiv(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rmod(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rdivmod(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rpow(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rlshift(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rrshift(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rand(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rxor(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） ror(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） 增量赋值运算 iadd(self, other) 定义赋值加法的行为：+= isub(self, other) 定义赋值减法的行为：-= imul(self, other) 定义赋值乘法的行为：*= itruediv(self, other) 定义赋值真除法的行为：/= ifloordiv(self, other) 定义赋值整数除法的行为：//= imod(self, other) 定义赋值取模算法的行为：%= ipow(self, other[, modulo]) 定义赋值幂运算的行为：**= ilshift(self, other) 定义赋值按位左移位的行为：&lt;&lt;= irshift(self, other) 定义赋值按位右移位的行为：&gt;&gt;= iand(self, other) 定义赋值按位与操作的行为：&amp;= ixor(self, other) 定义赋值按位异或操作的行为：^= ior(self, other) 定义赋值按位或操作的行为：|= 一元操作符 pos(self) 定义正号的行为：+x neg(self) 定义负号的行为：-x abs(self) 定义当被 abs() 调用时的行为 invert(self) 定义按位求反的行为：~x 类型转换 complex(self) 定义当被 complex() 调用时的行为（需要返回恰当的值） int(self) 定义当被 int() 调用时的行为（需要返回恰当的值） float(self) 定义当被 float() 调用时的行为（需要返回恰当的值） round(self[, n]) 定义当被 round() 调用时的行为（需要返回恰当的值） index(self) 1. 当对象是被应用在切片表达式中时，实现整形强制转换 2. 如果你定义了一个可能在切片时用到的定制的数值型,你应该定义 index 3. 如果 index 被定义，则 int 也需要被定义，且返回相同的值 上下文管理（with 语句） enter(self) 1. 定义当使用 with 语句时的初始化行为 2. enter 的返回值被 with 语句的目标或者 as 后的名字绑定 exit(self, exc_type, exc_value, traceback) 1. 定义当一个代码块被执行或者终止后上下文管理器应该做什么 2. 一般被用来处理异常，清除工作或者做一些代码块执行完毕之后的日常工作 容器类型 len(self) 定义当被 len() 调用时的行为（返回容器中元素的个数） getitem(self, key) 定义获取容器中指定元素的行为，相当于 self[key] setitem(self, key, value) 定义设置容器中指定元素的行为，相当于 self[key] = value delitem(self, key) 定义删除容器中指定元素的行为，相当于 del self[key] iter(self) 定义当迭代容器中的元素的行为 reversed(self) 定义当被 reversed() 调用时的行为 contains(self, item) 定义当使用成员测试运算符（in 或 not in）时的行为 super()不是父类，而是继承顺序的下一个类super()可以避免重复调用 如果childA基础Base, childB继承childA和Base，如果childB需要调用Base的init()方法时，就会导致init()被执行两次： 1234567891011121314151617class Base(object): def __init__(self): print 'Base create'class childA(Base): def __init__(self): print 'enter A ' Base.__init__(self) print 'leave A'class childB(childA, Base): def __init__(self): childA.__init__(self) Base.__init__(self)b = childB() Base的init()方法被执行了两次 1234enter A Base createleave ABase create 使用super()是可避免重复调用 123456789101112131415161718192021class Base(object): def __init__(self): print 'Base create'class childA(Base): def __init__(self): print 'enter A ' super(childA, self).__init__() print 'leave A'class childB(childA, Base): def __init__(self): super(childB, self).__init__()b = childB()print b.__class__.mro()enter A Base createleave A[&lt;class '__main__.childB'&gt;, &lt;class '__main__.childA'&gt;, &lt;class '__main__.Base'&gt;, &lt;type 'object'&gt;] Python变量前’‘和’*‘的作用在Python的在形参前加’‘和’*‘表示动态形参 在形参前加’*’表示可以接受多个实参值存进数组 12345678910def F(a, *b) print(a) print(b)F(1, 2, 3)&apos;&apos;&apos;1 (2, 3)&apos;&apos;&apos; 对于在形参前加’**’表示表示接受参数转化为字典类型 123456def F(**a) print(a)F(x=1, y=2)#&#123;&apos;x&apos;: 1, &apos;y&apos;: 2&#125; 混合运用 123456789101112def F(a, *b, **c) print(a) print(b) print(c)F(1, 2, 3, x=4, y=5)&apos;&apos;&apos;1(2, 3)&#123;&apos;x&apos;: 4, &apos;y&apos;: 5&#125;&apos;&apos;&apos; 1234567891011def F(*a) print(a)ls = [1, 2, 3]F(ls) #表示列表作为一个元素传入F(*ls) #表示列表元素作为多个元素传入'''([1, 2, 3],)(1, 2, 3)''' 12345678910111213141516def F(**a) print(a)dt = dict(x=1, y=2)F(x=1, y=2) F(**dt) #作为字典传入'''&#123;'x': 1, 'y':2&#125;&#123;'x': 1, 'y':2&#125;函数调用时dt = dict(color='red', fontproperties='SimHei')plt.plot(**dt) 等价于plt.plot(color='red', fontproperties='SimHei')''' python迭代器详解迭代器 迭代是访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 1. 可迭代对象 我们已经知道可以对list、tuple、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，我们把这样的过程称为遍历，也叫迭代。 但是，是否所有的数据类型都可以放到for…in…的语句中，然后让for…in…每次从中取出一条数据供我们使用，即供我们迭代吗？ 123456789101112131415161718192021222324252627&gt;&gt;&gt; for i in 100:... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'int' object is not iterable&gt;&gt;&gt;# int整型不是iterable，即int整型不是可以迭代的# 我们自定义一个容器MyList用来存放数据，可以通过add方法向其中添加数据&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; mylist.add(1)&gt;&gt;&gt; mylist.add(2)&gt;&gt;&gt; mylist.add(3)&gt;&gt;&gt; for num in mylist:... print(num)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyList' object is not iterable&gt;&gt;&gt;# MyList容器的对象也是不能迭代的 我们自定义了一个容器类型MyList，在将一个存放了多个数据的MyList对象放到for…in…的语句中，发现for…in…并不能从中依次取出一条数据返回给我们，也就说我们随便封装了一个可以存放多条数据的类型却并不能被迭代使用。 我们把可以通过for…in…这类语句迭代读取一条数据供我们使用的对象称之为可迭代对象（Iterable）**。 2. 如何判断一个对象是否可以迭代 可以使用 isinstance() 判断一个对象是否是 Iterable 对象： 12345678910111213141516In [50]: from collections import IterableIn [51]: isinstance([], Iterable)Out[51]: TrueIn [52]: isinstance(&#123;&#125;, Iterable)Out[52]: TrueIn [53]: isinstance('abc', Iterable)Out[53]: TrueIn [54]: isinstance(mylist, Iterable)Out[54]: FalseIn [55]: isinstance(100, Iterable)Out[55]: False 3. 可迭代对象的本质 我们分析对可迭代对象进行迭代使用的过程，发现每迭代一次（即在for…in…中每循环一次）都会返回对象中的下一条数据，一直向后读取数据直到迭代了所有数据后结束。那么，在这个过程中就应该有一个“人”去记录每次访问到了第几条数据，以便每次迭代都可以返回下一条数据。我们把这个能帮助我们进行数据迭代的“人”称为迭代器(Iterator)。 可迭代对象的本质就是可以向我们提供一个这样的中间“人”即迭代器帮助我们对其进行迭代遍历使用。 可迭代对象通过__iter__方法向我们提供一个迭代器，我们在迭代一个可迭代对象的时候，实际上就是先获取该对象提供的一个迭代器，然后通过这个迭代器来依次获取对象中的每一个数据. 那么也就是说，一个具备了__iter__方法的对象，就是一个可迭代对象。 12345678910111213141516&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)... def __iter__(self):... """返回一个迭代器"""... # 我们暂时忽略如何构造一个迭代器对象... pass...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(mylist, Iterable)True&gt;&gt;&gt;# 这回测试发现添加了__iter__方法的mylist对象已经是一个可迭代对象了 4. iter()函数与next()函数 list、tuple等都是可迭代对象，我们可以通过iter()函数获取这些可迭代对象的迭代器。然后我们可以对获取到的迭代器不断使用next()函数来获取下一条数据。iter()函数实际上就是调用了可迭代对象的__iter__方法。 1234567891011121314151617&gt;&gt;&gt; li = [11, 22, 33, 44, 55]&gt;&gt;&gt; li_iter = iter(li)&gt;&gt;&gt; next(li_iter)11&gt;&gt;&gt; next(li_iter)22&gt;&gt;&gt; next(li_iter)33&gt;&gt;&gt; next(li_iter)44&gt;&gt;&gt; next(li_iter)55&gt;&gt;&gt; next(li_iter)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 注意，当我们已经迭代完最后一个数据之后，再次调用next()函数会抛出StopIteration的异常，来告诉我们所有数据都已迭代完成，不用再执行next()函数了。** 5. 如何判断一个对象是否是迭代器 可以使用 isinstance() 判断一个对象是否是 Iterator 对象： 1234567In [56]: from collections import IteratorIn [57]: isinstance([], Iterator)Out[57]: FalseIn [58]: isinstance(iter([]), Iterator)Out[58]: TrueIn [59]: isinstance(iter("abc"), Iterator)Out[59]: True 6. 迭代器Iterator 通过上面的分析，我们已经知道，迭代器是用来帮助我们记录每次迭代访问到的位置，当我们对迭代器使用next()函数的时候，迭代器会向我们返回它所记录位置的下一个位置的数据。实际上，在使用next()函数的时候，调用的就是迭代器对象的__next__方法（Python3中是对象的__next__方法，Python2中是对象的next()方法）。所以，我们要想构造一个迭代器，就要实现它的__next__方法。但这还不够，python要求迭代器本身也是可迭代的，所以我们还要为迭代器实现__iter__方法，而__iter__方法要返回一个迭代器，迭代器自身正是一个迭代器，所以迭代器的__iter__方法返回自身即可。 一个实现了iter方法和next方法的对象，就是迭代器。 123456789101112131415161718192021222324252627282930313233343536373839class MyList(object): """自定义的一个可迭代对象""" def __init__(self): self.items = [] def add(self, val): self.items.append(val) def __iter__(self): myiterator = MyIterator(self) return myiteratorclass MyIterator(object): """自定义的供上面可迭代对象使用的一个迭代器""" def __init__(self, mylist): self.mylist = mylist # current用来记录当前访问到的位置 self.current = 0 def __next__(self): if self.current &lt; len(self.mylist.items): item = self.mylist.items[self.current] self.current += 1 return item else: raise StopIteration def __iter__(self): return self if __name__ == '__main__': mylist = MyList() mylist.add(1) mylist.add(2) mylist.add(3) mylist.add(4) mylist.add(5) for num in mylist: print(num) 7. for…in…循环的本质 for item in Iterable 循环的本质就是先通过iter()函数获取可迭代对象Iterable的迭代器，然后对获取到的迭代器不断调用next()方法来获取下一个值并将其赋值给item，当遇到StopIteration的异常后循环结束。 8. 迭代器的应用场景 我们发现迭代器最核心的功能就是可以通过next()函数的调用来返回下一个数据值。如果每次返回的数据值不是在一个已有的数据集合中读取的，而是通过程序按照一定的规律计算生成的，那么也就意味着可以不用再依赖一个已有的数据集合，也就是说不用再将所有要迭代的数据都一次性缓存下来供后续依次读取，这样可以节省大量的存储（内存）空间。 举个例子，比如，数学中有个著名的斐波拉契数列（Fibonacci），数列中第一个数为0，第二个数为1，其后的每一个数都可由前两个数相加得到： 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, … 现在我们想要通过for…in…循环来遍历迭代斐波那契数列中的前n个数。那么这个斐波那契数列我们就可以用迭代器来实现，每次迭代都通过数学计算来生成下一个数。 123456789101112131415161718192021222324252627282930313233class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return selfif __name__ == '__main__': fib = FibIterator(10) for num in fib: print(num, end=" ") 9. 并不是只有for循环能接收可迭代对象 除了for循环能接收可迭代对象，list、tuple等也能接收。 1234li = list(FibIterator(15))print(li)tp = tuple(FibIterator(6))print(tp) ###Python装饰器 python raise当程序出现错误，python会自动引发异常，也可以通过raise显示地引发异常。一旦执行了raise语句，raise后面的语句将不能执行]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP开发板灯光控制上位机]]></title>
    <url>%2F2019%2F08%2F12%2FDSP%E5%BC%80%E5%8F%91%E6%9D%BF%E7%81%AF%E5%85%89%E6%8E%A7%E5%88%B6%E4%B8%8A%E4%BD%8D%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[想法在前几天对PYQT5的学习以后，自己试着做了一个上位机 来对DSP开发板实现LED亮灭操作。 软硬件:pycharm + PYQT5 + CCS6.0 + DSP开发板 成果上位机界面 LED控制 描述：当按下 &lt;打开1灯&gt;时 1亮、2灭。当按下 &lt;打开2灯&gt;时 2亮、1灭。 遇到的问题 在CCS对GPIO进行写操作时(GPXDTA.bit = ) 无法成功写入，在顺华师兄建议下 改用SET CLEAR操作解决. 至于为何DAT不好用还未知？ 关于串口发送 16进制问题 代码 ui_demo.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233# -*- coding: utf-8 -*-# Form implementation generated from reading ui file 'ui_demo_1.ui'## Created by: PyQt5 UI code generator 5.11.3## WARNING! All changes made in this file will be lost!from PyQt5 import QtCore, QtGui, QtWidgetsclass Ui_Form(object): def setupUi(self, Form): Form.setObjectName("Form") Form.resize(868, 452) self.formGroupBox = QtWidgets.QGroupBox(Form) self.formGroupBox.setGeometry(QtCore.QRect(20, 20, 167, 301)) self.formGroupBox.setObjectName("formGroupBox") self.formLayout = QtWidgets.QFormLayout(self.formGroupBox) self.formLayout.setContentsMargins(10, 10, 10, 10) self.formLayout.setSpacing(10) self.formLayout.setObjectName("formLayout") self.s1__lb_1 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_1.setObjectName("s1__lb_1") self.formLayout.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.s1__lb_1) self.s1__box_1 = QtWidgets.QPushButton(self.formGroupBox) self.s1__box_1.setAutoRepeatInterval(100) self.s1__box_1.setDefault(True) self.s1__box_1.setObjectName("s1__box_1") self.formLayout.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.s1__box_1) self.s1__lb_2 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_2.setObjectName("s1__lb_2") self.formLayout.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.s1__lb_2) self.s1__box_2 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_2.setObjectName("s1__box_2") self.formLayout.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.s1__box_2) self.s1__lb_3 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_3.setObjectName("s1__lb_3") self.formLayout.setWidget(3, QtWidgets.QFormLayout.LabelRole, self.s1__lb_3) self.s1__box_3 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_3.setObjectName("s1__box_3") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.formLayout.setWidget(3, QtWidgets.QFormLayout.FieldRole, self.s1__box_3) self.s1__lb_4 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_4.setObjectName("s1__lb_4") self.formLayout.setWidget(4, QtWidgets.QFormLayout.LabelRole, self.s1__lb_4) self.s1__box_4 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_4.setObjectName("s1__box_4") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.formLayout.setWidget(4, QtWidgets.QFormLayout.FieldRole, self.s1__box_4) self.s1__lb_5 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_5.setObjectName("s1__lb_5") self.formLayout.setWidget(5, QtWidgets.QFormLayout.LabelRole, self.s1__lb_5) self.s1__box_5 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_5.setObjectName("s1__box_5") self.s1__box_5.addItem("") self.formLayout.setWidget(5, QtWidgets.QFormLayout.FieldRole, self.s1__box_5) self.open_button = QtWidgets.QPushButton(self.formGroupBox) self.open_button.setObjectName("open_button") self.formLayout.setWidget(7, QtWidgets.QFormLayout.SpanningRole, self.open_button) self.close_button = QtWidgets.QPushButton(self.formGroupBox) self.close_button.setObjectName("close_button") self.formLayout.setWidget(8, QtWidgets.QFormLayout.SpanningRole, self.close_button) self.s1__lb_6 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_6.setObjectName("s1__lb_6") self.formLayout.setWidget(6, QtWidgets.QFormLayout.LabelRole, self.s1__lb_6) self.s1__box_6 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_6.setObjectName("s1__box_6") self.s1__box_6.addItem("") self.formLayout.setWidget(6, QtWidgets.QFormLayout.FieldRole, self.s1__box_6) self.state_label = QtWidgets.QLabel(self.formGroupBox) self.state_label.setText("") self.state_label.setTextFormat(QtCore.Qt.AutoText) self.state_label.setScaledContents(True) self.state_label.setAlignment(QtCore.Qt.AlignRight|QtCore.Qt.AlignTrailing|QtCore.Qt.AlignVCenter) self.state_label.setObjectName("state_label") self.formLayout.setWidget(2, QtWidgets.QFormLayout.SpanningRole, self.state_label) self.verticalGroupBox = QtWidgets.QGroupBox(Form) self.verticalGroupBox.setGeometry(QtCore.QRect(210, 20, 401, 241)) self.verticalGroupBox.setObjectName("verticalGroupBox") self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalGroupBox) self.verticalLayout.setContentsMargins(10, 10, 10, 10) self.verticalLayout.setObjectName("verticalLayout") self.s2__receive_text = QtWidgets.QTextBrowser(self.verticalGroupBox) self.s2__receive_text.setObjectName("s2__receive_text") self.verticalLayout.addWidget(self.s2__receive_text) self.verticalGroupBox_2 = QtWidgets.QGroupBox(Form) self.verticalGroupBox_2.setGeometry(QtCore.QRect(210, 280, 401, 101)) self.verticalGroupBox_2.setObjectName("verticalGroupBox_2") self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.verticalGroupBox_2) self.verticalLayout_2.setContentsMargins(10, 10, 10, 10) self.verticalLayout_2.setObjectName("verticalLayout_2") self.s3__send_text = QtWidgets.QTextEdit(self.verticalGroupBox_2) self.s3__send_text.setObjectName("s3__send_text") self.verticalLayout_2.addWidget(self.s3__send_text) self.s3__send_button = QtWidgets.QPushButton(Form) self.s3__send_button.setGeometry(QtCore.QRect(620, 310, 61, 31)) self.s3__send_button.setObjectName("s3__send_button") self.s3__clear_button = QtWidgets.QPushButton(Form) self.s3__clear_button.setGeometry(QtCore.QRect(620, 350, 61, 31)) self.s3__clear_button.setObjectName("s3__clear_button") self.formGroupBox1 = QtWidgets.QGroupBox(Form) self.formGroupBox1.setGeometry(QtCore.QRect(20, 340, 171, 101)) self.formGroupBox1.setObjectName("formGroupBox1") self.formLayout_2 = QtWidgets.QFormLayout(self.formGroupBox1) self.formLayout_2.setContentsMargins(10, 10, 10, 10) self.formLayout_2.setSpacing(10) self.formLayout_2.setObjectName("formLayout_2") self.label = QtWidgets.QLabel(self.formGroupBox1) self.label.setObjectName("label") self.formLayout_2.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.label) self.label_2 = QtWidgets.QLabel(self.formGroupBox1) self.label_2.setObjectName("label_2") self.formLayout_2.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.label_2) self.lineEdit = QtWidgets.QLineEdit(self.formGroupBox1) self.lineEdit.setObjectName("lineEdit") self.formLayout_2.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.lineEdit) self.lineEdit_2 = QtWidgets.QLineEdit(self.formGroupBox1) self.lineEdit_2.setObjectName("lineEdit_2") self.formLayout_2.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.lineEdit_2) self.hex_send = QtWidgets.QCheckBox(Form) self.hex_send.setGeometry(QtCore.QRect(620, 280, 71, 16)) self.hex_send.setObjectName("hex_send") self.hex_receive = QtWidgets.QCheckBox(Form) self.hex_receive.setGeometry(QtCore.QRect(620, 40, 71, 16)) self.hex_receive.setObjectName("hex_receive") self.s2__clear_button = QtWidgets.QPushButton(Form) self.s2__clear_button.setGeometry(QtCore.QRect(620, 80, 61, 31)) self.s2__clear_button.setObjectName("s2__clear_button") self.timer_send_cb = QtWidgets.QCheckBox(Form) self.timer_send_cb.setGeometry(QtCore.QRect(260, 390, 71, 16)) self.timer_send_cb.setObjectName("timer_send_cb") self.lineEdit_3 = QtWidgets.QLineEdit(Form) self.lineEdit_3.setGeometry(QtCore.QRect(350, 390, 61, 20)) self.lineEdit_3.setAlignment(QtCore.Qt.AlignRight|QtCore.Qt.AlignTrailing|QtCore.Qt.AlignVCenter) self.lineEdit_3.setObjectName("lineEdit_3") self.dw = QtWidgets.QLabel(Form) self.dw.setGeometry(QtCore.QRect(420, 390, 54, 20)) self.dw.setObjectName("dw") self.line = QtWidgets.QFrame(Form) self.line.setGeometry(QtCore.QRect(700, 30, 20, 351)) self.line.setFrameShape(QtWidgets.QFrame.VLine) self.line.setFrameShadow(QtWidgets.QFrame.Sunken) self.line.setObjectName("line") self.s4__open1_button = QtWidgets.QPushButton(Form) self.s4__open1_button.setGeometry(QtCore.QRect(740, 160, 61, 31)) self.s4__open1_button.setObjectName("s4__open1_button") self.s4__open2_button = QtWidgets.QPushButton(Form) self.s4__open2_button.setGeometry(QtCore.QRect(740, 210, 61, 31)) self.s4__open2_button.setObjectName("s4__open2_button") self.verticalGroupBox.raise_() self.verticalGroupBox_2.raise_() self.formGroupBox.raise_() self.s3__send_button.raise_() self.s3__clear_button.raise_() self.formGroupBox.raise_() self.hex_send.raise_() self.hex_receive.raise_() self.s2__clear_button.raise_() self.timer_send_cb.raise_() self.lineEdit_3.raise_() self.dw.raise_() self.line.raise_() self.s4__open1_button.raise_() self.s4__open2_button.raise_() self.retranslateUi(Form) QtCore.QMetaObject.connectSlotsByName(Form) def retranslateUi(self, Form): _translate = QtCore.QCoreApplication.translate Form.setWindowTitle(_translate("Form", "Form")) self.formGroupBox.setTitle(_translate("Form", "串口设置")) self.s1__lb_1.setText(_translate("Form", "串口检测：")) self.s1__box_1.setText(_translate("Form", "检测串口")) self.s1__lb_2.setText(_translate("Form", "串口选择：")) self.s1__lb_3.setText(_translate("Form", "波特率：")) self.s1__box_3.setItemText(0, _translate("Form", "115200")) self.s1__box_3.setItemText(1, _translate("Form", "2400")) self.s1__box_3.setItemText(2, _translate("Form", "4800")) self.s1__box_3.setItemText(3, _translate("Form", "9600")) self.s1__box_3.setItemText(4, _translate("Form", "14400")) self.s1__box_3.setItemText(5, _translate("Form", "19200")) self.s1__box_3.setItemText(6, _translate("Form", "38400")) self.s1__box_3.setItemText(7, _translate("Form", "57600")) self.s1__box_3.setItemText(8, _translate("Form", "76800")) self.s1__box_3.setItemText(9, _translate("Form", "12800")) self.s1__box_3.setItemText(10, _translate("Form", "230400")) self.s1__box_3.setItemText(11, _translate("Form", "460800")) self.s1__lb_4.setText(_translate("Form", "数据位：")) self.s1__box_4.setItemText(0, _translate("Form", "8")) self.s1__box_4.setItemText(1, _translate("Form", "7")) self.s1__box_4.setItemText(2, _translate("Form", "6")) self.s1__box_4.setItemText(3, _translate("Form", "5")) self.s1__lb_5.setText(_translate("Form", "校验位：")) self.s1__box_5.setItemText(0, _translate("Form", "N")) self.open_button.setText(_translate("Form", "打开串口")) self.close_button.setText(_translate("Form", "关闭串口")) self.s1__lb_6.setText(_translate("Form", "停止位：")) self.s1__box_6.setItemText(0, _translate("Form", "1")) self.verticalGroupBox.setTitle(_translate("Form", "接受区")) self.verticalGroupBox_2.setTitle(_translate("Form", "发送区")) self.s3__send_text.setHtml(_translate("Form", "&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\"&gt;\n""&lt;html&gt;&lt;head&gt;&lt;meta name=\"qrichtext\" content=\"1\" /&gt;&lt;style type=\"text/css\"&gt;\n""p, li &#123; white-space: pre-wrap; &#125;\n""&lt;/style&gt;&lt;/head&gt;&lt;body style=\" font-family:\'SimSun\'; font-size:9pt; font-weight:400; font-style:normal;\"&gt;\n""&lt;p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"&gt;123456&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;")) self.s3__send_button.setText(_translate("Form", "发送")) self.s3__clear_button.setText(_translate("Form", "清除")) self.formGroupBox1.setTitle(_translate("Form", "串口状态")) self.label.setText(_translate("Form", "已接收：")) self.label_2.setText(_translate("Form", "已发送：")) self.hex_send.setText(_translate("Form", "Hex发送")) self.hex_receive.setText(_translate("Form", "Hex接收")) self.s2__clear_button.setText(_translate("Form", "清除")) self.timer_send_cb.setText(_translate("Form", "定时发送")) self.lineEdit_3.setText(_translate("Form", "1000")) self.dw.setText(_translate("Form", "ms/次")) self.s4__open1_button.setText(_translate("Form", "打开1灯")) self.s4__open2_button.setText(_translate("Form", "打开2灯")) ui_demo.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243import sysimport serialimport serial.tools.list_portsfrom PyQt5 import QtWidgetsfrom PyQt5.QtWidgets import QMessageBoxfrom PyQt5.QtCore import QTimerfrom ui_demo_1 import Ui_Formclass Pyqt5_Serial(QtWidgets.QWidget, Ui_Form): def __init__(self): super(Pyqt5_Serial, self).__init__() self.setupUi(self) self.init() self.setWindowTitle("串口小助手") self.ser = serial.Serial() self.port_check() # 接收数据和发送数据数目置零 self.data_num_received = 0 self.lineEdit.setText(str(self.data_num_received)) self.data_num_sended = 0 self.lineEdit_2.setText(str(self.data_num_sended)) def init(self): # 串口检测按钮 self.s1__box_1.clicked.connect(self.port_check) # 串口信息显示 self.s1__box_2.currentTextChanged.connect(self.port_imf) # 打开串口按钮 self.open_button.clicked.connect(self.port_open) # 关闭串口按钮 self.close_button.clicked.connect(self.port_close) # 发送数据按钮 self.s3__send_button.clicked.connect(self.data_send) # 定时发送数据 self.timer_send = QTimer() self.timer_send.timeout.connect(self.data_send) self.timer_send_cb.stateChanged.connect(self.data_send_timer) # 定时器接收数据 self.timer = QTimer(self) self.timer.timeout.connect(self.data_receive) # 清除发送窗口 self.s3__clear_button.clicked.connect(self.send_data_clear) # 清除接收窗口 self.s2__clear_button.clicked.connect(self.receive_data_clear) # 打开1灯 self.s4__open1_button.clicked.connect(self.open_led1) # 打开2灯 self.s4__open2_button.clicked.connect(self.open_led2) # 串口检测 def port_check(self): # 检测所有存在的串口，将信息存储在字典中 self.Com_Dict = &#123;&#125; port_list = list(serial.tools.list_ports.comports()) self.s1__box_2.clear() for port in port_list: self.Com_Dict["%s" % port[0]] = "%s" % port[1] self.s1__box_2.addItem(port[0]) if len(self.Com_Dict) == 0: self.state_label.setText(" 无串口") # 串口信息 def port_imf(self): # 显示选定的串口的详细信息 imf_s = self.s1__box_2.currentText() if imf_s != "": self.state_label.setText(self.Com_Dict[self.s1__box_2.currentText()]) # 打开串口 def port_open(self): self.ser.port = self.s1__box_2.currentText() self.ser.baudrate = int(self.s1__box_3.currentText()) self.ser.bytesize = int(self.s1__box_4.currentText()) self.ser.stopbits = int(self.s1__box_6.currentText()) self.ser.parity = self.s1__box_5.currentText() try: self.ser.open() except: QMessageBox.critical(self, "Port Error", "此串口不能被打开！") return None # 打开串口接收定时器，周期为2ms self.timer.start(2) if self.ser.isOpen(): self.open_button.setEnabled(False) self.close_button.setEnabled(True) self.formGroupBox1.setTitle("串口状态（已开启）") # 关闭串口 def port_close(self): self.timer.stop() self.timer_send.stop() try: self.ser.close() except: pass self.open_button.setEnabled(True) self.close_button.setEnabled(False) self.lineEdit_3.setEnabled(True) # 接收数据和发送数据数目置零 self.data_num_received = 0 self.lineEdit.setText(str(self.data_num_received)) self.data_num_sended = 0 self.lineEdit_2.setText(str(self.data_num_sended)) self.formGroupBox1.setTitle("串口状态（已关闭）") # 发送数据 def data_send(self): if self.ser.isOpen(): input_s = self.s3__send_text.toPlainText() if input_s != "": # 非空字符串 if self.hex_send.isChecked(): # hex发送 input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) else: # ascii发送 input_s = (input_s + '\r\n').encode('utf-8') num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended)) else: pass # 接收数据 def data_receive(self): try: num = self.ser.inWaiting() except: self.port_close() return None if num &gt; 0: data = self.ser.read(num) num = len(data) # hex显示 if self.hex_receive.checkState(): out_s = '' for i in range(0, len(data)): out_s = out_s + '&#123;:02X&#125;'.format(data[i]) + ' ' self.s2__receive_text.insertPlainText(out_s) else: # 串口接收到的字符串为b'123',要转化成unicode字符串才能输出到窗口中去 self.s2__receive_text.insertPlainText(data.decode('iso-8859-1')) # 统计接收字符的数量 self.data_num_received += num self.lineEdit.setText(str(self.data_num_received)) # 获取到text光标 textCursor = self.s2__receive_text.textCursor() # 滚动到底部 textCursor.movePosition(textCursor.End) # 设置光标到text中去 self.s2__receive_text.setTextCursor(textCursor) else: pass # 定时发送数据 def data_send_timer(self): if self.timer_send_cb.isChecked(): self.timer_send.start(int(self.lineEdit_3.text())) self.lineEdit_3.setEnabled(False) else: self.timer_send.stop() self.lineEdit_3.setEnabled(True) # 清除显示 def send_data_clear(self): self.s3__send_text.setText("") def receive_data_clear(self): self.s2__receive_text.setText("") def open_led1(self): input_s = '1' input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended)) def open_led2(self): input_s = '2' input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended))if __name__ == '__main__': app = QtWidgets.QApplication(sys.argv) myshow = Pyqt5_Serial() myshow.show() sys.exit(app.exec_()) CCS源代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174#include "DSP2833x_Device.h" // DSP2833x 头文件#include "DSP2833x_Examples.h" // DSP2833x 例子相关头文件/**************************************函数声明************************************************/void scib_echoback_init(void);void scib_xmit(int a);void scib_msg(char *msg);/**********************************************************************************************/// 使用前，声明本文件中的相关函数；void configtestled(void);/**************************************宏定义************************************************/#define InitDIR() \ EALLOW; \ GpioCtrlRegs.GPBPUD.bit.GPIO49 = 0; \ GpioCtrlRegs.GPBDIR.bit.GPIO49 = 1; \ GpioDataRegs.GPBCLEAR.bit.GPIO49 = 1; \ EDIS;#define RX_EN GpioDataRegs.GPBCLEAR.bit.GPIO49 = 1;#define TX_EN GpioDataRegs.GPBSET.bit.GPIO49 = 1;/**********************************************************************************************/void main(void)&#123; Uint16 ReceivedChar; //变量定义 char *msg; //指针// 步骤 1. 初始化系统控制:// 设置PLL, WatchDog, 使能外设时钟// 下面这个函数可以从DSP2833x_SysCtrl.c文件中找到.. InitSysCtrl(); InitScibGpio();// 步骤 2. 初始化通用输入输出多路复用器GPIO:// 这个函数在DSP2833x_Gpio.c源文件中被定义了// 这个函数使GPIO控制类寄存器初始化到默认状态// InitGpio(); // 本例不用此子函数 InitDIR();// 本例使用下面的GPIO配置 configtestled();// 总线初始化函数 InitXintf16Gpio(); //zq// 步骤 3. 清除所有中断初始化中断向量表:// 禁止CPU全局中断 DINT;// 初始化PIE控制寄存器到他们的默认状态.// 这个默认状态就是禁止PIE中断及清除所有PIE中断标志// 这个函数放在DSP2833x_PieCtrl.c源文件里 InitPieCtrl();// 禁止CPU中断和清除所有CPU中断标志 IER = 0x0000; IFR = 0x0000;//初始化PIE中断向量表，并使其指向中断服务子程序（ISR）// 这些中断服务子程序被放在了DSP280x_DefaultIsr.c源文件中// 这个函数放在了DSP2833x_PieVect.c源文件里面. InitPieVectTable(); // 步骤 4. 初始化片内外设:// 这个函数可以在DSP280x_CpuTimers.c源文件中找到// InitCpuTimers(); // 这个例子仅初始化了Cpu定时器// 步骤 5. 用户特定的代码 scib_echoback_init(); msg = "\r\n\nled control\0"; //发送语句 scib_msg(msg); //发送函数 msg = "\r\n \n\0"; //发送语句 scib_msg(msg); //发送函数 GpioDataRegs.GPASET.bit.GPIO1 = 1; GpioDataRegs.GPASET.bit.GPIO2 = 1; for(;;) &#123; msg = "\r\n \0"; //发送语句 scib_msg(msg); //发送函数 //等待接收到数据，否则在此循环 while(ScibRegs.SCIRXST.bit.RXRDY !=1) &#123; &#125; // wait for XRDY =1 for empty state //把接收BUF里的数据赋值给ReceivedChar ReceivedChar = ScibRegs.SCIRXBUF.all; msg = " led open \0"; //发送语句 scib_msg(msg); //发送函数 scib_xmit(ReceivedChar); //发送ReceivedChar if(ScibRegs.SCIRXBUF.all == 1) &#123; GpioDataRegs.GPACLEAR.bit.GPIO1 = 1; GpioDataRegs.GPASET.bit.GPIO2 = 1; msg = "1"; scib_msg(msg); &#125; else if (ScibRegs.SCIRXBUF.all == 2) &#123; GpioDataRegs.GPACLEAR.bit.GPIO2 = 1; GpioDataRegs.GPASET.bit.GPIO1 = 1; msg = "2"; scib_msg(msg); &#125;&#125;void configtestled(void) //GPIO初始化函数&#123; EALLOW; GpioCtrlRegs.GPAMUX1.bit.GPIO1 = 0; // GPIO0复用为GPIO功能 GpioCtrlRegs.GPADIR.bit.GPIO1 = 1; // GPIO0设置为输出 GpioCtrlRegs.GPAMUX1.bit.GPIO2 = 0; // GPIO1复用为GPIO功能 GpioCtrlRegs.GPADIR.bit.GPIO2 = 1; // GPIO1设置为输出 EDIS;&#125;void scib_echoback_init()&#123; // Note: Clocks were turned on to the SCIA peripheral // in the InitSysCtrl() function //SCI的工作模式和参数需要用户在后面的学习中，深入的了解一个寄存器底层相关的资料了，多看看芯片手册和寄存器的意思。 //因为28335的寄存器太多了，所以在以后的学习过程中，就不会对寄存器进行详细的注释了。 ScibRegs.SCICTL1.bit.SWRESET =0; ScibRegs.SCICCR.all =0x0007; // 1 stop bit, No loopback // No parity,8 char bits, // async mode, idle-line protocol ScibRegs.SCICTL1.all =0x0003; // enable TX, RX, internal SCICLK, // Disable RX ERR, SLEEP, TXWAKE #if (CPU_FRQ_150MHZ) ScibRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 37.5MHz. ScibRegs.SCILBAUD =0x00E7; #endif #if (CPU_FRQ_100MHZ) ScibRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 20MHz. ScibRegs.SCILBAUD =0x0044; #endif ScibRegs.SCICTL1.all =0x0023; // Relinquish SCI from Reset&#125;// Transmit a character from the SCIvoid scib_xmit(int a) //发送字节的函数&#123; while (ScibRegs.SCICTL2.bit.TXRDY == 0) &#123;&#125; ScibRegs.SCITXBUF=a;&#125;void scib_msg(char * msg) //发送数组的函数&#123; int i; i = 0; TX_EN; while(msg[i] != '\0') &#123; scib_xmit(msg[i]); i++; &#125; RX_EN;&#125;//===========================================================================// No more.//=========================================================================== 参考[1] 参考的串口程序 [2] 王硕,孙洋洋.PyQt5快速开发与实战[M].电子工业出版社:北京,2017]]></content>
      <tags>
        <tag>上位机</tag>
        <tag>DSP开发板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP基础知识一览]]></title>
    <url>%2F2019%2F08%2F10%2FDSP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[CCS编程基础CCS开发环境已经为我们封装好了许多片内外设寄存器的结构体，我们只需要包含相应的官方的头文件就可以使用了，那么它的内部具体是如何实现的呢？ 下面来一个典型的例子： 1.使用结构体和联合体A.用struct定义位域的作用：在DSP2833x_Sci.h中有一段: 1234567891011121314struct SCICCR_BITS &#123; // bit description Uint16 SCICHAR:3; // 2:0 Character length control Uint16 ADDRIDLE_MODE:1; // 3 ADDR/IDLE Mode control Uint16 LOOPBKENA:1; // 4 Loop Back enable Uint16 PARITYENA:1; // 5 Parity enable Uint16 PARITY:1; // 6 Even or Odd Parity Uint16 STOPBITS:1; // 7 Number of Stop Bits Uint16 rsvd1:8; // 15:8 reserved 保留&#125;; union SCICCR_REG &#123; Uint16 all; struct SCICCR_BITS bit;&#125;; Uint16 SCICHAR:3 表示定义SCICHAR，它占一个字节中的3位；*注意：必须以4字节对齐！观察上面的SCICCR_BITS的定义也会发现前面定义了3+1+1+1+1+1=8位=1字节 B.再来看union的作用 1234union SCICCR_REG&#123; Uint16 all; struct SCICCR_BITS bit;&#125; 这样定义有什么效果？当我想操作SCICCR_BITS中的每一位时，只需定义union SCICCR_REG reg即可 我们可以整体操作，如：reg.all = 0x0011; 我们可以操作其中一位：reg.bit.PARITY = 0; 还记得c语言中union中的共享同一个内存空间地址么？ 2.使用cmd文件进行数据段与存储器空间映射既然官方已经帮我们做好了上面的一切，上面的东西肯定可以直接使用，那么为什么可以直接使用呢？ 定义一个上面的变量就可以访问到真正硬件上的寄存器了吗？肯定不行！ 我们需要将上面的变量和实际硬件的寄存器存储空间绑定，怎么绑定，通过cmd文件。 下面是官方DSP2833x_GlobalVariableDefs.c中的一段代码： 1234567//----------------------------------------#ifdef __cplusplus#pragma DATA_SECTION("ScicRegsFile")#else#pragma DATA_SECTION(ScicRegs,"ScicRegsFile");#endifvolatile struct SCI_REGS ScicRegs; 官方定义了ScicRegs来操作串口SCI-C的相关的寄存器，但是肯定没法直接使用，还没有做绑定； 使用#pragma DATA_SECTION可以将变量与数据段绑定，变量和数据段是自己定义的，只需要将他们绑定即可； 这样绑定显然还不行，还需要通过cmd文件数据段映射到硬件的寄存器地址空间中去！ 查看DSP2833x_Headers_nonBIOS.cmd文件我们发现其中有这样几行： 12345678910111213141516171819MEMORY&#123; PAGE 0: /* Program Memory */ PAGE 1: /* Data Memory */ ADC : origin = 0x007100, length = 0x000020 /* ADC registers */ SCIB : origin = 0x007750, length = 0x000010 /* SCI-B registers */ SCIC : origin = 0x007770, length = 0x000010 /* SCI-C registers */ I2CA : origin = 0x007900, length = 0x000040 /* I2C-A registers */ &#125; SECTIONS&#123; AdcRegsFile : &gt; ADC, PAGE = 1 ScibRegsFile : &gt; SCIB, PAGE = 1 ScicRegsFile : &gt; SCIC, PAGE = 1 I2caRegsFile : &gt; I2CA, PAGE = 1&#125; MEMORY代表内存空间，PAGE0是程序空间， PAGE1是数据空间； (还记得第一课的介绍么？28335采样的哈佛总线结构，程序与数据分开了~) SECTIONS代表需要映射的段； 通过上面的映射后，操作ScicRegs就可以实际操作串口了，目的也就达到了； 时钟TMS320F28335上有一个基于PLL电路的片上时钟模块，如图1所示，为CPU及外设提供时钟有两种方式： 一种是用外部的时钟源，将其连接到X1引脚上或者XCLKIN引脚上，X2接地；另一种是使用振荡器产生时钟，用30MHz的晶体和两个20PF的电容组成的电路分别连接到X1和X2引脚上，XCLKIN引脚接地。 我们常用第二种来产生时钟。此时钟将通过一个内部PLL锁相环电路，进行倍频。由于F28335的最大工作频率是150M，所以倍频值最大是5。其中倍频值由PLLCR的低四位和PLLSTS的第7、8位来决定。其详细的倍频值可以参照TMS320F28335的Datasheet。 三种时钟输入的接法: 如果我们希望DSP工作在某一个频率下，我们就可以对Uint16 val, Uint16 divsel两个参数进行设定。说白了就相当于乘10，除2 (30*10/2 = 150MHZ) GPIO引脚GPIO（General-Purpose Input/Output）——通用输入/输出口 DSP28335 GPIO模块分为三类IO口：PORTA(0-31),PORTB(32-63),PORTC(64-87) 对GPIO模块的设置主要通过三类寄存器来完成，分别是：控制寄存器、数据寄存器、中断寄存器。 1、控制寄存器 12345678910111213141516171819 GPxCTRL; // GPIO x Control Register (GPIO0 to 31) //设置采样窗周期T=2*GPXCTRL*Tsysclk； GPxQSEL1; // GPIO x Qualifier Select 1 Register (GPIO0 to 15)(32-47) GPxQSEL2; // GPIO x Qualifier Select 2 Register (GPIO16 to 31)(48-63) //每两位控制一个引脚，确定是3周期采样还是6周期采样或者不用采样 GPxMUX1; // GPIO x Mux 1 Register (GPIO0 to 15)(32-47)(64-79) GPxMUX2; // GPIO x Mux 2 Register (GPIO16 to 31)(48-63)(80-95) //配置各个引脚的功能，0：I/O功能，1：外设功能。 GPxDIR; // GPIO x Direction Register (GPIO0 to 31)(32-63)(64-95) //配置每个引脚是输入还是输出，0：数字量输入；1：数字量输出。 GPxPUD; // GPIO x Pull Up Disable Register (GPIO0 to 31)(32-63)(64-95) //使能或禁止内部上拉 0：开启上拉，1：禁止上拉 2、数据寄存器 123456 GPxDAT; // GPIO Data Register (GPIO0 to 31)(32-63)(64-95) GPxSET; // GPIO Data Set Register (GPIO0 to 31)(32-63)(64-95)——置位 GPxCLEAR; // GPIO Data Clear Register (GPIO0 to 31)(32-63)(64-95) GPxTOGGLE; // GPIO Data Toggle Register (GPIO0 to 31)(32-63)(64-95)—反转 3、中断寄存器 12345678910111213141516 GPIOXINT1SEL; // XINT1 GPIO Input Selection GPIOXINT2SEL; // XINT2 GPIO Input Selection GPIOXNMISEL; // XNMI_Xint13 GPIO Input Selection GPIOXINT3SEL; // XINT3 GPIO Input Selection GPIOXINT4SEL; // XINT4 GPIO Input Selection GPIOXINT5SEL; // XINT5 GPIO Input Selection GPIOXINT6SEL; // XINT6 GPIO Input Selection GPIOXINT7SEL; // XINT7 GPIO Input Selection GPIOLPMSEL; // Low power modes GP I/O input select 可以对GPIO0-63进行外部中断设置； 具体定义在DSP28335Gpio.h中，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950struct GPIO_CTRL_REGS &#123; union GPACTRL_REG GPACTRL; // GPIO A Control Register (GPIO0 to 31) union GPA1_REG GPAQSEL1; // GPIO A Qualifier Select 1 Register (GPIO0 to 15) union GPA2_REG GPAQSEL2; // GPIO A Qualifier Select 2 Register (GPIO16 to 31) union GPA1_REG GPAMUX1; // GPIO A Mux 1 Register (GPIO0 to 15) union GPA2_REG GPAMUX2; // GPIO A Mux 2 Register (GPIO16 to 31) union GPADAT_REG GPADIR; // GPIO A Direction Register (GPIO0 to 31) union GPADAT_REG GPAPUD; // GPIO A Pull Up Disable Register (GPIO0 to 31) Uint32 rsvd1; union GPBCTRL_REG GPBCTRL; // GPIO B Control Register (GPIO32 to 63) union GPB1_REG GPBQSEL1; // GPIO B Qualifier Select 1 Register (GPIO32 to 47) union GPB2_REG GPBQSEL2; // GPIO B Qualifier Select 2 Register (GPIO48 to 63) union GPB1_REG GPBMUX1; // GPIO B Mux 1 Register (GPIO32 to 47) union GPB2_REG GPBMUX2; // GPIO B Mux 2 Register (GPIO48 to 63) union GPBDAT_REG GPBDIR; // GPIO B Direction Register (GPIO32 to 63) union GPBDAT_REG GPBPUD; // GPIO B Pull Up Disable Register (GPIO32 to 63) Uint16 rsvd2[8]; union GPC1_REG GPCMUX1; // GPIO C Mux 1 Register (GPIO64 to 79) union GPC2_REG GPCMUX2; // GPIO C Mux 2 Register (GPIO80 to 95) union GPCDAT_REG GPCDIR; // GPIO C Direction Register (GPIO64 to 95) union GPCDAT_REG GPCPUD; // GPIO C Pull Up Disable Register (GPIO64 to 95)&#125;; struct GPIO_DATA_REGS &#123; union GPADAT_REG GPADAT; // GPIO Data Register (GPIO0 to 31) union GPADAT_REG GPASET; // GPIO Data Set Register (GPIO0 to 31) union GPADAT_REG GPACLEAR; // GPIO Data Clear Register (GPIO0 to 31) union GPADAT_REG GPATOGGLE; // GPIO Data Toggle Register (GPIO0 to 31) union GPBDAT_REG GPBDAT; // GPIO Data Register (GPIO32 to 63) union GPBDAT_REG GPBSET; // GPIO Data Set Register (GPIO32 to 63) union GPBDAT_REG GPBCLEAR; // GPIO Data Clear Register (GPIO32 to 63) union GPBDAT_REG GPBTOGGLE; // GPIO Data Toggle Register (GPIO32 to 63) union GPCDAT_REG GPCDAT; // GPIO Data Register (GPIO64 to 95) union GPCDAT_REG GPCSET; // GPIO Data Set Register (GPIO64 to 95) union GPCDAT_REG GPCCLEAR; // GPIO Data Clear Register (GPIO64 to 95) union GPCDAT_REG GPCTOGGLE; // GPIO Data Toggle Register (GPIO64 to 95) Uint16 rsvd1[8];&#125;; struct GPIO_INT_REGS &#123; union GPIOXINT_REG GPIOXINT1SEL; // XINT1 GPIO Input Selection union GPIOXINT_REG GPIOXINT2SEL; // XINT2 GPIO Input Selection union GPIOXINT_REG GPIOXNMISEL; // XNMI_Xint13 GPIO Input Selection union GPIOXINT_REG GPIOXINT3SEL; // XINT3 GPIO Input Selection union GPIOXINT_REG GPIOXINT4SEL; // XINT4 GPIO Input Selection union GPIOXINT_REG GPIOXINT5SEL; // XINT5 GPIO Input Selection union GPIOXINT_REG GPIOXINT6SEL; // XINT6 GPIO Input Selection union GPIOXINT_REG GPIOXINT7SEL; // XINT7 GPIO Input Selection union GPADAT_REG GPIOLPMSEL; // Low power modes GP I/O input select&#125;; 注意：GPIO相关寄存器介绍 1、GPxMUX寄存器（功能选择寄存器） 每个I/O口都有一个功能选择寄存器，功能选择寄存器主要用于选择I/O工作在特殊功能还是通用数组I/O模式。在复位时，所有GPIO配置成通用数字模式。 1）如果GPxMUX.bit = 0，配置成通用数字I/O功能； 2）如果GPxMUX.bit = 1 2 3，配置成特殊外设功能口（如SCI、CAN）； I/O的输入功能和外设的输入通道总是被使能的，输出通道是通用数组I/O和特殊外设复用的。如果引脚配置成通用数组I/O功能，相应的外设功能将被禁止。 2、GPxDIR（方向控制寄存器） 每个I/O口都有数据方向控制寄存器，数据方向控制寄存器用于设置通用数字I/O为输入还是输出口，在复位时，引脚的默认状态为输入状态。 1）如果GPxDIR.bit = 0，引脚设置为通用数字量输入； 2）如果GPxDIR.bit = 1，引脚设置为通用数字量输出； 复位时，GPxMUX和GPxDIR默认值都为0，所以在复位时，引脚的默认状态为数字I/O输入。 3、GPxDAT寄存器（数据寄存器） 每个I/O口都有一个数据寄存器，数据寄存器是可读可写寄存器。 1）I/O设置为输出功能时，如果GPxDAT.bit = 0，那么操作将会使相应的引脚拉低； 2）I/O口设置为输入功能时，如果GPxDAT.bit = 0，反映相应的引脚状态为低电平； 3）I/O口设置为输出功能时，如果GPxDAT.bit = 1，那么操作将会使相应的引脚拉高； 4）I/O口设置为输入功能时，如果GPxDAT.bit = 1，反映相应的引脚状态为高电平。 需要说明的是，当用户试图改变一个数字I/O的状态时，不要改变另一个I/O的引脚状态。 4、GOxSET寄存器（置位寄存器） 每个I/O口都有一个置位寄存器，置位寄存器是只写寄存器，任何读操作都返回0，如果相应的引脚配置成数据量输出，写1后相应的引脚会置高，写0时没有反映。 1）如果GPxSET.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxSET.bit = 1，那么操作将会使引脚置高。 5、GPxCLEAR寄存器（清除寄存器） 每个I/O口都有一个清除寄存器，清除寄存器是只写寄存器，任何读操作都返回0。 1）如果GPxCLEAR.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxCLEAR.bit = 1，将相应的引脚置成低电平。 6、GPxTOGGLE寄存器（取反触发寄存器） 每个I/O口都有一个取反触发寄存器，该寄存器是只写寄存器，任何读操作都返回0。 1）如果GPxTOGGLE.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxTOGGLE.bit = 1，那么操作将使相应的引脚取反。 中断1.中断系统 在这里我们要十分清楚DSP的中断系统。C28XX一共有16个中断源，其中有2个不可屏蔽的中断RESET和NMI、定时器1和定时器2分别使用中断13和14。这样还有12个中断都直接连接到外设中断扩展模块PIE上。说的简单一点就是PIE通过12根线与28335核的12个中断线相连。而PIE的另外一侧有12*8根线分别连接到外设，如AD、SPI、EXINT等等。 PIE共管理12*8=96个外部中断。这12组大中断由28335核的中断寄存器IER来控制，即IER确定每个中断到底属于哪一组大中断（如IER |= M_INT12; 说明我们要用第12组的中断，但是第12组里面的什么中断CPU并不知道需要再由PIEIER确定）。 接下来再由PIE模块中的寄存器PIEIER中的低8确定该中断是这一组的第几个中断，这些配置都要告诉CPU（我们不难想象到PIEIER共有12总即从PIEIER1-PIEIER12）。另外，PIE模块还有中断标志寄存器PIEIFR，同样它的低8位是来自外部中断的8个标志位，同样CPU的IFR寄存器是中断组的标志寄存器。由此看来，CPU的所有中断寄存器控制12组的中断，PIE的所有中断寄存器控制每组内8个的中断。除此之外，我们用到哪一个外部中断，相应的还有外部中断的寄存器，需要注意的就是外部中断的标志要自己通过软件来清零。而PIE和CPU的中断标志寄存器由硬件来清零。 12345678EALLOW; // This is needed to write to EALLOW protected registers PieVectTable.XINT2 = &amp;ISRExint; //告诉中断入口地址EDIS; // This is needed to disable write to EALLOW protected registersPieCtrlRegs.PIECTRL.bit.ENPIE = 1; // Enable the PIE block使能PIEPieCtrlRegs.PIEIER1.bit.INTx5= 1; //使能第一组中的中断5IER |= M_INT1; // Enable CPU 第一组中断EINT; // Enable Global interrupt INTMERTM; // Enable Global realtime interrupt DBGM 也就是说，12组中的每个中断都要完成上面的相同配置，剩下的才是去配置自己的中断。如我们提到的EXINT，即外面来个低电平我们就进入中断，完成我们的程序。在这里要介绍一下，DSP的GPIO口都可以配置为外部中断口，其配置方法如下： 1234567891011121314151617181920212223242526272829GpioCtrlRegs.GPBMUX2.bit.GPIO54 = 0; //选择他们是GPIO口GpioCtrlRegs.GPBMUX2.bit.GPIO55 = 0;GpioCtrlRegs.GPBMUX2.bit.GPIO56 = 0;GpioCtrlRegs.GPBMUX2.bit.GPIO57 = 0; GpioCtrlRegs.GPBDIR.bit.GPIO54 = 0;//选择他们都是输入口GpioCtrlRegs.GPBDIR.bit.GPIO55 = 0;GpioCtrlRegs.GPBDIR.bit.GPIO56 = 0;GpioCtrlRegs.GPBDIR.bit.GPIO57 = 0; GpioCtrlRegs.GPBQSEL2.bit.GPIO54= 0;//使GPIO时钟和系统时钟一样 且支持GPIOGpioCtrlRegs.GPBQSEL2.bit.GPIO55= 0;GpioCtrlRegs.GPBQSEL2.bit.GPIO56= 0;//配置输入口权限，对于选择为输入口的需配置GPACTRL,GPBCTRL,GPAQSEL1GpioCtrlRegs.GPBQSEL2.bit.GPIO57= 0;//GPAQSEL2, GPBQSEL1, and GPBQSEL2寄存器所有输入信号与CPU输出系统时钟同步； GpioIntRegs.GPIOXINT3SEL.bit.GPIOSEL = 54;//中断3选择GPIOGpioIntRegs.GPIOXINT4SEL.bit.GPIOSEL = 55;GpioIntRegs.GPIOXINT5SEL.bit.GPIOSEL = 56;GpioIntRegs.GPIOXINT6SEL.bit.GPIOSEL = 57; XIntruptRegs.XINT3CR.bit.POLARITY= 0;//触发模式为下降沿触发XIntruptRegs.XINT4CR.bit.POLARITY= 0;XIntruptRegs.XINT5CR.bit.POLARITY= 0;XIntruptRegs.XINT6CR.bit.POLARITY= 0; XIntruptRegs.XINT3CR.bit.ENABLE = 1;//使能中断XIntruptRegs.XINT4CR.bit.ENABLE = 1;XIntruptRegs.XINT5CR.bit.ENABLE = 1;XIntruptRegs.XINT6CR.bit.ENABLE = 1; 注意一点就是外部中断1和2只能对GPIO0—GPIO31配置；外部中断3和4、5、6、7只对GPIO32—GPIO63配置。 GPIO分为A(0-31)、B(32-63)、C(64-87);C组的不能配置为外部中断； 2.如何开启某个中断？ 设置中断向量。例如：PieVectTable.ADCINT = &amp;adc_isr;等打开PIE控制器。PieCtrlRegs.PIECTRL.bit.ENPIE = 1;使能PIE中对应外设的中断（相应group的相应pin）。例如：PieCtrlRegs.PIEIER1.bit.INTx8 = 1; PieCtrlRegs.PIEIER1.bit.INTx6 = 1;等使能CPU的相应中断（INT1~INT12）IER |= M_INT1;使能CPU响应中断EINT、ERTM;; 3.中断标志有几级？作用是什么？ 中断标志主要有三级CPU（有16个标志位）、PIE（有12组每组有12个标志位）和外设（有的外设没有）。 标志位在中断发生后锁存中断状态，即表示中断发生。在CPU响应中断后，会自动清除cpu级别的标志位IFR bit，同时将INTM bit 置位，以防止其它中断的发生； CPU在从PIE中取中断向量时PIE会自动清除PIE级别的标志位PIEIFRx.y。所以在进入中断处理程序后除了外设所有中断位都已经清除。 而中断处理程序中需要清除PIEACKx和外设的中断标志位（如果有的话）。 在CPU响应一个中断后，在进入ISR的时候，默认会关断全局中断，即在执行中断服务程序时，不会有其他中断来打断CPU，包括本次的中断事件。另外，如果外设的中断标志位不清除，不会循环进入这个中断服务函数，这个外设中断被阻断了。所以只有清除外设的中断服务程序，才能响应下一次的外设中断。PIEACK同理，如果没有PIEACK，这组所有中断都被阻断。 参考文献[1] TMS320F2833x Datasheet [2] 风雨也无晴 CSDN [3] GPIO blog]]></content>
      <tags>
        <tag>DSP28335</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP 串口接口及通信通信]]></title>
    <url>%2F2019%2F08%2F09%2F%E4%B8%B2%E5%8F%A3%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[对串口通信进行简单知识点梳理串行通信可以分为两大类： 同步通信：典型 I2C,SPI 异步通信：典型 SCI(serial communication interface,串行通信接口) 进行串口异步通信接口，一般可以看作UART口(Universal Asynchronous Receiver Transmitter：通用异步收发器/异步串行通信口) UART、COM指物理接口形式(硬件), TTL、RS232、RS485 指电平标准(电信号) SCI 串口先来补充一个概念:FIFO（First Input First Output），即先进先出队列。 SCI模块介绍TMS320F28335内部有三个SCI模块，SCIA、SCIB、SCIC。 每一个SCI模块都有一个接收器和发送器，SCI的接收器和发送器各有一个16级的FIFO(First In First Out先入先出)队列，它们都还有自己独立的使能位和中断位；可以工作在半双工或全双工模式。 1. SCI的CPU 接口SCI 模块具有两个引脚， SCITXDA 和 SCIRXDA，分别实现发送数据和接收数据的功能，这两个引脚对应于 GPIOF 模块的第4和第5位，在编程初始化的时候，需要将GPIOFMUX 寄存器的第4和第5位置为1，才能使得这两个引脚具有发送和接收的功能，否则就是普通的I/O引脚。外部晶振通 PLL 模块产生了CPU 的系统时钟SYSCLKOUT，然后SYSCLKOUT经过低速预定标器之后输出低速时钟LSPCLK 供给SCI。要保证SCI的正常运行，系统控制模块下必须使能SCI的时钟，也就是在系统初始化函数中需要将外设时钟控制寄存器PCLKCR的SCIAENCLK位置1。从下图，我们可以清楚的看到SCIA可以产生两个中断，SCIRXINTA 和SCITXINTA，即发送中断和接收中断。 2. SCI相关寄存器 SCICR：SCI通信参数设置寄存器，设置数据位，停止位，奇偶校验位。 SCICTL1：使能SCI的发送接收功能 注：SW RESET需置1 SCILBAUD、SCIHBAUD：通信速率（波特率）的设置。 SCICTL2：使能接收发送中断，以及发送中断标志位。 SCIRXST：接收相关标志位。 SCIRXBUF：8位发送缓存寄存器 SCITXBUF：8位接收缓存寄存器。 SCI中断配置： SCI的中断采用三级中断管理。分别是SCI外设中断，PIE中断，CPU中断。SCIA的PIE中断是第九组，分别是INT９.１和INT９.２.PIE中断的配置在前面已经说过了，此处不多说。注意：在中断不要忘记将PIEACK写１清除。 SCI的FIFO模式： FIFO：先入先出队列。SCI采用这种模式时，接收或者发送完指定字节数量的数据后，才进入中断处理。这样可以节省了CPU的使用效率，CPU不用每次接收完一个字节的数据后就进入中断处理。 相关寄存器： SCIFFTX：配置发送的数据量，使能SCI的FIFO模式，使能中断等 SCIFFRX：配置接收的数据量，使能接收中断等。 在学习FIFO模式时，遇到一个问题，就是接收完指定数量字节的数据后，总是重复进入两次发送中断，一次找不到原因。下面贴出代码，希望各位读者不吝赐教。 3. SCI 模块发送和接收数据的工作原理 SCI 模块的工作原理如下图所示，之所以SCI 能工作于全双工模式，是因为它有独立的数据发送器和数据接收器，这样能够保证SCI既能够同时进行，也能够独立进行发送和接收的操作。 SCI 发送数据的过程如下：如下图右半部分所示， 在FIFO功能使能的情况下， 首先，发送数据缓冲寄存器SCITXBUF从TX FIFO 中获取由 CPU 加载的需要发送的数据，然后 SCITXBUF将数据传输给发送移位寄存器TXSHF， 如果SCI的发送功能使能， TXSHF 则将接收到的数据逐位逐位的移到 SCITXD 引脚上。 SCI接收数据的过程如下：如X下图的左半部分所示，首先，接收移位寄存器 RXSHF 逐位逐位的接收来自于 SCIRXD 引脚的数据， 如果 SCI 的接收功能使能， RXSHF 将这些数据传输给接收缓冲寄存器 SCIRXBUF，CPU 就能从 SCIRXBUF 读取外部发送来的数据。当然，如果 FIFO 功能使能的话， SCIRXBUF 会将数据加载到RX FIFO 的队列中， CPU 再从FIFO 的队列读取数据。 4. SCI数据格式 在 SCI 中，通信协议体现在 SCI 的数据格式上。 通常将 SCI 的数据格式称之为可编程的数据格式，原因就是可以通过 SCI 的通信控制寄存器 SCICCR 来进行设置，规定通信过程中所使用的数据格式。 在空闲线模式下， SCI 发送或者接收一帧的数据格式如图 4 所示，其中 LSB 是数据的最低位， MSB 是数据的最高位。 使用 SCICCR 进行数据格式编程 12345678SciaRegs.SCICCR.bit.SCICHAR=8;//选择数据长度，为 8 个数据位SciaRegs.SCICCR.bit.PARITYENA=1;//开启极性功能，值为 0 的时候取消极性功能SciaRegs.SCICCR.bit.PARITY=0;//在开启极性功能的前提下，该位值为 0 时选择偶极性，值为 1 时选择奇极性SciaRegs.SCICCR.bit.STOPBITS=0;//选择停止位，该位为 0 时有 1 个停止位，该位为 1 时有 2 个停止位 当然，上述这几个语句，我们也可以合并成如下的语句： 1SciaRegs.SCICCR.all=0x13; 5. SCI通信波特率 所谓的波特率就是指每秒所能发送的位数。SCI波特率设置寄存器SCIHBAUD和SCILBAUD，0-15是高字节与低字节连在一起，构成16位波特率设置寄存器BRR。BRR = SCIHBAUD + SCILBAUD 如果1&lt;= BRR &lt;=65535，那么SCI波特率=LSPCLK / ( (BRR+1) * 8 )，由此，可以带入你需要的波特率，既可以得到BRR的值；如果BRR = 0，那么SCI波特率=LSPCLK/ 16 我们举例来进行说明。例如外部晶振位 30M，经过 PLL 之后 SYSCLKOUT 为 150MHz，然后，当低速预定标器 LOSPCP 的值为 2 的时候， SYSCLKOUT 经过低速预定标器之后产生&gt;低速外设时钟 LSPCLK 为 37.5MHz，也就是说 SCI 的时钟为 37.5MHz。如果我们需要 SCI 的波特率为 19200，则将 LSPCLK 和波特率的数值代入式 1，便可得到BRR=243.14，由于寄存器都是正整数，所以省略掉小数后可以得到 BRR=243。将 243 转换成 16 进制是 0xF3，因此 SCIHBAUD 的值为 0， SCIHBAUD 的值为 0XF3。由于省略了小数，将会产生 0.06%的误差。 当 LSPCLK 为 37.5M 时，对于 SCI 常见的波特率，其寄存器的值如下表所示： 6. 串口SCI编程 A. 先初始化IO管脚 (以SCI-A为例，SCI-B、SCI-C的初始化方法一样，就是照着改对应的管脚就行) 1234567891011121314void InitSciaGpio() //初始化SCIA的GPIO管脚为例子&#123;EALLOW;//根据硬件设计决定采用GPIO28/29和GPIO35/36中的哪一组。这里以35/36为例//定义管脚为上拉GpioCtrlRegs.GPBPUD.bit.GPIO36 = 0;GpioCtrlRegs.GPBPUD.bit.GPIO35 = 0;//定义管脚为异步输入GpioCtrlRegs.GPBQSEL1.bit.GPIO36 = 3;//配置管脚为SCI功能管脚GpioCtrlRegs.GPBMUX1.bit.GPIO36 = 1;GpioCtrlRegs.GPBMUX1.bit.GPIO35 = 1;EDIS;&#125; B. SCI初始化配置 12345678910111213void scia_init()&#123;SciaRegs.SCICCR.all =0x0007; // 1 stop bit, No loopback// No parity,8 char bits,// async mode, idle-line protocolSciaRegs.SCICTL1.all =0x0003; // enable TX, RX, internal SCICLK,// Disable RX ERR, SLEEP, TXWAKESciaRegs.SCICTL2.bit.TXINTENA =1; //发送中断使能SciaRegs.SCICTL2.bit.RXBKINTENA =1;//接收中断使能SciaRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 37.5MHz.SciaRegs.SCILBAUD =0x00E7;SciaRegs.SCICTL1.all =0x0023; // Relinquish SCI from Reset&#125; C. 接着进行中断的配置 123456EALLOW; // This is needed to write to EALLOW protected registersPieVectTable.SCIRXINTA = &amp;sciaRxIsr;PieVectTable.SCITXINTA = &amp;sciaTxIsr;PieVectTable.SCIRXINTB = &amp;scibRxIsr;PieVectTable.SCITXINTB = &amp;scibTxIsr;EDIS; // This is needed to disable write to EALLOW protected registers D. 上面是将SCIA和SCIB的中断服务程序连到PIE的中断表中，发生中断就会跑到你的ISR去了，下面是开中断： 1234567PieCtrlRegs.PIECTRL.bit.ENPIE = 1; // Enable the PIE blockPieCtrlRegs.PIEIER9.bit.INTx1=1; // PIE Group 9, int1PieCtrlRegs.PIEIER9.bit.INTx2=1; // PIE Group 9, INT2PieCtrlRegs.PIEIER9.bit.INTx3=1; // PIE Group 9, INT3PieCtrlRegs.PIEIER9.bit.INTx4=1; // PIE Group 9, INT4IER = 0x100; // Enable CPU INTEINT; 这样串口基本就OK了。 参考文献[1] 接口及协议总结[2] SCI通信]]></content>
      <tags>
        <tag>DSP28335</tag>
        <tag>上位机</tag>
        <tag>串口通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP电机平台上位机编写(Python+pyqt)]]></title>
    <url>%2F2019%2F08%2F06%2FDSP%E5%B9%B3%E5%8F%B0%E4%B8%8A%E4%BD%8D%E6%9C%BA%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[准备给DSP电机平台增加一个电机转速调节的上位机。 目的： 1.学习python，并利用它做点东西 2.对串口通信有更加深刻的了解 计划步骤： 1.先把Python的基础知识有大体了解 2.学习pyqt并绘制上位机界面 3.做好串口通信的工作 环境的搭建环境搭建参考： 环境搭建 再由.ui转成.py时，再最后添加：参考 Ui_MainWindow 要和前面类名一致 1234567891011class MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): def __init__(self): super(MyWindow, self).__init__() self.setupUi(self)if __name__ == '__main__': import sys app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() mywindow.show() sys.exit(app.exec_()) 槽函数 未完待续~~]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>上位机</tag>
        <tag>Python</tag>
        <tag>pyqt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[柔性负载-肖曦]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%9F%94%E6%80%A7%E8%B4%9F%E8%BD%BD-%E8%82%96%E6%9B%A6%2F</url>
    <content type="text"><![CDATA[关于肖老师柔性负载的文献综述一、柔性负载建模–中心刚体-悬臂梁系统(欧拉-伯努利梁)[5]应用于 工业机器人中的柔性机械臂。单柔性连杆伺服驱动系统如下图所示。图中：u(x,t)为挠性负载在x处的挠度；θm(t)为伺服电机转轴的转角；Ta为伺服电机驱动转矩。 二、PMSM驱动柔性负载相关公式：[2]在对柔性负载建模后， 该系统的传递函数如下:(Ta表示电磁转矩，参数Ω 表示系统谐振频率， 而 Fa 则可以表示每一阶模态频率的谐振程度，通过系统模型和参数可以很容易地观察系统谐振状况。)(一般选取一阶模态，即Fa，Ω均为标量) 转速环和电流环传递函数 系统控制框图 (1)电流环特征–柔性负载影响小 刚性电流环开环传递函数 与上面的 柔性负载 电流环传递函数相比，其差别主要在分母的第三项。故可以对其分母第三项单独分析。 但是，由于柔性负载的振动频率与电流环带宽差别较大，因而，柔性对 PMSM 电流环的影响较小. (2)转速环环特征–柔性负载影响大 刚性负载 转速外环控制框图 柔性负载 转速外环控制框图（在不考虑电流环影响下，由系统框体可得） 转速环开环伯德图 结论： 在 PMSM 直接驱动柔性负载系统中，负载柔性对系统电流环影响较小，对转速环影响较大。 需要在 柔性负载振荡频率处 进行谐振补偿 三、柔性负载常用控制方法 转速环设计(1)PI设计[3] 转速环的开环传递函数(II型系统) （系统谐振模态幅值 η ） 在不考虑电流内环的影响，采用PI调节器 。转速外环的控制框图如下 (2)状态反馈 + PI 调节器控制[4] 在PI调节器的基础上，估计系统谐振模态幅值 η 和 负载转矩 TL 其中 GFF(s)是Wm与 TL 解耦得到 并存在 k1、k2、kP、kI 四个可调参数，因而四个极点能够任意配置. 参考文献[1]丁有爽,肖曦.基于负载位置反馈的永磁同步电机驱动柔性负载谐振抑制方法[J].电工技术学报,2017,32(11):96-110. [2]丁有爽,肖曦.永磁同步电机直接驱动柔性负载控制方法[J].电工技术学报,2017,32(04):123-132. [3]丁有爽,肖曦.基于极点配置的永磁同步电机驱动柔性负载PI调节器参数确定方法[J].中国电机工程学报,2017,37(04):1225-1239. [4]丁有爽,肖曦.基于状态反馈和转矩补偿的永磁同步电机驱动柔性负载控制方法[J].中国电机工程学报,2017,37(13):3892-3900. [5]丁有爽,肖曦.伺服系统柔性负载建模方法研究[J].中国电机工程学报,2016,36(03):818-827. [6] Hori Y，Sawada H，Chun Y．Slow resonance ratio control for vibration suppression and disturbance rejection in torsional system[J]．IEEE Transactions on Industrial Electronics，1999，46(1)：162-168]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>柔性负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于加入站内搜索]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%85%A5%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[今天按照网上的加入本站搜索功能 无论如何都加不进去，后来在查看 NEXT给的官方解读中看到解决方法网址如下，查看Local Search方法 https://theme-next.org/docs/third-party-services/search-services]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于DSP平台双UDE 参数调试]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%85%B3%E4%BA%8EDSP%E5%B9%B3%E5%8F%B0%E5%8F%8CUDE%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1.挑选了几组典型数据画图，这里面的滤波器参数都是随意设置的（因为现在还无法确定kp和alpha之间的关系） 2.xx_xxx3.fig 中红色为实际数值 蓝色为指令，绿色为经过滤波器的输出结果(画图程序在最后)。 3.图中的时间 10000点 = 5s 4.Main_twoloop190801.c为源程序。 5.文件夹中 01-07为电流环调试过程, 08-11为速度环调试过程, 12 给了一个比较极端的速度环滤波器参数。 数据对应参数： spd_Factor spd_kp spd_ki iq_Factor iq_kp iq_ki 01参数 0.1667 0.8 0.0015 0.007 1.0 0.0025 05参数 0.1667 0.8 0.0015 0.007 2.1 0.0025 06参数 0.1667 0.8 0.0015 0.007 2.5 0.0025 10参数 0.1667 3.0 0.0015 0.007 2.5 0.0025 12参数 0.0007 0.8 0.0015 0.007 2.5 0.0025 6.画图函数 1234567891011121314figure(1)plot(t,spd,'r','LineWidth',1)hold onplot(t,spdr-5,'g','LineWidth',1)hold onplot(t,spdc-5,'b','LineWidth',1)grid onfigure(2)plot(t,iq,'r','LineWidth',1)hold onplot(t,iqr,'g','LineWidth',1)hold onplot(t,iqc,'b','LineWidth',1)grid on]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>PMSM</tag>
        <tag>UDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不懂的知识点 查阅 汇总]]></title>
    <url>%2F2019%2F08%2F01%2F%E4%B8%8D%E6%87%82%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[单工、半双工和全双工的区别 一、单工1、数据只在一个方向上传输，不能实现双方通信。 2、栗子：电视、广播。 二、半双工1、允许数据在两个方向上传输，但是同一时间数据只能在一个方向上传输，其实际上是切换的单工。 2、栗子：对讲机。 三、全双工1、允许数据在两个方向上同时传输。 2、栗子：手机通话。 TCP/IP （传输控制协议）Transmission Control Protocol TCP 用于应用程序之间的通信。当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信。 IP 用于计算机之间的通信。IP 是无连接的通信协议。它不会占用两个正在通信的计算机之间的通信线路。这样，IP 就降低了对网络线路的需求。每条线可以同时满足许多不同的计算机之间的通信需要。 TCP/IP 意味着 TCP 和 IP 在一起协同工作。TCP 负责应用软件（比如你的浏览器）和网络软件之间的通信。IP 负责计算机之间的通信。TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。IP 负责将包发送至接受者。 api （应用程序编程接口）API 是一套明确定义的各种软件组件之间的通信方法。 http、MQTT、CoAPHTTP是一个简单的请求-响应协议，它通常运行在TCP之上。它指定了客户端可能发送给服务器什么样的消息以及得到什么样的响应。 MQTT（Message Queuing Telemetry Transport，消息队列遥测传输协议），是一种基于发布/订阅（publish/subscribe）模式的”轻量级”通讯协议，该协议构建于TCP/IP协议上，由IBM在1999年发布。MQTT最大优点在于，可以以极少的代码和有限的带宽，为连接远程设备提供实时可靠的消息服务。支持长连接！适用于抄表 CoAP 由于物联网中的很多设备都是资源受限型的，即只有少量的内存空间和有限的计算能力，所以传统的HTTP协议应用在物联网上就显得过于庞大而不适用。 IETF的CoRE工作组提出了一种基于REST架构的CoAP协议;是一种在物联网世界的类web协议。不支持长连接 基于UDO 可靠性不高 适用于智能家居 OSI七层模型/TCP/IP五层模型：OSI七层模型(整个过程以公司A和公司B的一次商业报价单发送为例子进行讲解。) &lt;1&gt; 应用层 OSI参考模型中最靠近用户的一层，是为计算机用户提供应用接口，也为用户直接提供各种网络服务。我们常见应用层的网络服务协议有：HTTP，HTTPS，FTP，POP3、SMTP等。 实际公司A的老板就是我们所述的用户，而他要发送的商业报价单，就是应用层提供的一种网络服务，当然，老板也可以选择其他服务，比如说，发一份商业合同，发一份询&gt; 价单，等等。 &lt;2&gt;表示层 表示层提供各种用于应用层数据的编码和转换功能,确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于将计算机内部的多种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。 由于公司A和公司B是不同国家的公司，他们之间的商定统一用英语作为交流的语言，所以此时表示层（公司的文秘），就是将应用层的传递信息转翻译成英语。同时为了防止别的公司看到，公司A的人也会对这份报价单做一些加密的处理。这就是表示的作用，将应用层的数据转换翻译等。 &lt;3&gt;会话 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。 会话层的同事拿到表示层的同事转换后资料，（会话层的同事类似公司的外联部），会话层的同事那里可能会掌握本公司与其他好多公司的联系方式，这里公司就是实际传递过程中的实体。他们要管理本公司与外界好多公司的联系会话。当接收到表示层的数据后，会话层将会建立并记录本次会话，他首先要找到公司B的地址信息，然后将整份资料放进信封，并写上地址和联系方式。准备将资料寄出。等到确定公司B接收到此份报价单后，此次会话就算结束了，外联部的同事就会终止此次会话。 &lt;4&gt;传输层 传输层建立了主机端到端的链接，传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务，包括处理差错控制和流量控制等问题。该层向高层屏蔽了下层数据通信的细节，使高层用户看到的只是在两个传输实体间的一条主机到主机的、可由用户控制和设定的、可靠的数据通路。我们通常说的， TCP UDP 就是在这一层。端口号既是这里的“端”。 传输层就相当于公司中的负责快递邮件收发的人，公司自己的投递员，他们负责将上一层的要寄出的资料投递到快递公司或邮局。 &lt;5&gt;网络层 本层通过IP寻址来建立两个节点之间的连接，为源端的运输层送来的分组，选择合适的路由和交换节点，正确无误地按照地址传送给目的端的运输层。就是通常说的IP层。这一层就是我们经常说的IP协议层。IP协议是Internet的基础。 网络层就相当于快递公司庞大的快递网络，全国不同的集散中心，比如说，从深圳发往北京的顺丰快递（陆运为例啊，空运好像直接就飞到北京了），首先要到顺丰的深圳集散中心，从深圳集散中心再送到武汉集散中心，从武汉集散中心再寄到北京顺义集散中心。这个每个集散中心，就相当于网络中的一个IP节点。 &lt;6&gt;数据链路层 将比特组合成字节,再将字节组合成帧,使用链路层地址 (以太网使用MAC地址)来访问介质,并进行差错检测。数据链路层又分为2个子层：逻辑链路控制子层（LLC）和媒体访问控制子层（MAC）。MAC子层处理CSMA/CD算法、数据出错校验、成帧等；LLC子层定义了一些字段使上次协议能共享数据链路层。 在实际使用中，LLC子层并非必需的。 这个没找到合适的例子 &lt;7&gt; 物理层 实际最终信号的传输是通过物理层实现的。通过物理介质传输比特流。规定了电平、速度和电缆针脚。常用设备有（各种物理设备）集线器、中继器、调制解调器、网线、双绞线、同轴电缆。这些都是物理层的传输介质。 快递寄送过程中的交通工具，就相当于我们的物理层，例如汽车，火车，飞机，船。 [ TCP/IP五层模型 TCP/IP五层协议和OSI的七层协议对应关系如下。 在每一层都工作着不同的设备，比如我们常用的交换机就工作在数据链路层的，一般的路由器是工作在网络层的。 在每一层实现的协议也各不同，即每一层的服务也不同。下图列出了每层主要的协议。 鉴权 鉴权（authentication）是指验证用户是否拥有访问系统的权利。 CIG、IOCM、DM Server、Mongo DBCIG (Cell Interconnection Gateway) 信元互连网关 $\color{red}{未完待续~~}$]]></content>
      <tags>
        <tag>IOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交流电机前言讲座考试总结]]></title>
    <url>%2F2019%2F07%2F29%2F%E4%BA%A4%E6%B5%81%E7%94%B5%E6%9C%BA%E5%89%8D%E6%B2%BF%E8%AE%B2%E5%BA%A7%E8%80%83%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[简单的做一下知识点梳理。 电机的控制问题 交流电机 同步电机 附赠：电机的应用]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于404问题及感谢]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%85%B3%E4%BA%8E404%E9%97%AE%E9%A2%98%E5%8F%8A%E6%84%9F%E8%B0%A2%2F</url>
    <content type="text"><![CDATA[CNAME里面是写xxx.github.io 解析域名的时候最好用CNAME并 解析到 xxx.github.io GitHub 仓库里的Setting 最好也改成www.xxx.xxx 顺便感谢一下搭建博客参考网站 UP：CodeSheep UP：吃饱睡觉的猫 遇见西门]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于dsp平台电流环解耦]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%85%B3%E4%BA%8EDSP%E5%B9%B3%E5%8F%B0%E7%94%B5%E6%B5%81%E7%8E%AF%E8%A7%A3%E8%80%A6%2F</url>
    <content type="text"><![CDATA[120V电压 400rpm的给定电流 应该加的补偿值 ​ttt = 0.00105 \* \_IQmpy(pi_id.Fbk,pi_spd.Fbk) \* 4500 \* 9 /(volt1.DcBusVolt\*409.9) + 0.065 \* pi_spd.Fbk \* 4500 /(volt1.DcBusVolt\*409.9); 下面是解耦后应该的公式. ​ipark1.Qs = pi_iq.Out + ttt;​ 在未解耦的时候，测得ipark1.Qs = 0.515 . 换算成真实的电压值为(乘当前的电流值) 0.515 * 120 V=61.8V 测得ttt = 0.22 . 换算成真实的电压值为26.4V. ttt的主要成份是反电势 ​flux\*we=0.1552\*400\*4\*3.14/30=26.0V​ 所以证明电流环解耦 程序是正确的. 实验结果 （右为解耦）]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>PMSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first generage blog]]></title>
    <url>%2F2019%2F07%2F23%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[第一个博客 杠杠滴]]></content>
  </entry>
</search>
