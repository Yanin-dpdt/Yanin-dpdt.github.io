<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL语句复习]]></title>
    <url>%2F2020%2F05%2F31%2F%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[距离上一次学习SQL已经过去N多天了,碰巧在知乎看到了一千行MySQL学习笔记 顺便借此机会复习一下！12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028/* 启动MySQL */net start mysql/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码/* 跳过权限验证登录MySQL */mysqld --skip-grant-tables-- 修改root密码密码加密函数password()update mysql.user set password=password(&apos;root&apos;);SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- /* 数据库操作 */ -------------------- 查看当前数据库 select database();-- 显示当前时间、用户名、数据库版本 select now(), user(), version();-- 创建库 create database[ if not exists] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 show databases[ like &apos;pattern&apos;]-- 查看当前库信息 show create database 数据库名-- 修改库的选项信息 alter database 库名 选项信息-- 删除库 drop database[ if exists] 数据库名 同时删除该数据库相关的目录及其目录内容/* 表的操作 */ -------------------- 创建表 create [temporary] table[ if not exists] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 temporary 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &apos;string&apos;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 数据文件目录 DATA DIRECTORY = &apos;目录&apos; -- 索引文件目录 INDEX DIRECTORY = &apos;目录&apos; -- 表注释 COMMENT = &apos;string&apos; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &apos;pattern&apos;] SHOW TABLES FROM 表名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &apos;PATTERN&apos;] SHOW TABLE STATUS [FROM db_name] [LIKE &apos;pattern&apos;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 EG: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构 ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段名 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 ADD DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件]/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &apos;character_set_%&apos; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 set character_set_client = gbk; set character_set_results = gbk; set character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &apos;pattern&apos;]/SHOW CHARSET [LIKE &apos;pattern&apos;] 查看所有字符集 SHOW COLLATION [LIKE &apos;pattern&apos;] 查看所有校对集 charset 字符集编码 设置字符集编码 collate 校对集编码 设置校对集编码/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&apos;123&apos;，补填后为&apos;00123&apos; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155 datetime “YYYY-MM-DD hh:mm:ss”timestamp “YY-MM-DD hh:mm:ss” “YYYYMMDDhhmmss” “YYMMDDhhmmss” YYYYMMDDhhmmss YYMMDDhhmmssdate “YYYY-MM-DD” “YY-MM-DD” “YYYYMMDD” “YYMMDD” YYYYMMDD YYMMDDtime “hh:mm:ss” “hhmmss” hhmmssyear “YYYY” “YY” YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&apos;男&apos;, &apos;女&apos;, &apos;无&apos;) ); insert into tab values (&apos;男, 女&apos;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。/* 选择类型 */-- PHP角度1. 功能满足2. 存储空间尽量小，处理效率更高3. 考虑兼容问题-- IP存储 ----------1. 只需存储，可用字符串2. 如果需计算，查找等，可存储为4个字节的无符号int，即unsigned 1) PHP函数转换 ip2long可转换为整型，但会出现携带符号问题。需格式化为无符号的整型。 利用sprintf函数格式化字符串 sprintf(&quot;%u&quot;, ip2long(&apos;192.168.3.134&apos;)); 然后用long2ip将整型转回IP字符串 2) MySQL函数转换(无符号整型，UNSIGNED) INET_ATON(&apos;127.0.0.1&apos;) 将IP转为整型 INET_NTOA(2130706433) 将整型转为IP /* 列属性（列约束） */ ------------------1. 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. unique 唯一索引（唯一约束） 使得某字段的值也不能重复。 3. null 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &apos;val&apos;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null 4. default 默认值属性 当前字段的默认值。 insert into tab values (default, &apos;val&apos;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. auto_increment 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. comment 注释 例：create table tab ( id int ) comment &apos;注释内容&apos;;7. foreign key 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。/* select */ ------------------select [all|distinct] select_expr from -&gt; where -&gt; group by [合计函数] -&gt; having -&gt; order by -&gt; limita. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. from 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 select * from tb1 as tt, tb2 as bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 select * from tb1, tb2;c. where 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. group by 子句, 分组子句 group by 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 group by 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. having 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. order by 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. limit 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. distinct, all 选项 distinct 去除重复记录 默认为 all, 全部记录/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id;/* 导入导出 */ ------------------select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理-- 控制格式fields 控制字段格式默认：fields terminated by &apos;\t&apos; enclosed by &apos;&apos; escaped by &apos;\\&apos; terminated by &apos;string&apos; -- 终止 enclosed by &apos;char&apos; -- 包裹 escaped by &apos;char&apos; -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE &apos;/tmp/result.text&apos; FIELDS TERMINATED BY &apos;,&apos; OPTIONALLY ENCLOSED BY &apos;&quot;&apos; LINES TERMINATED BY &apos;\n&apos; FROM test_table;lines 控制行格式默认：lines terminated by &apos;\n&apos; terminated by &apos;string&apos; -- 终止 /* insert */ ------------------select语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 insert into tbl_name set field=value,...；可以一次性使用多个值，采用(), (), ();的形式。 insert into tbl_name values (), (), ();可以在列值指定时，使用表达式。 insert into tbl_name values (field_value, 10+10, now());可以使用一个特殊值 default，表示该列使用默认值。 insert into tbl_name values (field_value, default);可以通过一个查询的结果，作为需要插入的值。 insert into tbl_name select ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 insert into tbl_name values/set/select on duplicate key update 字段=值, …;/* delete */ ------------------DELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]按照条件删除指定删除的最多记录数。Limit可以通过排序条件删除。order by + limit支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。/* truncate */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 -B 库名 &gt; 文件名(D:/a.sql)可以-w携带备份条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件/* 视图 */ ------------------什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name -- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。/* 事务(transaction) */ ------------------事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务)/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1[, str2,...])-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。 -- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取日期部分time(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取时间部分date_format(&apos;yyyy-mm-dd hh:ii:ss&apos;, &apos;%d %y %a %d %m %b %j&apos;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &apos;partten&apos; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END/* 用户和权限管理 */ ------------------用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &apos;user_name&apos;@&apos;192.168.1.1&apos; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&apos;密码&apos;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&apos;密码&apos;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &apos;password&apos;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \&apos;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\G&quot;, &quot;\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <tags>
        <tag>python</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PySpark学习回顾]]></title>
    <url>%2F2020%2F05%2F01%2FPySpark%E5%AD%A6%E4%B9%A0%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[1. 前言 三人行必有我师焉 – 中国古代谚语 本文是在GitHub上无意间找到的有关PySpark的宝贵学习资料，很全面。希望借此对之前的学习有更加深入的理解！ 2. 为什么是 Spark 和 Python 磨刀不误砍柴工。 – 中国古代谚语 我想从以下两个部分回答这个问题： 2.1. 为什么是 Spark我认为 Apache Spark™ 官网的以下四个主要原因足以说服您使用 Spark。 速度 在内存中运行程序比 Hadoop MapReduce 快 100 倍，或者比磁盘上运行快 10 倍。 Apache Spark 拥有先进的 DAG 执行引擎，支持非循环数据流和内存计算。 Hadoop 和 Spark 中的逻辑回归 易于使用 使用 Java，Scala，Python，R 快速编写应用。 Spark 提供 80 多个高级操作符，可以轻松构建并行应用。 您可以从 Scala，Python 和 R shell 中以交互方式使用它。 通用性 结合SQL，流式和复杂的分析。 Spark 支持很多库，包括 SQL 和 DataFrames，用于机器学习的 MLlib，GraphX 和 Spark Streaming。您可以在同一个应用中无缝地组合这些库。 Spark 技术栈 随处运行 Spark 在 Hadoop，Mesos，独立或云端运行。 它可以访问各种数据源，包括 HDFS，Cassandra，HBase 和 S3。 Spark 平台 2.2. 为什么是 PySpark?无论你喜欢与否，Python 都是最受欢迎的编程语言之一。 KDnuggets 分析/数据科学 2017 软件调查，来自 kdnuggets。 3. 配置运行平台 工欲善其事，必先利其器。 – 中国古代谚语 一个好的编程平台可以为您节省大量的麻烦和时间。 在这里，我将仅介绍如何安装我最喜欢的编程平台，并且只展示我在 Linux 系统上设置它的最简单的方法。 如果要在其他操作系统上安装，可以通过搜索引擎。 在本节中，您可以学习如何在相应的编程平台和包上设置 Pyspark。 3.1. 在 Databricks 社区云上运行如果您对 Linux 或 Unix 操作系统没有任何经验，我很乐意建议您在 Databricks 社区云上使用 Spark。 因为你不需要设置 Spark，它对于社区版来说完全是免费的**。 请按照下面列出的步骤操作。 在 https://community.cloud.databricks.com/login.html 建立账户： 使用您的帐户登录，然后您可以创建集群（计算机），表（数据集）和笔记本（代码）。 创建运行代码的集群 导入你的数据集 注意 您需要保存Uploaded to DBFS中显示的路径: /FileStore/tables/05rmhuqv1489687378010/，由于我们会使用这个路径来上传数据集。 创建你的笔记本 完成上述 5 个步骤后，您就可以在 Databricks 社区云上运行 Spark 代码了。 我将在 Databricks 社区云上运行以下所有演示。在运行演示代码时，希望您将获得以下结果： 1234567891011121314151617+---+-----+-----+---------+-----+|_c0| TV|Radio|Newspaper|Sales|+---+-----+-----+---------+-----+| 1|230.1| 37.8| 69.2| 22.1|| 2| 44.5| 39.3| 45.1| 10.4|| 3| 17.2| 45.9| 69.3| 9.3|| 4|151.5| 41.3| 58.5| 18.5|| 5|180.8| 10.8| 58.4| 12.9|+---+-----+-----+---------+-----+only showing top 5 rowsroot |-- _c0: integer (nullable = true) |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 3.2. 在 Mac 和 Ubuntu 上配置 Spark3.2.1. 安装先决条件我强烈建议您安装 Anaconda，因为它包含大部分先决条件并支持多个操作系统。 安装 Python 转到 Ubuntu 软件中心并按照以下步骤操作： 打开 Ubuntu 软件中心 搜索 python 并点击“安装” 或者打开终端执行以下命令： 123456sudo apt-get install build-essential checkinstallsudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-devsudo apt-get install pythonsudo easy_install pipsudo pip install ipython 3.2.2. 安装 JavaJava 被许多其他软件使用。 所以你很可能已经安装了它。 您可以在命令提示符中使用以下命令： 1java -version 否则，您可以按照如何为我的 Mac 安装 Java？中的步骤，在 Mac 上安装 java 并在命令提示符中使用以下命令来在 Ubuntu 上安装： 123sudo apt-add-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installer 3.2.3. 安装 JRE我安装了 ORACLE Java JDK。 警告 安装 Java 和 Java SE 运行时环境的步骤非常重要，因为 Spark 是一种用 Java 编写的领域特定语言。 您可以在命令提示符中使用以下命令检查 Java 是否可用并找到它的版本： 1java -version 如果您的 Java 安装成功，您将获得如下的类似结果： 123java version &quot;1.8.0_131&quot;Java(TM) SE Runtime Environment (build 1.8.0_131-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) 3.2.4. 安装 Apache Spark实际上，预构建版本不需要安装。 你在解包时可以使用它。 下载：您可以从 下载 Apache Spark™ 获得预构建的 Apache Spark™。 解压缩：将 Apache Spark™ 解压缩到您要安装 Spark 的路径。 测试：测试先决条件：修改路径spark-#.#.#-bin-hadoop#.#/bin并运行 1./pyspark 12345678910111213141516171819202122Python 2.7.13 |Anaconda 4.4.0 (x86_64)| (default, Dec 20 2016, 23:05:08)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Anaconda is brought to you by Continuum Analytics.Please check out: http://continuum.io/thanks and https://anaconda.orgUsing Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR,use setLogLevel(newLevel).17/08/30 13:30:12 WARN NativeCodeLoader: Unable to load native-hadooplibrary for your platform... using builtin-java classes where applicable17/08/30 13:30:17 WARN ObjectStore: Failed to get database global_temp,returning NoSuchObjectExceptionWelcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &apos;_/ /__ / .__/\_,_/_/ /_/\_\ version 2.1.1 /_/Using Python version 2.7.13 (default, Dec 20 2016 23:05:08)SparkSession available as &apos;spark&apos;. 3.2.5. 配置 Spark Mac 操作系统：在终端打开你的bash_profile 1vim ~/.bash_profile 并将以下行添加到bash_profile（记得改变路径） 123456# 为 spark 添加export SPARK_HOME=your_spark_installation_pathexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinexport PATH=$PATH:$SPARK_HOME/binexport PYSPARK_DRIVE_PYTHON="jupyter"export PYSPARK_DRIVE_PYTHON_OPTS="notebook" 最后，记得执行你的bash_profile 1source ~/.bash_profile Ubuntu 操作系统：在终端打开bashrc 1vim ~/.bashrc 并将以下行添加到bashrc（记得改变路径） 123456# 为 spark 添加export SPARK_HOME=your_spark_installation_pathexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinexport PATH=$PATH:$SPARK_HOME/binexport PYSPARK_DRIVE_PYTHON="jupyter"export PYSPARK_DRIVE_PYTHON_OPTS="notebook" 最后，记得执行你的bash_profile 1source ~/.bashrc 3.3. 在 Windows 上配置 Spark在 Windows 上安装开源软件对我来说总是一场噩梦。 感谢 Deelesh Mandloi。 您可以按照博客 Windows 上的 PySpark 入门中的详细步骤，在 Windows 操作系统上安装 Apache Spark™。 3.4. PySpark 和文本编辑器或 IDE3.4.1. PySpark 和 Jupyter 笔记本完成在 Mac 和 Ubuntu 上配置 Spark中的上述设置步骤后，您应该在 Jupyter 笔 记本中编写和运行 PySpark 代码。 3.4.2. PySpark 和 Apache Zeppelin完成在 Mac 和 Ubuntu 上配置 Spark中的上述设置步骤后，您应该在 Apache Zeppelin 中编写和运行 PySpark 代码。 3.4.3. PySpark 和 Sublime Text完成在 Mac 和 Ubuntu 上配置 Spark中的上述设置步骤后，您应该可以使用 Sublime Text 编写 PySpark 代码,并在终端中将代码作为普通的 python 代码运行。 1python test_pyspark.py 然后你应该在你的终端获得输出结果。 3.4.4. PySpark 和 Eclipse如果要在 Eclipse 上运行 PySpark 代码，则需要为当前项目添加外部库的路径，如下所示： 打开你的项目的属性 为外部添加路径 然后你应该足以用 PyDev 在 Eclipse 上运行你的代码。 3.5. PySparkling 水: Spark + H2O 从 https://s3.amazonaws.com/h2o-release/sparkling-water/rel-2.4/5/index.html 下载Sparkling Water： 测试 PySparking 123unzip sparkling-water-2.4.5.zipcd ~/sparkling-water-2.4.5/bin./pysparkling 如果您有正确设置了 PySpark，那么您将获得以下结果： 12345678910111213141516171819202122232425Using Spark defined in the SPARK_HOME=/Users/dt216661/spark environmental propertyPython 3.7.1 (default, Dec 14 2018, 13:28:58)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.2019-02-15 14:08:30 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2019-02-15 14:08:31 WARN Utils:66 - Service &apos;SparkUI&apos; could not bind on port 4040\. Attempting port 4041.2019-02-15 14:08:31 WARN Utils:66 - Service &apos;SparkUI&apos; could not bind on port 4041\. Attempting port 4042.17/08/30 13:30:12 WARN NativeCodeLoader: Unable to load native-hadooplibrary for your platform... using builtin-java classes where applicable17/08/30 13:30:17 WARN ObjectStore: Failed to get database global_temp,returning NoSuchObjectExceptionWelcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &apos;_/ /__ / .__/\_,_/_/ /_/\_\ version 2.4.0 /_/Using Python version 3.7.1 (default, Dec 14 2018 13:28:58)SparkSession available as &apos;spark&apos;. 使用 Jupyter notebook pysparkling 将以下别名添加到bashrc（Linux 系统）或bash_profile（Mac 系统） 1alias sparkling="PYSPARK_DRIVER_PYTHON="ipython" PYSPARK_DRIVER_PYTHON_OPTS= "notebook" /~/~/sparkling-water-2.4.5/bin/pysparkling" 在终端打开pysparkling 1sparkling 3.6. 在云上配置 Spark按照在 Mac 和 Ubuntu 上配置 Spark中的设置步骤，您可以在云上设置自己的集群，例如 AWS，Google Cloud。 实际上，对于那些云，他们有自己的大数据工具。 你可以直接在任何设置上运行它们，就像 Databricks 社区云一样。 如果您想了解更多详情，请随时与作者联系。 3.7. 这一节的示例代码此部分的代码可在test_pyspark下载，Jupyter 笔记本可从test_pyspark_ipynb下载。 Python 源代码 12345678910111213141516## 建立 SparkSessionfrom pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark SQL basic example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()df = spark.read.format('com.databricks.spark.csv').\ options(header='true', \ inferschema='true').\ load("/home/feng/Spark/Code/data/Advertising.csv",header=True)df.show(5)df.printSchema() 4. Apache Spark 入门知己知彼，百战百胜。 – 《孙子兵法》 4.1. 核心概念以下大部分内容来自 [Kirillov2016]。 所以版权属于 Anton Kirillov。我向您推荐 Apache Spark 核心概念，架构和内部来获取更多详细信息。 在深入研究 Apache Spark 之前，让我们阅读 Apache Spark 的行话。 作业：从 HDFS 或本地读取一些输入的代码，对数据执行一些计算并写入一些输出数据。 阶段：工作分为几个阶段。 阶段被分类为映射或归约阶段（如果您已经使用过 Hadoop 并希望关联，则更容易理解）。 阶段基于计算边界划分，所有计算（操作符）不能在单个阶段中更新。 它发生在很多阶段。 任务：每个阶段都有一些任务，每个任务一个分区。 一个任务执行在一个执行器（机器）上的一个数据分区上。 DAG：DAG 代表有向无环图，在本文中是操作符的 DAG。 执行器：负责执行任务的进程。 主机：运行驱动程序的机器 从机：运行执行程序的机器 4.2. Spark 组件 Spark 驱动 隔离进程来执行用户应用 创建SparkContext来调度任务执行并与集群管理器协商 执行器 运行由驱动调度的任务 在内存、磁盘或者 off-heap 中储存计算结果 与储存系统个交互 集群管理器 Mesos YARN Spark Standalone Spark 驱动包含更多组件，负责将用户代码转换为集群上的实际作业： SparkContext 表示 spark 集群的连接，可用于在该集群上创建 RDD，累加器和广播变量 DAGScheduler 计算机每个作业的阶段的 DAG 并将它们提交给TaskScheduler，来确定任务的首选位置（基于缓存状态或随机文件位置）并找到运行作业的最小调度 TaskScheduler 负责将任务发送到集群，运行它们，在发生故障时重试，以及减轻负担 SchedulerBackend 用于调度系统的后端接口，允许插入不同的实现（mesos，yarn，standalone，local） BlockManager 提供接口，用于在本地和远程将块放置到各种存储（内存，磁盘和堆外）和检索 4.3. 架构4.4. Spark 的工作原理Spark 具有较小的代码库，系统分为不同的层。 每一层都有一些责任。 这些层彼此独立。 第一层是解释器，Spark 使用 Scala 解释器，并进行了一些修改。 当您在 spark 控制台中输入代码（创建 RDD 并应用操作符）时，Spark 会创建一个操作符图。 当用户运行操作（如收集）时，图将提交给 DAG 调度器。 DAG 调度器将操作符图分为（映射和归约）阶段。 阶段由基于输入数据的分区的任务组成。 DAG 调度器将操作符连接在一起来优化图。 例如 许多图的操作符可以安排在一个阶段中。 此优化是 Sparks 性能的关键。 DAG 调度器的最终结果是一组阶段。 这些阶段将传递给任务调度器。 任务调度器通过集群管理器启动任务（Spark Standalone/Yarn/Mesos）。 任务调度器不知道阶段之间的依赖关系。 5. 使用 RDD 的编程知彼知己，百战不殆；不知彼而知己，一胜一负；不知彼，不知己，每战必殆。 – 《孙子兵法》 RDD 表示弹性分布式数据集。 Spark 中的 RDD 只是对象集的不可变分布式集合。 每个 RDD 被分成多个分区（具有较小集合的类似模式），可以在集群的不同节点上计算。 5.1. 创建 RDD通常，有两种创建 RDD 的流行方法：加载外部数据集或分发一组对象集合。 以下示例显示了一些使用parallelize()函数创建 RDD 的最简单方法，该方法接受程序中已有的集合，并将其传递给 Spark 上下文。 通过使用parallelize( )函数 1234567891011from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark create RDD example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'), (4, 5, 6, 'd e f'), (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4']) 然后你将获得 RDD 数据： 123456789df.show()+----+----+----+-----+|col1|col2|col3| col4|+----+----+----+-----+| 1| 2| 3|a b c|| 4| 5| 6|d e f|| 7| 8| 9|g h i|+----+----+----+-----+ 123456789from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark create RDD example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)]) 然后你将获得 RDD 数据： 123myData.collect()[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)] 通过使用createDataFrame( )函数： 123456789101112131415from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark create RDD example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()Employee = spark.createDataFrame([ ('1', 'Joe', '70000', '1'), ('2', 'Henry', '80000', '2'), ('3', 'Sam', '60000', '2'), ('4', 'Max', '90000', '1')], ['Id', 'Name', 'Sallary','DepartmentId'] ) 然后你将获得 RDD 数据： 12345678+---+-----+-------+------------+| Id| Name|Sallary|DepartmentId|+---+-----+-------+------------+| 1| Joe| 70000| 1|| 2|Henry| 80000| 2|| 3| Sam| 60000| 2|| 4| Max| 90000| 1|+---+-----+-------+------------+ 通过使用read和load函数 从 .csv 文件读取数据集 12345678910111213141516## 建立 SparkSessionfrom pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark create RDD example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()df = spark.read.format('com.databricks.spark.csv').\ options(header='true', \ inferschema='true').\ load("/home/feng/Spark/Code/data/Advertising.csv",header=True)df.show(5)df.printSchema() 然后你将获得 RDD 数据： 1234567891011121314151617+---+-----+-----+---------+-----+|_c0| TV|Radio|Newspaper|Sales|+---+-----+-----+---------+-----+| 1|230.1| 37.8| 69.2| 22.1|| 2| 44.5| 39.3| 45.1| 10.4|| 3| 17.2| 45.9| 69.3| 9.3|| 4|151.5| 41.3| 58.5| 18.5|| 5|180.8| 10.8| 58.4| 12.9|+---+-----+-----+---------+-----+only showing top 5 rowsroot |-- _c0: integer (nullable = true) |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 创建后，RDD 提供两种类型的操作：转换和动作。 从数据库读取数据集 12345678910111213141516171819202122## 建立 SparkSessionfrom pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("Python Spark create RDD example") \ .config("spark.some.config.option", "some-value") \ .getOrCreate()## 用户信息user = 'your_username'pw = 'your_password'## 数据库信息table_name = 'table_name'url = 'jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'&amp;password='+pwproperties =&#123;'driver': 'org.postgresql.Driver', 'password': pw,'user': user&#125;df = spark.read.jdbc(url=url, table=table_name, properties=properties)df.show(5)df.printSchema() 然后你将获得 RDD 数据： 1234567891011121314151617+---+-----+-----+---------+-----+|_c0| TV|Radio|Newspaper|Sales|+---+-----+-----+---------+-----+| 1|230.1| 37.8| 69.2| 22.1|| 2| 44.5| 39.3| 45.1| 10.4|| 3| 17.2| 45.9| 69.3| 9.3|| 4|151.5| 41.3| 58.5| 18.5|| 5|180.8| 10.8| 58.4| 12.9|+---+-----+-----+---------+-----+only showing top 5 rowsroot |-- _c0: integer (nullable = true) |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 注意 从数据库中读取表，需要相应数据库的正确驱动。 例如，上面的演示需要org.postgresql.Driver，你需要下载它并将它放在你的 spark 安装路径的jars文件夹中。 我从官方网站下载postgresql-42.1.1.jar并将其放在jars文件夹中。 从 HDFS 读取数据 123456789101112from pyspark.conf import SparkConffrom pyspark.context import SparkContextfrom pyspark.sql import HiveContextsc= SparkContext('local','example')hc = HiveContext(sc)tf1 = sc.textFile("hdfs://cdhstltest/user/data/demo.CSV")print(tf1.first())hc.sql("use intg_cme_w")spf = hc.sql("SELECT * FROM spf LIMIT 100")print(spf.show(5)) 5.2. Spark 操作 警告 以下所有数据均来自J effrey Thompson。 感兴趣的读者可以参考 pyspark 图片。 Spark 操作有两种主要类型：转换和动作 [Karau2015]。 注意 有些人定义了三种类型的操作：转换，动作和打乱。 5.2.1. Spark 转换转换从前一个 RDD 构建新的 RDD。 例如，一个常见的转换是过滤匹配谓词的数据。 5.2.2. Spark 动作另一方面，动作基于 RDD 计算结果，并将其返回到驱动，或将其保存到外部存储系统（例如，HDFS）。 5.3. rdd.DataFrame VS pd.DataFrame5.3.1. 创建DataFrame来自列表 12my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]col_name = ['A', 'B', 'C'] Python 代码： 1234# 注意 columns=pd.DataFrame(my_list,columns= col_name)#spark.createDataFrame(my_list, col_name).show() 比较： 1234567 +---+---+---+ | A| B| C| A B C +---+---+---+0 a 1 2 | a| 1| 2|1 b 2 3 | b| 2| 3|2 c 3 4 | c| 3| 4| +---+---+---+ 注意 注意pd.DataFrame中的参数columns=，由于默认值将使列表成为行。 Python 代码： 1234# 注意 columns=pd.DataFrame(my_list, columns= col_name)#pd.DataFrame(my_list, col_name) 比较： 1234 A B C 0 1 20 a 1 2 A a 1 21 b 2 3 B b 2 32 c 3 4 C c 3 4 来自字典 123d = &#123;'A': [0, 1, 0], 'B': [1, 0, 1], 'C': [1, 0, 0]&#125; Python 代码： 123pd.DataFrame(d)for# PySpark 很麻烦spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show() 比较： 1234567 +---+---+---+ | A| B| C| A B C +---+---+---+0 0 1 1 | 0| 1| 1|1 1 0 0 | 1| 0| 0|2 0 1 0 | 0| 1| 0| +---+---+---+ 5.3.2. 加载DataFrame来自数据库 大多数情况下，您需要与同事共享代码，或为了代码审查或质量保证（QA）发布代码。 您肯定不希望在代码中包含您的“用户信息”。 所以你可以将它们保存在login.txt中： 12runawayhorse001PythonTips 并使用以下代码导入您的“用户信息”： 12345678910111213# 用户信息try: login = pd.read_csv(r'login.txt', header=None) user = login[0][0] pw = login[0][1] print('User information is ready!')except: print('Login information is not available!!!')# 数据库信息host = '##.###.###.##'db_name = 'db_name'table_name = 'table_name' 比较： 12345678conn = psycopg2.connect(host=host, database=db_name, user=user, password=pw)cur = conn.cursor()sql = """ select * from &#123;table_name&#125; """.format(table_name=table_name)dp = pd.read_sql(sql, conn) 1234# 连接数据库url = 'jdbc:postgresql://'+host+':5432/'+db_name+'?user='+user+'&amp;password='+pwproperties =&#123;'driver': 'org.postgresql.Driver', 'password': pw,'user': user&#125;ds = spark.read.jdbc(url=url, table=table_name, properties=properties) 注意 使用 PySpark 从数据库中读取表，需要相应数据库的正确驱动。 例如，上面的演示需要org.postgresql.Driver，你需要下载它并将它放在你的 spark 安装路径的jars文件夹中。 我从官方网站下载postgresql-42.1.1.jar并将其放在jars文件夹中。 来自.csv 比较： 123456789# pd.DataFrame dp: DataFrame pandasdp = pd.read_csv('Advertising.csv')#rdd.DataFrame. dp: DataFrame sparkds = spark.read.csv(path='Advertising.csv',# sep=',',# encoding='UTF-8',# comment=None, header=True, inferSchema=True) 来自.json 数据来自：http://api.luftdaten.info/static/v1/data.json 12dp = pd.read_json("data/data.json")ds = spark.read.json('data/data.json') Python 代码： 123dp[['id','timestamp']].head(4)#ds[['id','timestamp']].show(4) 比较： 123456789 +----------+-------------------+ | id| timestamp| id timestamp +----------+-------------------+0 2994551481 2019-02-28 17:23:52 |2994551481|2019-02-28 17:23:52|1 2994551482 2019-02-28 17:23:52 |2994551482|2019-02-28 17:23:52|2 2994551483 2019-02-28 17:23:52 |2994551483|2019-02-28 17:23:52|3 2994551484 2019-02-28 17:23:52 |2994551484|2019-02-28 17:23:52| +----------+-------------------+ only showing top 4 rows 5.3.3. 前n行Python 代码： 123dp.head(4)#ds.show(4) 比较： 123456789 +-----+-----+---------+-----+ | TV|Radio|Newspaper|Sales| TV Radio Newspaper Sales +-----+-----+---------+-----+0 230.1 37.8 69.2 22.1 |230.1| 37.8| 69.2| 22.1|1 44.5 39.3 45.1 10.4 | 44.5| 39.3| 45.1| 10.4|2 17.2 45.9 69.3 9.3 | 17.2| 45.9| 69.3| 9.3|3 151.5 41.3 58.5 18.5 |151.5| 41.3| 58.5| 18.5| +-----+-----+---------+-----+ only showing top 4 rows 5.3.4. 列名Python 代码： 123dp.columns#ds.columns 比较： 12Index(['TV', 'Radio', 'Newspaper', 'Sales'], dtype='object')['TV', 'Radio', 'Newspaper', 'Sales'] 5.3.5. 数据类型Python 代码： 123dp.dtypes#ds.dtypes 比较： 12345TV float64 [('TV', 'double'),Radio float64 ('Radio', 'double'),Newspaper float64 ('Newspaper', 'double'),Sales float64 ('Sales', 'double')]dtype: object 5.3.6. 填充空值123456my_list = [['a', 1, None], ['b', 2, 3],['c', 3, 4]]dp = pd.DataFrame(my_list,columns=['A', 'B', 'C'])ds = spark.createDataFrame(my_list, ['A', 'B', 'C'])#dp.head()ds.show() 比较： 1234567 +------+---+----+ | A| B| C| A B C +------+---+----+0 male 1 NaN | male| 1|null|1 female 2 3.0 |female| 2| 3|2 male 3 4.0 | male| 3| 4| +------+---+----+ Python 代码： 123dp.fillna(-99)#ds.fillna(-99).show() 比较： 1234567 +------+---+----+ | A| B| C| A B C +------+---+----+0 male 1 -99 | male| 1| -99|1 female 2 3.0 |female| 2| 3|2 male 3 4.0 | male| 3| 4| +------+---+----+ 5.3.7. 替换值Python 代码： 12345# 警告：您需要选择特定的列dp.A.replace(['male', 'female'],[1, 0], inplace=True)dp# 警告：不支持混合类型替换ds.na.replace(['male','female'],['1','0']).show() 比较： 1234567 +---+---+----+ | A| B| C| A B C +---+---+----+0 1 1 NaN | 1| 1|null|1 0 2 3.0 | 0| 2| 3|2 1 3 4.0 | 1| 3| 4| +---+---+----+ 5.3.8. 重命名列重命名所有列 Python 代码： 1234dp.columns = ['a','b','c','d']dp.head(4)#ds.toDF('a','b','c','d').show(4) 比较： 123456789 +-----+----+----+----+ | a| b| c| d| a b c d +-----+----+----+----+0 230.1 37.8 69.2 22.1 |230.1|37.8|69.2|22.1|1 44.5 39.3 45.1 10.4 | 44.5|39.3|45.1|10.4|2 17.2 45.9 69.3 9.3 | 17.2|45.9|69.3| 9.3|3 151.5 41.3 58.5 18.5 |151.5|41.3|58.5|18.5| +-----+----+----+----+ only showing top 4 rows 重命名一列或多列 1mapping = &#123;'Newspaper':'C','Sales':'D'&#125; Python 代码： 1234dp.rename(columns=mapping).head(4)#new_names = [mapping.get(col,col) for col in ds.columns]ds.toDF(*new_names).show(4) 比较： 123456789 +-----+-----+----+----+ | TV|Radio| C| D| TV Radio C D +-----+-----+----+----+0 230.1 37.8 69.2 22.1 |230.1| 37.8|69.2|22.1|1 44.5 39.3 45.1 10.4 | 44.5| 39.3|45.1|10.4|2 17.2 45.9 69.3 9.3 | 17.2| 45.9|69.3| 9.3|3 151.5 41.3 58.5 18.5 |151.5| 41.3|58.5|18.5| +-----+-----+----+----+ only showing top 4 rows 注意 您还可以使用withColumnRenamed重命名 PySpark 中的一列。 Python 代码： 1ds.withColumnRenamed('Newspaper','Paper').show(4) 比较： 123456789+-----+-----+-----+-----+| TV|Radio|Paper|Sales|+-----+-----+-----+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5|+-----+-----+-----+-----+only showing top 4 rows 5.3.9. 丢弃列1drop_name = ['Newspaper','Sales'] Python 代码： 123dp.drop(drop_name,axis=1).head(4)#ds.drop(*drop_name).show(4) 比较： 123456789 +-----+-----+ | TV|Radio| TV Radio +-----+-----+0 230.1 37.8 |230.1| 37.8|1 44.5 39.3 | 44.5| 39.3|2 17.2 45.9 | 17.2| 45.9|3 151.5 41.3 |151.5| 41.3| +-----+-----+ only showing top 4 rows 5.3.10. 过滤12345dp = pd.read_csv('Advertising.csv')#ds = spark.read.csv(path='Advertising.csv', header=True, inferSchema=True) Python 代码： 123dp[dp.Newspaper&lt;20].head(4)#ds[ds.Newspaper&lt;20].show(4) 比较： 123456789 +-----+-----+---------+-----+ | TV|Radio|Newspaper|Sales| TV Radio Newspaper Sales +-----+-----+---------+-----+7 120.2 19.6 11.6 13.2 |120.2| 19.6| 11.6| 13.2|8 8.6 2.1 1.0 4.8 | 8.6| 2.1| 1.0| 4.8|11 214.7 24.0 4.0 17.4 |214.7| 24.0| 4.0| 17.4|13 97.5 7.6 7.2 9.7 | 97.5| 7.6| 7.2| 9.7| +-----+-----+---------+-----+ only showing top 4 rows Python 代码： 123dp[(dp.Newspaper&lt;20)&amp;(dp.TV&gt;100)].head(4)#ds[(ds.Newspaper&lt;20)&amp;(ds.TV&gt;100)].show(4) 比较： 123456789 +-----+-----+---------+-----+ | TV|Radio|Newspaper|Sales| TV Radio Newspaper Sales +-----+-----+---------+-----+7 120.2 19.6 11.6 13.2 |120.2| 19.6| 11.6| 13.2|11 214.7 24.0 4.0 17.4 |214.7| 24.0| 4.0| 17.4|19 147.3 23.9 19.1 14.6 |147.3| 23.9| 19.1| 14.6|25 262.9 3.5 19.5 12.0 |262.9| 3.5| 19.5| 12.0| +-----+-----+---------+-----+ only showing top 4 rows 5.3.11. 添加新列Python 代码： 1234dp['tv_norm'] = dp.TV/sum(dp.TV)dp.head(4)#ds.withColumn('tv_norm', ds.TV/ds.groupBy().agg(F.sum("TV")).collect()[0][0]).show(4) 比较： 123456789 +-----+-----+---------+-----+--------------------+ | TV|Radio|Newspaper|Sales| tv_norm| TV Radio Newspaper Sales tv_norm +-----+-----+---------+-----+--------------------+0 230.1 37.8 69.2 22.1 0.007824 |230.1| 37.8| 69.2| 22.1|0.007824268493802813|1 44.5 39.3 45.1 10.4 0.001513 | 44.5| 39.3| 45.1| 10.4|0.001513167961643...|2 17.2 45.9 69.3 9.3 0.000585 | 17.2| 45.9| 69.3| 9.3|5.848649200061207E-4|3 151.5 41.3 58.5 18.5 0.005152 |151.5| 41.3| 58.5| 18.5|0.005151571824472517| +-----+-----+---------+-----+--------------------+ only showing top 4 rows Python 代码： 12345dp['cond'] = dp.apply(lambda c: 1 if ((c.TV&gt;100)&amp;(c.Radio&lt;40)) else 2 if c.Sales&gt; 10 else 3,axis=1)#ds.withColumn('cond',F.when((ds.TV&gt;100)&amp;(ds.Radio&lt;40),1)\ .when(ds.Sales&gt;10, 2)\ .otherwise(3)).show(4) 比较： 123456789 +-----+-----+---------+-----+----+ | TV|Radio|Newspaper|Sales|cond| TV Radio Newspaper Sales cond +-----+-----+---------+-----+----+0 230.1 37.8 69.2 22.1 1 |230.1| 37.8| 69.2| 22.1| 1|1 44.5 39.3 45.1 10.4 2 | 44.5| 39.3| 45.1| 10.4| 2|2 17.2 45.9 69.3 9.3 3 | 17.2| 45.9| 69.3| 9.3| 3|3 151.5 41.3 58.5 18.5 2 |151.5| 41.3| 58.5| 18.5| 2| +-----+-----+---------+-----+----+ only showing top 4 rows Python 代码： 1234dp['log_tv'] = np.log(dp.TV)dp.head(4)#ds.withColumn('log_tv',F.log(ds.TV)).show(4) 比较： 123456789 +-----+-----+---------+-----+------------------+ | TV|Radio|Newspaper|Sales| log_tv| TV Radio Newspaper Sales log_tv +-----+-----+---------+-----+------------------+0 230.1 37.8 69.2 22.1 5.438514 |230.1| 37.8| 69.2| 22.1| 5.43851399704132|1 44.5 39.3 45.1 10.4 3.795489 | 44.5| 39.3| 45.1| 10.4|3.7954891891721947|2 17.2 45.9 69.3 9.3 2.844909 | 17.2| 45.9| 69.3| 9.3|2.8449093838194073|3 151.5 41.3 58.5 18.5 5.020586 |151.5| 41.3| 58.5| 18.5| 5.020585624949423| +-----+-----+---------+-----+------------------+ only showing top 4 rows Python 代码： 1234dp['tv+10'] = dp.TV.apply(lambda x: x+10)dp.head(4)#ds.withColumn('tv+10', ds.TV+10).show(4) 比较： 123456789 +-----+-----+---------+-----+-----+ | TV|Radio|Newspaper|Sales|tv+10| TV Radio Newspaper Sales tv+10 +-----+-----+---------+-----+-----+0 230.1 37.8 69.2 22.1 240.1 |230.1| 37.8| 69.2| 22.1|240.1|1 44.5 39.3 45.1 10.4 54.5 | 44.5| 39.3| 45.1| 10.4| 54.5|2 17.2 45.9 69.3 9.3 27.2 | 17.2| 45.9| 69.3| 9.3| 27.2|3 151.5 41.3 58.5 18.5 161.5 |151.5| 41.3| 58.5| 18.5|161.5| +-----+-----+---------+-----+-----+ only showing top 4 rows 5.3.12. 连接1234567891011121314leftp = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']&#125;, index=[0, 1, 2, 3])rightp = pd.DataFrame(&#123;'A': ['A0', 'A1', 'A6', 'A7'], 'F': ['B4', 'B5', 'B6', 'B7'], 'G': ['C4', 'C5', 'C6', 'C7'], 'H': ['D4', 'D5', 'D6', 'D7']&#125;, index=[4, 5, 6, 7])lefts = spark.createDataFrame(leftp)rights = spark.createDataFrame(rightp) 12345 A B C D A F G H0 A0 B0 C0 D0 4 A0 B4 C4 D41 A1 B1 C1 D1 5 A1 B5 C5 D52 A2 B2 C2 D2 6 A6 B6 C6 D63 A3 B3 C3 D3 7 A7 B7 C7 D7 左连接 Python 代码： 1234leftp.merge(rightp,on='A',how='left')#lefts.join(rights,on='A',how='left') .orderBy('A',ascending=True).show() 比较： 12345678 +---+---+---+---+----+----+----+ | A| B| C| D| F| G| H| A B C D F G H +---+---+---+---+----+----+----+0 A0 B0 C0 D0 B4 C4 D4 | A0| B0| C0| D0| B4| C4| D4|1 A1 B1 C1 D1 B5 C5 D5 | A1| B1| C1| D1| B5| C5| D5|2 A2 B2 C2 D2 NaN NaN NaN | A2| B2| C2| D2|null|null|null|3 A3 B3 C3 D3 NaN NaN NaN | A3| B3| C3| D3|null|null|null| +---+---+---+---+----+----+----+ 右连接 Python 代码： 1234leftp.merge(rightp,on='A',how='right')#lefts.join(rights,on='A',how='right') .orderBy('A',ascending=True).show() 比较： 12345678 +---+----+----+----+---+---+---+ | A| B| C| D| F| G| H| A B C D F G H +---+----+----+----+---+---+---+0 A0 B0 C0 D0 B4 C4 D4 | A0| B0| C0| D0| B4| C4| D4|1 A1 B1 C1 D1 B5 C5 D5 | A1| B1| C1| D1| B5| C5| D5|2 A6 NaN NaN NaN B6 C6 D6 | A6|null|null|null| B6| C6| D6|3 A7 NaN NaN NaN B7 C7 D7 | A7|null|null|null| B7| C7| D7| +---+----+----+----+---+---+---+ 内连接 Python 代码： 1234leftp.merge(rightp,on='A',how='inner')#lefts.join(rights,on='A',how='inner') .orderBy('A',ascending=True).show() 比较： 123456 +---+---+---+---+---+---+---+ | A| B| C| D| F| G| H| A B C D F G H +---+---+---+---+---+---+---+0 A0 B0 C0 D0 B4 C4 D4 | A0| B0| C0| D0| B4| C4| D4|1 A1 B1 C1 D1 B5 C5 D5 | A1| B1| C1| D1| B5| C5| D5| +---+---+---+---+---+---+---+ 全连接 Python 代码： 1234leftp.merge(rightp,on='A',how='full')#lefts.join(rights,on='A',how='full') .orderBy('A',ascending=True).show() 比较： 12345678910 +---+----+----+----+----+----+----+ | A| B| C| D| F| G| H| A B C D F G H +---+----+----+----+----+----+----+0 A0 B0 C0 D0 B4 C4 D4 | A0| B0| C0| D0| B4| C4| D4|1 A1 B1 C1 D1 B5 C5 D5 | A1| B1| C1| D1| B5| C5| D5|2 A2 B2 C2 D2 NaN NaN NaN | A2| B2| C2| D2|null|null|null|3 A3 B3 C3 D3 NaN NaN NaN | A3| B3| C3| D3|null|null|null|4 A6 NaN NaN NaN B6 C6 D6 | A6|null|null|null| B6| C6| D6|5 A7 NaN NaN NaN B7 C7 D7 | A7|null|null|null| B7| C7| D7| +---+----+----+----+----+----+----+ 5.3.13. 连接列12345678910my_list = [('a', 2, 3), ('b', 5, 6), ('c', 8, 9), ('a', 2, 3), ('b', 5, 6), ('c', 8, 9)]col_name = ['col1', 'col2', 'col3']#dp = pd.DataFrame(my_list,columns=col_name)ds = spark.createDataFrame(my_list,schema=col_name) 1234567 col1 col2 col30 a 2 31 b 5 62 c 8 93 a 2 34 b 5 65 c 8 9 Python 代码： 1234dp['concat'] = dp.apply(lambda x:'%s%s'%(x['col1'],x['col2']),axis=1)dp#ds.withColumn('concat',F.concat('col1','col2')).show() 比较： 12345678910 +----+----+----+------+ |col1|col2|col3|concat| col1 col2 col3 concat +----+----+----+------+0 a 2 3 a2 | a| 2| 3| a2|1 b 5 6 b5 | b| 5| 6| b5|2 c 8 9 c8 | c| 8| 9| c8|3 a 2 3 a2 | a| 2| 3| a2|4 b 5 6 b5 | b| 5| 6| b5|5 c 8 9 c8 | c| 8| 9| c8| +----+----+----+------+ 5.3.14. 分组Python 代码： 123dp.groupby(['col1']).agg(&#123;'col2':'min','col3':'mean'&#125;)#ds.groupBy(['col1']).agg(&#123;'col2': 'min', 'col3': 'avg'&#125;).show() 比较： 1234567 +----+---------+---------+ col2 col3 |col1|min(col2)|avg(col3)|col1 +----+---------+---------+a 2 3 | c| 8| 9.0|b 5 6 | b| 5| 6.0|c 8 9 | a| 2| 3.0| +----+---------+---------+ 5.3.15. 透视Python 代码： 123pd.pivot_table(dp, values='col3', index='col1', columns='col2', aggfunc=np.sum)#ds.groupBy(['col1']).pivot('col2').sum('col3').show() 比较： 1234567 +----+----+----+----+col2 2 5 8 |col1| 2| 5| 8|col1 +----+----+----+----+a 6.0 NaN NaN | c|null|null| 18|b NaN 12.0 NaN | b|null| 12|null|c NaN NaN 18.0 | a| 6|null|null| +----+----+----+----+ 5.3.16. 窗口123d = &#123;'A':['a','b','c','d'],'B':['m','m','n','n'],'C':[1,2,3,6]&#125;dp = pd.DataFrame(d)ds = spark.createDataFrame(dp) Python 代码： 12345dp['rank'] = dp.groupby('B')['C'].rank('dense',ascending=False)#from pyspark.sql.window import Windoww = Window.partitionBy('B').orderBy(ds.C.desc())ds = ds.withColumn('rank',F.rank().over(w)) 比较： 12345678 +---+---+---+----+ | A| B| C|rank| A B C rank +---+---+---+----+0 a m 1 2.0 | b| m| 2| 1|1 b m 2 1.0 | a| m| 1| 2|2 c n 3 2.0 | d| n| 6| 1|3 d n 6 1.0 | c| n| 3| 2| +---+---+---+----+ 5.3.17. rank VS dense_rank123456d =&#123;'Id':[1,2,3,4,5,6], 'Score': [4.00, 4.00, 3.85, 3.65, 3.65, 3.50]&#125;#data = pd.DataFrame(d)dp = data.copy()ds = spark.createDataFrame(data) 1234567 Id Score0 1 4.001 2 4.002 3 3.853 4 3.654 5 3.655 6 3.50 Python 代码： 12345678910dp['Rank_dense'] = dp['Score'].rank(method='dense',ascending =False)dp['Rank'] = dp['Score'].rank(method='min',ascending =False)dp#import pyspark.sql.functions as Ffrom pyspark.sql.window import Windoww = Window.orderBy(ds.Score.desc())ds = ds.withColumn('Rank_spark_dense',F.dense_rank().over(w))ds = ds.withColumn('Rank_spark',F.rank().over(w))ds.show() 比较： 12345678910 +---+-----+----------------+----------+ | Id|Score|Rank_spark_dense|Rank_spark| Id Score Rank_dense Rank +---+-----+----------------+----------+0 1 4.00 1.0 1.0 | 1| 4.0| 1| 1|1 2 4.00 1.0 1.0 | 2| 4.0| 1| 1|2 3 3.85 2.0 3.0 | 3| 3.85| 2| 3|3 4 3.65 3.0 4.0 | 4| 3.65| 3| 4|4 5 3.65 3.0 4.0 | 5| 3.65| 3| 4|5 6 3.50 4.0 6.0 | 6| 3.5| 4| 6| +---+-----+----------------+----------+ 6. 统计与线性代数预备知彼知己，百战不殆；不知彼而知己，一胜一负；不知彼，不知己，每战必殆。 – 《孙子兵法》 6.1. 表示法 m：样本数 n：特征数 ：第i个标签 ：第i个预测标签 ： 的均值 ：标签向量 ：预测标签向量 6.2. 线性代数预备由于我在我的数值分析考试笔记中记录了线性代数预备，有兴趣的读者可以参考 [Feng2014]了解更多细节。 线性代数预备 6.3. 测量公式6.3.1. 平均绝对误差在统计学中，MAE（平均绝对误差）衡量两个连续变量间的差异。 平均绝对误差由下式给出： 6.3.2. 均方误差在统计中，估计器（估计未观测量的过程）的 MSE（均方误差）测量了误差或偏差的平方的平均值 - 即估计器与被估计值之间的差异。 6.3.3. 均方根误差 6.3.4. 总体平方和在统计数据分析中，TSS（总体平方和）是一个数量，作为呈现此类分析结果的标准方式的一部分。 它被定义为在所有观察中，每个观测值与总体平均值的平方差的总和。 6.3.5. 解释平方和在统计学中，ESS（解释平方和），或者称为模型平方和或回归平方和。 ESS 是预测值和响应变量的均值的差的平方和，由下式给出： 6.3.6. 残差平方和在统计中，RSS/SSR（残差平方和），也称为预测误差平方和 预测（SSE），由下式给出： 6.3.7. 判定系数 注意 一般来说，()，总体平方和，等于解释平方和加上残差平方和，也就是： 更多细节可以在普通最小二乘模型中的分区中找到。 6.4. 混淆矩阵 混淆矩阵 6.4.1. 召回率 6.4.2. 精确率 6.4.3. 准确率 6.4.4. F1 得分 6.5. 统计检验6.5.1. 互相关检验 Pearson 互相关: 检验两个连续变量之间的相关度。 Spearman 互相关: 检验两个序数变量之间的相关度（不依赖于正态分布数据的假设）。 卡方: 检验两个类别变量之间的相关度。 6.5.2. 均值检验的比较 配对 T 检验: 检验两个相关变量之间的差异 独立 T 检验: 检验两个独立变量之间的差异 ANOVA: 在考虑结果变量中的任何其他变化之后，检验组均值之间的差异。 6.5.3. 非配对检验 Wilcoxon 秩和检验: 检验两个独立变量之间的差异 - 考虑差异的大小和方向。 Wilcoxon 符号秩检验: 检验两个相关变量之间的差异 - 考虑差异的大小和方向。 符号检验: 检验两个相关变量是否不同 - 忽略变化大小，仅考虑方向。 7. 数据探索千里之行，始于足下。 – 《老子》 我不是说，理解你的数据集是数据科学中最困难的事情，但它非常重要且耗时。 数据探索是通过统计和可视化技术来描述数据。 我们探索数据来了解特征并将其带到我们的模型。 7.1. 单变量分析在数学中，单变量是指仅含一个变量的表达式，方程式，函数或多项式。 “Uni”表示“一个”，换句话说，您的数据只有一个变量。 因此，您无需在此步骤中处理原因或关系。单变量分析获取数据，逐个汇总变量（属性）并发现数据中的模式。 单变量数据中发现的模式可以通过多种方式描述，包括集中趋势（均值，众数和中值）和离散度：极差，方差，最大值，最小值，四分位数（包括四分位数极差），方差和标准差。 您还可以使用多个选项来视化和描述单变量数据。 如频率分布表，条形图，直方图，频率多边形，扇形图。 变量可以是分类变量或数值变量，我将演示不同的统计和可视化技术来研究变量的每种类型。 Jupyter 笔记本可以从数据探索下载。 数据可以从 German Credit 下载。 7.1.1. 数值变量描述 pandas和spark中的describe函数将给出大部分统计结果，例如最小值，中值，最大值，四分位数和标准差。 借助用户定义的函数，您可以获得更多的统计结果。 123# 为选择要展示的变量num_cols = ['Account Balance','No of dependents']df.select(num_cols).describe().show() 123456789+-------+------------------+-------------------+|summary| Account Balance| No of dependents|+-------+------------------+-------------------+| count| 1000| 1000|| mean| 2.577| 1.155|| stddev|1.2576377271108936|0.36208577175319395|| min| 1| 1|| max| 4| 2|+-------+------------------+-------------------+ 您可能会发现 PySpark 中的默认函数不包含四分位数。 以下函数将帮助您在 Pandas 中获得相同的结果： 1234567891011121314151617181920212223242526def describe_pd(df_in, columns, deciles=False): ''' Function to union the basic stats results and deciles :param df_in: the input dataframe :param columns: the cloumn name list of the numerical variable :param deciles: the deciles output :return : the numerical describe info. of the input dataframe :author: Ming Chen and Wenqiang Feng :email: von198@gmail.com ''' if deciles: percentiles = np.array(range(0, 110, 10)) else: percentiles = [25, 50, 75] percs = np.transpose([np.percentile(df_in.select(x).collect(), percentiles) for x in columns]) percs = pd.DataFrame(percs, columns=columns) percs['summary'] = [str(p) + '%' for p in percentiles] spark_describe = df_in.describe().toPandas() new_df = pd.concat([spark_describe, percs],ignore_index=True) new_df = new_df.round(2) return new_df[['summary'] + columns] 1describe_pd(df,num_cols) 123456789101112+-------+------------------+-----------------+|summary| Account Balance| No of dependents|+-------+------------------+-----------------+| count| 1000.0| 1000.0|| mean| 2.577| 1.155|| stddev|1.2576377271108936|0.362085771753194|| min| 1.0| 1.0|| max| 4.0| 2.0|| 25%| 1.0| 1.0|| 50%| 2.0| 1.0|| 75%| 4.0| 1.0|+-------+------------------+-----------------+ 有时，由于机密数据问题，您无法提供真实数据，您的客户可能会请求更多统计结果，例如“十分位数”。 您可以应用以下函数来实现它。 1describe_pd(df,num_cols,deciles=True) 1234567891011121314151617181920+-------+------------------+-----------------+|summary| Account Balance| No of dependents|+-------+------------------+-----------------+| count| 1000.0| 1000.0|| mean| 2.577| 1.155|| stddev|1.2576377271108936|0.362085771753194|| min| 1.0| 1.0|| max| 4.0| 2.0|| 0%| 1.0| 1.0|| 10%| 1.0| 1.0|| 20%| 1.0| 1.0|| 30%| 2.0| 1.0|| 40%| 2.0| 1.0|| 50%| 2.0| 1.0|| 60%| 3.0| 1.0|| 70%| 4.0| 1.0|| 80%| 4.0| 1.0|| 90%| 4.0| 2.0|| 100%| 4.0| 2.0|+-------+------------------+-----------------+ 偏度和峰度 这个小节来自维基百科偏度。 在概率论和统计学中，偏度是实值随机变量概率分布关于其均值的不对称性的度量。 偏度值可以是正数或负数，或者是未定义的。对于单峰分布，负偏度通常表示尾部位于分布的左侧，而正偏度表示尾部位于右侧。 考虑下图中的两个分布。 在每个图中，分布右侧的值与左侧的值不同。 这些逐渐变细的一端称为尾部，它们提供了一种可视方法来确定分布中的两种偏斜中的哪一种： 负偏度：左尾较长；分布的质量集中在图的右侧。尽管曲线本身看起来是向右倾斜的，但这种分布成为左倾的。左是指尾部向左侧延伸，并且通常，平均值偏向数据的典型中心的左侧。 左倾分布通常表现为右倾曲线。 正偏度：右尾更长; 分布的质量集中在图的左侧。尽管曲线本身看起来是向左倾斜的，但这种分布成为右倾的。右边是指尾部向右侧延伸，通常，平均值偏向于典型数据中心的右侧。 右倾分布通常表现为左倾曲线。 这一小节来自维基百科峰度。 在概率论和统计学中，峰度（kyrtos 或 kurtos，意思是“弯曲的，拱形的”）是实值随机变量的概率分布的“尾部”的度量。 与偏度概念类似，峰度描述概率分布形状，正如偏度一样，有不同的方法来量化它的理论分布，和相应的方法来估计它来自一个样本总体。 12from pyspark.sql.functions import col, skewness, kurtosisdf.select(skewness(var),kurtosis(var)).show() 12345+---------------------+---------------------+|skewness(Age (years))|kurtosis(Age (years))|+---------------------+---------------------+| 1.0231743160548064| 0.6114371688367672|+---------------------+---------------------+ 警告 有时统计量可能产生误导！ F. J. Anscombe 曾经说到执行计算和制作图表。 应研究两种结果；每个都有助于理解。 图相同统计量的不同图表（Datasaurus，和 12 个其他东西）中的这 13 个数据集各自具有相同的汇总统计量（x/y均值，x/y标准差和 Pearson 相关性），虽然外观完全不同。 这项工作描述了我们开发的技术，用于创建此数据集，以及其他类似的数据集。 更多细节和有趣的结果可以在相同统计量和不同图表中找到。 相同统计量和不同图表 直方图 警告 直方图经常和条形图混淆！ 直方图和条形图之间的根本区别，将帮助您轻松识别两者，条形图中的条形之间存在间隙，但在直方图中，条形彼此相邻。 感兴趣的读者可以参考直方图和条形图之间的差异。 123456789101112131415var = 'Age (years)'x = data1[var]bins = np.arange(0, 100, 5.0)plt.figure(figsize=(10,8))# 数据直方图plt.hist(x, bins, alpha=0.8, histtype='bar', color='gold', ec='black',weights=np.zeros_like(x) + 100\. / x.size)plt.xlabel(var)plt.ylabel('percentage')plt.xticks(bins)plt.show()fig.savefig(var+".pdf", bbox_inches='tight') 123456789101112131415161718192021222324252627282930313233343536373839404142434445var = 'Age (years)'x = data1[var]bins = np.arange(0, 100, 5.0)########################################################################hist, bin_edges = np.histogram(x,bins, weights=np.zeros_like(x) + 100\. / x.size)# 生成直方图fig = plt.figure(figsize=(20, 8))ax = fig.add_subplot(1, 2, 1)# 绘制高度与 x 轴上的整数的直方图ax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black', color='gold')# 将刻度设在条形中间ax.set_xticks([0.5+i for i,j in enumerate(hist)])# 将 xticklabels 设置为一个字符串，告诉我们桶的边缘是什么labels =['&#123;&#125;'.format(int(bins[i+1])) for i,j in enumerate(hist)]labels.insert(0,'0')ax.set_xticklabels(labels)plt.xlabel(var)plt.ylabel('percentage')########################################################################hist, bin_edges = np.histogram(x,bins) # 生成直方图ax = fig.add_subplot(1, 2, 2)# 绘制高度与 x 轴上的整数的直方图ax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black', color='gold')# 将刻度设在条形中间ax.set_xticks([0.5+i for i,j in enumerate(hist)])# 将 xticklabels 设置为一个字符串，告诉我们桶的边缘是什么labels =['&#123;&#125;'.format(int(bins[i+1])) for i,j in enumerate(hist)]labels.insert(0,'0')ax.set_xticklabels(labels)plt.xlabel(var)plt.ylabel('count')plt.suptitle('Histogram of &#123;&#125;: Left with percentage output;Right with count output' .format(var), size=16)plt.show()fig.savefig(var+".pdf", bbox_inches='tight') 有时，有些人会要求您绘制不等宽度的条形（直方图的无效参数）。 你仍然可以通过以下方法实现它。 12345678910111213141516171819202122232425var = 'Credit Amount'plot_data = df.select(var).toPandas()x= plot_data[var]bins =[0,200,400,600,700,800,900,1000,2000,3000,4000,5000,6000,10000,25000]hist, bin_edges = np.histogram(x,bins,weights=np.zeros_like(x) + 100\. / x.size) # make the histogramfig = plt.figure(figsize=(10, 8))ax = fig.add_subplot(1, 1, 1)# 绘制高度与 x 轴上的整数的直方图ax.bar(range(len(hist)),hist,width=1,alpha=0.8,ec ='black',color = 'gold')# 将刻度设在条形中间ax.set_xticks([0.5+i for i,j in enumerate(hist)])# 将 xticklabels 设置为一个字符串，告诉我们桶的边缘是什么#labels =['&#123;&#125;k'.format(int(bins[i+1]/1000)) for i,j in enumerate(hist)]labels =['&#123;&#125;'.format(bins[i+1]) for i,j in enumerate(hist)]labels.insert(0,'0')ax.set_xticklabels(labels)#plt.text(-0.6, -1.4,'0')plt.xlabel(var)plt.ylabel('percentage')plt.show() 箱形图和提琴图 请注意，虽然提琴图与 Tukey（1977）的箱形图密切相关，但提琴图可以显示比箱形图更多的信息。 当我们进行探索性分析时，没有样本的知识。 因此，样本分布不能假设为正态分布，并且通常当您获得大数据时，正态分布将在箱形图中显示一些溢出。 然而，对于较小的样本大小，提琴图可能会产生误导，其中即使在为标准正常数据生成时，密度图也可能显示出有趣的特征（以及其中的分组差异）。 一些作者建议样本量应大于 250（例如，n&gt; 250或理想情况甚至更大）。其中核密度图提供了分布的合理准确表示，可能表现诸如双峰性或其他形式的细微差别，它在箱形图中是不可见的或不太清楚。 更多细节可以在[箱形图和小提琴图的简单比较]中找到(https://figshare.com/articles/A_simple_comparison_of_box_plots_and_violin_plots/1544525)。 12345678x = df.select(var).toPandas()fig = plt.figure(figsize=(20, 8))ax = fig.add_subplot(1, 2, 1)ax = sns.boxplot(data=x)ax = fig.add_subplot(1, 2, 2)ax = sns.violinplot(data=x) 7.1.2. 类别变量与数值变量相比，分类变量更容易进行探索。 频率表 123456789101112131415from pyspark.sql import functions as Ffrom pyspark.sql.functions import rank,sum,colfrom pyspark.sql import Windowwindow = Window.rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)# withColumn('Percent %',F.format_string("%5.0f%%\n",col('Credit_num')*100/col('total'))).\tab = df.select(['age_class','Credit Amount']).\ groupBy('age_class').\ agg(F.count('Credit Amount').alias('Credit_num'), F.mean('Credit Amount').alias('Credit_avg'), F.min('Credit Amount').alias('Credit_min'), F.max('Credit Amount').alias('Credit_max')).\ withColumn('total',sum(col('Credit_num')).over(window)).\ withColumn('Percent',col('Credit_num')*100/col('total')).\ drop(col('total')) 12345678910+---------+----------+------------------+----------+----------+-------+|age_class|Credit_num| Credit_avg|Credit_min|Credit_max|Percent|+---------+----------+------------------+----------+----------+-------+| 45-54| 120|3183.0666666666666| 338| 12612| 12.0|| &lt;25| 150| 2970.733333333333| 276| 15672| 15.0|| 55-64| 56| 3493.660714285714| 385| 15945| 5.6|| 35-44| 254| 3403.771653543307| 250| 15857| 25.4|| 25-34| 397| 3298.823677581864| 343| 18424| 39.7|| 65+| 23|3210.1739130434785| 571| 14896| 2.3|+---------+----------+------------------+----------+----------+-------+ 扇形图 12345678910111213# 要绘制的数据labels = plot_data.age_classsizes = plot_data.Percentcolors = ['gold', 'yellowgreen', 'lightcoral','blue', 'lightskyblue','green','red']explode = (0, 0.1, 0, 0,0,0) # explode 1st slice# 绘制plt.figure(figsize=(10,8))plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)plt.axis('equal')plt.show() 条形图 1234567891011labels = plot_data.age_classmissing = plot_data.Percentind = [x for x, _ in enumerate(labels)]plt.figure(figsize=(10,8))plt.bar(ind, missing, width=0.8, label='missing', color='gold')plt.xticks(ind, labels)plt.ylabel("percentage")plt.show() 1234567891011121314151617labels = ['missing', '&lt;25', '25-34', '35-44', '45-54','55-64','65+']missing = np.array([0.000095, 0.024830, 0.028665, 0.029477, 0.031918,0.037073,0.026699])man = np.array([0.000147, 0.036311, 0.038684, 0.044761, 0.051269, 0.059542, 0.054259])women = np.array([0.004035, 0.032935, 0.035351, 0.041778, 0.048437, 0.056236,0.048091])ind = [x for x, _ in enumerate(labels)]plt.figure(figsize=(10,8))plt.bar(ind, women, width=0.8, label='women', color='gold', bottom=man+missing)plt.bar(ind, man, width=0.8, label='man', color='silver', bottom=missing)plt.bar(ind, missing, width=0.8, label='missing', color='#CD853F')plt.xticks(ind, labels)plt.ylabel("percentage")plt.legend(loc="upper left")plt.title("demo")plt.show() 7.2. 多变量分析在本节中，我将仅演示双变量分析。 由于多变量分析由双变量派生。 7.2.1. 数值 VS 数值相关矩阵 123456789101112from pyspark.mllib.stat import Statisticsimport pandas as pdcorr_data = df.select(num_cols)col_names = corr_data.columnsfeatures = corr_data.rdd.map(lambda row: row[0:])corr_mat=Statistics.corr(features, method="pearson")corr_df = pd.DataFrame(corr_mat)corr_df.index, corr_df.columns = col_names, col_namesprint(corr_df.to_string()) 123456+--------------------+--------------------+| Account Balance| No of dependents|+--------------------+--------------------+| 1.0|-0.01414542650320914||-0.01414542650320914| 1.0|+--------------------+--------------------+ 散点图 123456import seaborn as snssns.set(style="ticks")df = sns.load_dataset("iris")sns.pairplot(df, hue="species")plt.show() 7.2.2. 类别 VS 类别卡方检验 警告 pyspark.ml.stat 只在 Spark 2.4.0 中可用。 123456789101112131415from pyspark.ml.linalg import Vectorsfrom pyspark.ml.stat import ChiSquareTestdata = [(0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0))]df = spark.createDataFrame(data, ["label", "features"])r = ChiSquareTest.test(df, "features", "label").head()print("pValues: " + str(r.pValues))print("degreesOfFreedom: " + str(r.degreesOfFreedom))print("statistics: " + str(r.statistics)) 123pValues: [0.687289278791,0.682270330336]degreesOfFreedom: [2, 3]statistics: [0.75,1.5] 交叉表 1df.stat.crosstab("age_class", "Occupation").show() 12345678910+--------------------+---+---+---+---+|age_class_Occupation| 1| 2| 3| 4|+--------------------+---+---+---+---+| &lt;25| 4| 34|108| 4|| 55-64| 1| 15| 31| 9|| 25-34| 7| 61|269| 60|| 35-44| 4| 58|143| 49|| 65+| 5| 3| 6| 9|| 45-54| 1| 29| 73| 17|+--------------------+---+---+---+---+ 堆栈图 1234567891011121314151617labels = ['missing', '&lt;25', '25-34', '35-44', '45-54','55-64','65+']missing = np.array([0.000095, 0.024830, 0.028665, 0.029477, 0.031918,0.037073,0.026699])man = np.array([0.000147, 0.036311, 0.038684, 0.044761, 0.051269, 0.059542, 0.054259])women = np.array([0.004035, 0.032935, 0.035351, 0.041778, 0.048437, 0.056236,0.048091])ind = [x for x, _ in enumerate(labels)]plt.figure(figsize=(10,8))plt.bar(ind, women, width=0.8, label='women', color='gold', bottom=man+missing)plt.bar(ind, man, width=0.8, label='man', color='silver', bottom=missing)plt.bar(ind, missing, width=0.8, label='missing', color='#CD853F')plt.xticks(ind, labels)plt.ylabel("percentage")plt.legend(loc="upper left")plt.title("demo")plt.show() 8. 回归 千里之行，始于足下。 – 《老子》 在统计建模中，回归分析侧重于研究因变量与一个或多个自变量之间的关系。维基百科的回归分析。 在数据挖掘中，回归是一种模型，用于表示标签（或目标，它是数值变量）的值与一个或多个特征（或预测变量，它们可以是数值和分类变量）之间的关系。 8.1. 线性回归8.1.1. 简介给定数据集 ，它包含n个特征（变量）和m个样本（数据点），在简单线性回归模型中，使用j个自变量建模m个数据点 ：，公式由下式给出： 在矩阵表示法中，数据集写为 ，其中 ，，并且 。之后矩阵形式的方程写为： (1) 特征矩阵和标签 8.1.2. 如何求解 直接法 （更多信息请参考我的数值分析预备笔记）。 对于方阵或长方阵 奇异值分解 格兰施密特正交化 QR 分解 对于方阵 LU 分解 Cholesky 分解 正则分割 迭代方法 静态案例迭代法 Jacobi 方法 Gauss-Seidel 方法 Richardson 方法 连续过度放松 (SOR) 方法 动态案例迭代法 Chebyshev 迭代法 最小残差法 最小修正迭代法 最速下降法 共轭梯度法 8.1.3. 普通最小二乘在数学中，（1）是一个超定系统。 普通最小二乘法可用于找到超定系统的近似解。 对于系统超定系统（1），从问题中获得最小二乘公式 (2) 其解决方案可以用正规方程式编写： (3) 其中 表示矩阵转置，假设 存在（也就是假设 是列满秩的）。 注意 实际上，(3) 可以用下面的方式导出：将 和 (1) 相乘，之后在之前结果的两边乘上 。你也可以对 (2) 应用极值定理，并寻找 (3) 的解。 8.1.4. 梯度下降让我们使用下列假设： 之后求解 (2) 等价于最小化下面的损失函数： 8.1.5. 损失函数(4) 注意 我们倾向求解 (4) 而不是 (2) 的原因是，(4) 是凸的，并且属性良好，例如它是个唯一可解，对于足够小的学习率是能量稳定的。如果读者对非凸损失函数（能量）案例感兴趣，可以参考 [Feng2016PSD]。 一维中的梯度下降 二维中的梯度下降 8.1.6. 批量梯度下降梯度下降是用于找到函数最小值的一阶迭代优化算法。 它沿着最陡峭的方向搜索，该方向由“梯度的相反数”（参见图 1D 中的梯度下降和 2D 中的梯度下降）和学习率（搜索步长） 定义。 8.1.7. 随机梯度下降8.1.8. 小批量梯度下降8.1.9. 示例 Jupyter 笔记本可以从线性回归 下载，它不使用流水线实现。 upyter 笔记本可以从带流水线的线性回归，它使用流水线实现。 我下面仅仅展示流水线风格的代码。 参数的更多信息请见线性回归 API。 建立 spark 上下文和 SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark regression example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() 加载数据集 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;).\ load(&quot;../data/Advertising.csv&quot;,header=True); 检查数据集 12df.show(5,True)df.printSchema() 之后我们会得到 12345678910111213141516+-----+-----+---------+-----+| TV|Radio|Newspaper|Sales|+-----+-----+---------+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5||180.8| 10.8| 58.4| 12.9|+-----+-----+---------+-----+only showing top 5 rowsroot |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 您还可以从数据帧中获取统计结果（不幸的是，它仅适用于数字）。 1df.describe().show() 之后你会得到 123456789+-------+-----------------+------------------+------------------+------------------+|summary| TV| Radio| Newspaper| Sales|+-------+-----------------+------------------+------------------+------------------+| count| 200| 200| 200| 200|| mean| 147.0425|23.264000000000024|30.553999999999995|14.022500000000003|| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|| min| 0.7| 0.0| 0.3| 1.6|| max| 296.4| 49.6| 114.0| 27.0|+-------+-----------------+------------------+------------------+------------------+ 销售分布 将数据转换为密集向量（特征和标签） 123456789101112131415from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectors# I provide two ways to build the features and labels# method 1 (good for small feature):#def transData(row):# return Row(label=row[&quot;Sales&quot;],# features=Vectors.dense([row[&quot;TV&quot;],# row[&quot;Radio&quot;],# row[&quot;Newspaper&quot;]]))# Method 2 (good for large features):def transData(data):return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 注意 强烈建议您尝试使用我的get_dummy函数来处理数据集中的分类数据。 监督学习版本： 12345678910111213141516171819202122232425def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(indexCol,&apos;features&apos;,&apos;label&apos;) 无监督学习版本： 1234567891011121314151617181920212223242526272829def get_dummy(df,indexCol,categoricalCols,continuousCols): &apos;&apos;&apos; Get dummy variables and concat with continuous variables for unsupervised learning. :param df: the dataframe :param categoricalCols: the name list of the categorical data :param continuousCols: the name list of the numerical data :return k: feature matrix :author: Wenqiang Feng :email: von198@gmail.com &apos;&apos;&apos; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) return data.select(indexCol,&apos;features&apos;) 将数据集转换为数据帧 12transformed= transData(df)transformed.show(5) 12345678910+-----------------+-----+| features|label|+-----------------+-----+|[230.1,37.8,69.2]| 22.1|| [44.5,39.3,45.1]| 10.4|| [17.2,45.9,69.3]| 9.3||[151.5,41.3,58.5]| 18.5||[180.8,10.8,58.4]| 12.9|+-----------------+-----+only showing top 5 rows 注意 您会发现 Spark 中所有监督机器学习算法都基于特征和标签（Spark 中的无监督机器学习算法基于特征）。 也就是说，当您在管道架构中准备好特征和标签时，您可以使用 Spark 中的所有机器学习算法。 处理类别变量 12345678910111213from pyspark.ml import Pipelinefrom pyspark.ml.regression import LinearRegressionfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluator# Automatically identify categorical features, and index them.# We specify maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer = VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;,\ maxCategories=4).fit(transformed)data = featureIndexer.transform(transformed) 现在你可以这样检查数据集 1data.show(5,True) 你会得到 12345678910+-----------------+-----+-----------------+| features|label| indexedFeatures|+-----------------+-----+-----------------+|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|| [17.2,45.9,69.3]| 9.3| [17.2,45.9,69.3]||[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]||[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|+-----------------+-----+-----------------+only showing top 5 rows 将数据分割为训练和测试集（留出 40% 用于测试） 12# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = transformed.randomSplit([0.6, 0.4]) 您可以按照以下方式检查您的训练和测试数据（在我看来，在原型阶段始终跟踪您的数据总是很好）： 12trainingData.show(5)testData.show(5) 之后你会得到 123456789101112131415161718192021+---------------+-----+---------------+| features|label|indexedFeatures|+---------------+-----+---------------+| [4.1,11.6,5.7]| 3.2| [4.1,11.6,5.7]|| [5.4,29.9,9.4]| 5.3| [5.4,29.9,9.4]||[7.3,28.1,41.4]| 5.5|[7.3,28.1,41.4]||[7.8,38.9,50.6]| 6.6|[7.8,38.9,50.6]|| [8.6,2.1,1.0]| 4.8| [8.6,2.1,1.0]|+---------------+-----+---------------+only showing top 5 rows+----------------+-----+----------------+| features|label| indexedFeatures|+----------------+-----+----------------+| [0.7,39.6,8.7]| 1.6| [0.7,39.6,8.7]|| [8.4,27.2,2.1]| 5.7| [8.4,27.2,2.1]||[11.7,36.9,45.2]| 7.3|[11.7,36.9,45.2]||[13.2,15.9,49.6]| 5.6|[13.2,15.9,49.6]||[16.9,43.7,89.4]| 8.7|[16.9,43.7,89.4]|+----------------+-----+----------------+only showing top 5 rows 拟合普通最小二乘回归模型 参数的更多信息请见线性回归 API。 12345# Import LinearRegression classfrom pyspark.ml.regression import LinearRegression# Define LinearRegression algorithmlr = LinearRegression() 流水线架构 1234# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, lr])model = pipeline.fit(trainingData) 模型总结 Spark 不擅长数据和模型的汇总。 我为 PySpark 中线性回归写了一个汇总函数，其格式与 R 的输出类似。 1234567891011121314151617181920def modelsummary(model): import numpy as np print (&quot;Note: the last rows are the information for Intercept&quot;) print (&quot;##&quot;,&quot;-------------------------------------------------&quot;) print (&quot;##&quot;,&quot; Estimate | Std.Error | t Values | P-value&quot;) coef = np.append(list(model.coefficients),model.intercept) Summary=model.summary for i in range(len(Summary.pValues)): print (&quot;##&quot;,&apos;&#123;:10.6f&#125;&apos;.format(coef[i]),\ &apos;&#123;:10.6f&#125;&apos;.format(Summary.coefficientStandardErrors[i]),\ &apos;&#123;:8.3f&#125;&apos;.format(Summary.tValues[i]),\ &apos;&#123;:10.6f&#125;&apos;.format(Summary.pValues[i])) print (&quot;##&quot;,&apos;---&apos;) print (&quot;##&quot;,&quot;Mean squared error: % .6f&quot; \ % Summary.meanSquaredError, &quot;, RMSE: % .6f&quot; \ % Summary.rootMeanSquaredError ) print (&quot;##&quot;,&quot;Multiple R-squared: %f&quot; % Summary.r2, &quot;, \ Total iterations: %i&quot;% Summary.totalIterations) 1modelsummary(model.stages[-1]) 你会得到以下结果： 12345678910Note: the last rows are the information for Intercept(&apos;##&apos;, &apos;-------------------------------------------------&apos;)(&apos;##&apos;, &apos; Estimate | Std.Error | t Values | P-value&apos;)(&apos;##&apos;, &apos; 0.044186&apos;, &apos; 0.001663&apos;, &apos; 26.573&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos; 0.206311&apos;, &apos; 0.010846&apos;, &apos; 19.022&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos; 0.001963&apos;, &apos; 0.007467&apos;, &apos; 0.263&apos;, &apos; 0.793113&apos;)(&apos;##&apos;, &apos; 2.596154&apos;, &apos; 0.379550&apos;, &apos; 6.840&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos;---&apos;)(&apos;##&apos;, &apos;Mean squared error: 2.588230&apos;, &apos;, RMSE: 1.608798&apos;)(&apos;##&apos;, &apos;Multiple R-squared: 0.911869&apos;, &apos;, Total iterations: 1&apos;) 做出预测 12# Make predictions.predictions = model.transform(testData) 12# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+----------------+-----+------------------+| features|label| prediction|+----------------+-----+------------------+| [0.7,39.6,8.7]| 1.6| 10.81405928637388|| [8.4,27.2,2.1]| 5.7| 8.583086404079918||[11.7,36.9,45.2]| 7.3|10.814712818232422||[13.2,15.9,49.6]| 5.6| 6.557106943899219||[16.9,43.7,89.4]| 8.7|12.534151375058645|+----------------+-----+------------------+only showing top 5 rows 评估 12345678from pyspark.ml.evaluation import RegressionEvaluator# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator(labelCol=&quot;label&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;)rmse = evaluator.evaluate(predictions)print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) 最终的均方根误差（RMSE）如下： 1Root Mean Squared Error (RMSE) on test data = 1.63114 您还可以检查测试数据的 ![R^2](img/1ac835166928f502b55a31636602602a.jpg） 值： 123456y_true = predictions.select(&quot;label&quot;).toPandas()y_pred = predictions.select(&quot;prediction&quot;).toPandas()import sklearn.metricsr2_score = sklearn.metrics.r2_score(y_true, y_pred)print(&apos;r2_score: &#123;0&#125;&apos;.format(r2_score)) 之后你会得到 1r2_score: 0.854486655585 警告 你应该知道，模型不包含截距时，大多数软件使用不同的公式来计算 值。你可以从 StackExchange 上的讨论获得更多信息。 8.2. 广义线性回归8.2.1. 介绍8.2.2. 如何求解8.2.3. 示例 Jupyter 笔记本可以从广义线性回归下载。 参数的更多信息请见广义线性回归 API。 建立 spark 上下文和 SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark regression example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() 加载数据集 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;).\ load(&quot;../data/Advertising.csv&quot;,header=True); 查看数据集 12df.show(5,True)df.printSchema() 之后你会得到 12345678910111213141516+-----+-----+---------+-----+| TV|Radio|Newspaper|Sales|+-----+-----+---------+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5||180.8| 10.8| 58.4| 12.9|+-----+-----+---------+-----+only showing top 5 rowsroot |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 您还可以从数据帧中获取统计结果（不幸的是，它仅适用于数字）。 1df.describe().show() 之后你会得到 123456789+-------+-----------------+------------------+------------------+------------------+|summary| TV| Radio| Newspaper| Sales|+-------+-----------------+------------------+------------------+------------------+| count| 200| 200| 200| 200|| mean| 147.0425|23.264000000000024|30.553999999999995|14.022500000000003|| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|| min| 0.7| 0.0| 0.3| 1.6|| max| 296.4| 49.6| 114.0| 27.0|+-------+-----------------+------------------+------------------+------------------+ 将数据转换为密集向量（特征和标签） 注意 强烈建议您尝试使用我的get_dummy函数来处理数据集中的分类数据。 监督学习版本： 12345678910111213141516171819202122232425def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(indexCol,&apos;features&apos;,&apos;label&apos;) 无监督学习版本： 1234567891011121314151617181920212223242526272829def get_dummy(df,indexCol,categoricalCols,continuousCols): &apos;&apos;&apos; Get dummy variables and concat with continuous variables for unsupervised learning. :param df: the dataframe :param categoricalCols: the name list of the categorical data :param continuousCols: the name list of the numerical data :return k: feature matrix :author: Wenqiang Feng :email: von198@gmail.com &apos;&apos;&apos; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) return data.select(indexCol,&apos;features&apos;) 123456789101112131415from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectors# I provide two ways to build the features and labels# method 1 (good for small feature):#def transData(row):# return Row(label=row[&quot;Sales&quot;],# features=Vectors.dense([row[&quot;TV&quot;],# row[&quot;Radio&quot;],# row[&quot;Newspaper&quot;]]))# Method 2 (good for large features):def transData(data):return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 12transformed= transData(df)transformed.show(5) 12345678910+-----------------+-----+| features|label|+-----------------+-----+|[230.1,37.8,69.2]| 22.1|| [44.5,39.3,45.1]| 10.4|| [17.2,45.9,69.3]| 9.3||[151.5,41.3,58.5]| 18.5||[180.8,10.8,58.4]| 12.9|+-----------------+-----+only showing top 5 rows 注意 您会发现 Spark 中所有监督机器学习算法都基于特征和标签（Spark 中的无监督机器学习算法基于特征）。 也就是说，当您在管道架构中准备好特征和标签时，您可以使用 Spark 中的所有机器学习算法。 将数据转换为密集向量 12345678910# convert the data to dense vectordef transData(data): return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\ toDF([&apos;label&apos;,&apos;features&apos;])from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectorsdata= transData(df)data.show() 处理类别变量 1234567891011121314from pyspark.ml import Pipelinefrom pyspark.ml.regression import LinearRegressionfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluator# Automatically identify categorical features, and index them.# We specify maxCategories so features with &gt; 4# distinct values are treated as continuous.featureIndexer = VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;,\ maxCategories=4).fit(transformed)data = featureIndexer.transform(transformed) When you check you data at this point, you will get 12345678910+-----------------+-----+-----------------+| features|label| indexedFeatures|+-----------------+-----+-----------------+|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|| [17.2,45.9,69.3]| 9.3| [17.2,45.9,69.3]||[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]||[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|+-----------------+-----+-----------------+only showing top 5 rows 将数据分割为训练和测试集（留出 40% 用于测试） 12# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = transformed.randomSplit([0.6, 0.4]) 您可以按照以下方式检查您的训练和测试数据（在我看来，在原型阶段始终跟踪您的数据总是很好）： 12trainingData.show(5)testData.show(5) 之后你会得到 123456789101112131415161718192021+----------------+-----+----------------+| features|label| indexedFeatures|+----------------+-----+----------------+| [5.4,29.9,9.4]| 5.3| [5.4,29.9,9.4]|| [7.8,38.9,50.6]| 6.6| [7.8,38.9,50.6]|| [8.4,27.2,2.1]| 5.7| [8.4,27.2,2.1]|| [8.7,48.9,75.0]| 7.2| [8.7,48.9,75.0]||[11.7,36.9,45.2]| 7.3|[11.7,36.9,45.2]|+----------------+-----+----------------+only showing top 5 rows+---------------+-----+---------------+| features|label|indexedFeatures|+---------------+-----+---------------+| [0.7,39.6,8.7]| 1.6| [0.7,39.6,8.7]|| [4.1,11.6,5.7]| 3.2| [4.1,11.6,5.7]||[7.3,28.1,41.4]| 5.5|[7.3,28.1,41.4]|| [8.6,2.1,1.0]| 4.8| [8.6,2.1,1.0]||[17.2,4.1,31.6]| 5.9|[17.2,4.1,31.6]|+---------------+-----+---------------+only showing top 5 rows 拟合广义线性回归模型 123456# Import LinearRegression classfrom pyspark.ml.regression import GeneralizedLinearRegression# Define LinearRegression algorithmglr = GeneralizedLinearRegression(family=&quot;gaussian&quot;, link=&quot;identity&quot;,\ maxIter=10, regParam=0.3) 流水线架构 1234# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, glr])model = pipeline.fit(trainingData) 模型总结 Spark 不擅长数据和模型的汇总。 我为 PySpark 中线性回归写了一个汇总函数，其格式与 R 输出类似。 1234567891011121314151617181920def modelsummary(model): import numpy as np print (&quot;Note: the last rows are the information for Intercept&quot;) print (&quot;##&quot;,&quot;-------------------------------------------------&quot;) print (&quot;##&quot;,&quot; Estimate | Std.Error | t Values | P-value&quot;) coef = np.append(list(model.coefficients),model.intercept) Summary=model.summary for i in range(len(Summary.pValues)): print (&quot;##&quot;,&apos;&#123;:10.6f&#125;&apos;.format(coef[i]),\ &apos;&#123;:10.6f&#125;&apos;.format(Summary.coefficientStandardErrors[i]),\ &apos;&#123;:8.3f&#125;&apos;.format(Summary.tValues[i]),\ &apos;&#123;:10.6f&#125;&apos;.format(Summary.pValues[i])) print (&quot;##&quot;,&apos;---&apos;)# print (&quot;##&quot;,&quot;Mean squared error: % .6f&quot; \# % Summary.meanSquaredError, &quot;, RMSE: % .6f&quot; \# % Summary.rootMeanSquaredError )# print (&quot;##&quot;,&quot;Multiple R-squared: %f&quot; % Summary.r2, &quot;, \# Total iterations: %i&quot;% Summary.totalIterations) 1modelsummary(model.stages[-1]) 你会得到以下结果： 12345678Note: the last rows are the information for Intercept(&apos;##&apos;, &apos;-------------------------------------------------&apos;)(&apos;##&apos;, &apos; Estimate | Std.Error | t Values | P-value&apos;)(&apos;##&apos;, &apos; 0.042857&apos;, &apos; 0.001668&apos;, &apos; 25.692&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos; 0.199922&apos;, &apos; 0.009881&apos;, &apos; 20.232&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos; -0.001957&apos;, &apos; 0.006917&apos;, &apos; -0.283&apos;, &apos; 0.777757&apos;)(&apos;##&apos;, &apos; 3.007515&apos;, &apos; 0.406389&apos;, &apos; 7.401&apos;, &apos; 0.000000&apos;)(&apos;##&apos;, &apos;---&apos;) 做出预测 12# Make predictions.predictions = model.transform(testData) 12# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+---------------+-----+------------------+| features|label| prediction|+---------------+-----+------------------+| [0.7,39.6,8.7]| 1.6|10.937383732327625|| [4.1,11.6,5.7]| 3.2| 5.491166258750164||[7.3,28.1,41.4]| 5.5| 8.8571603947873|| [8.6,2.1,1.0]| 4.8| 3.793966281660073||[17.2,4.1,31.6]| 5.9| 4.502507124763654|+---------------+-----+------------------+only showing top 5 rows 评估 123456789from pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.evaluation import RegressionEvaluator# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator(labelCol=&quot;label&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;)rmse = evaluator.evaluate(predictions)print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) 最终的均方根误差（RMSE）如下： 1Root Mean Squared Error (RMSE) on test data = 1.89857 123456y_true = predictions.select(&quot;label&quot;).toPandas()y_pred = predictions.select(&quot;prediction&quot;).toPandas()import sklearn.metricsr2_score = sklearn.metrics.r2_score(y_true, y_pred)print(&apos;r2_score: &#123;0&#125;&apos;.format(r2_score)) 之后你会得到 值： 1r2_score: 0.87707391843 8.3. 决策树回归8.3.1. 介绍8.3.2. 如何求解8.3.3. 示例 Jupyter 笔记本可以从决策树回归下载。 参数的更多信息请见决策树回归 API。 建立 spark 上下文和 SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark regression example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() 加载数据集 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;).\ load(&quot;../data/Advertising.csv&quot;,header=True); 检查数据集 12df.show(5,True)df.printSchema() 之后你会得到 12345678910111213141516+-----+-----+---------+-----+| TV|Radio|Newspaper|Sales|+-----+-----+---------+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5||180.8| 10.8| 58.4| 12.9|+-----+-----+---------+-----+only showing top 5 rowsroot |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 您还可以从数据帧中获取统计结果（不幸的是，它仅适用于数字）。 1df.describe().show() 之后你会得到 123456789+-------+-----------------+------------------+------------------+------------------+|summary| TV| Radio| Newspaper| Sales|+-------+-----------------+------------------+------------------+------------------+| count| 200| 200| 200| 200|| mean| 147.0425|23.264000000000024|30.553999999999995|14.022500000000003|| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|| min| 0.7| 0.0| 0.3| 1.6|| max| 296.4| 49.6| 114.0| 27.0|+-------+-----------------+------------------+------------------+------------------+ 将数据转换为密集向量（特征和标签） 注意 强烈建议您尝试使用我的get_dummy函数来处理数据集中的分类数据。 监督学习版本： 12345678910111213141516171819202122232425def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(indexCol,&apos;features&apos;,&apos;label&apos;) 无监督学习版本： 1234567891011121314151617181920212223242526272829def get_dummy(df,indexCol,categoricalCols,continuousCols): &apos;&apos;&apos; Get dummy variables and concat with continuous variables for unsupervised learning. :param df: the dataframe :param categoricalCols: the name list of the categorical data :param continuousCols: the name list of the numerical data :return k: feature matrix :author: Wenqiang Feng :email: von198@gmail.com &apos;&apos;&apos; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) return data.select(indexCol,&apos;features&apos;) 123456789101112131415from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectors# I provide two ways to build the features and labels# method 1 (good for small feature):#def transData(row):# return Row(label=row[&quot;Sales&quot;],# features=Vectors.dense([row[&quot;TV&quot;],# row[&quot;Radio&quot;],# row[&quot;Newspaper&quot;]]))# Method 2 (good for large features):def transData(data):return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 12transformed= transData(df)transformed.show(5) 12345678910+-----------------+-----+| features|label|+-----------------+-----+|[230.1,37.8,69.2]| 22.1|| [44.5,39.3,45.1]| 10.4|| [17.2,45.9,69.3]| 9.3||[151.5,41.3,58.5]| 18.5||[180.8,10.8,58.4]| 12.9|+-----------------+-----+only showing top 5 rows 注意 您会发现 Spark 中所有监督机器学习算法都基于特征和标签（Spark 中的无监督机器学习算法基于特征）。 也就是说，当您在管道架构中准备好特征和标签时，您可以使用 Spark 中的所有机器学习算法。 将数据转换为密集向量 1234567# convert the data to dense vectordef transData(data): return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\ toDF([&apos;label&apos;,&apos;features&apos;])transformed = transData(df)transformed.show(5) 处理类别变量 1234567891011121314from pyspark.ml import Pipelinefrom pyspark.ml.regression import LinearRegressionfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluator# Automatically identify categorical features, and index them.# We specify maxCategories so features with &gt; 4# distinct values are treated as continuous.featureIndexer = VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;,\ maxCategories=4).fit(transformed)data = featureIndexer.transform(transformed) When you check you data at this point, you will get 12345678910+-----------------+-----+-----------------+| features|label| indexedFeatures|+-----------------+-----+-----------------+|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|| [17.2,45.9,69.3]| 9.3| [17.2,45.9,69.3]||[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]||[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|+-----------------+-----+-----------------+only showing top 5 rows 将数据分割为训练和测试集（留出 40% 用于测试） 12# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = transformed.randomSplit([0.6, 0.4]) 您可以按照以下方式检查您的训练和测试数据（在我看来，在原型阶段始终跟踪您的数据总是很好）： 12trainingData.show(5)testData.show(5) 之后你会得到 123456789101112131415161718192021+---------------+-----+---------------+| features|label|indexedFeatures|+---------------+-----+---------------+| [4.1,11.6,5.7]| 3.2| [4.1,11.6,5.7]||[7.3,28.1,41.4]| 5.5|[7.3,28.1,41.4]|| [8.4,27.2,2.1]| 5.7| [8.4,27.2,2.1]|| [8.6,2.1,1.0]| 4.8| [8.6,2.1,1.0]||[8.7,48.9,75.0]| 7.2|[8.7,48.9,75.0]|+---------------+-----+---------------+only showing top 5 rows+----------------+-----+----------------+| features|label| indexedFeatures|+----------------+-----+----------------+| [0.7,39.6,8.7]| 1.6| [0.7,39.6,8.7]|| [5.4,29.9,9.4]| 5.3| [5.4,29.9,9.4]|| [7.8,38.9,50.6]| 6.6| [7.8,38.9,50.6]||[17.2,45.9,69.3]| 9.3|[17.2,45.9,69.3]||[18.7,12.1,23.4]| 6.7|[18.7,12.1,23.4]|+----------------+-----+----------------+only showing top 5 rows 拟合决策树回归模型 1234from pyspark.ml.regression import DecisionTreeRegressor# Train a DecisionTree model.dt = DecisionTreeRegressor(featuresCol=&quot;indexedFeatures&quot;) 流水线架构 1234# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, dt])model = pipeline.fit(trainingData) 做出预测 12# Make predictions.predictions = model.transform(testData) 12# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+----------+-----+----------------+|prediction|label| features|+----------+-----+----------------+| 7.2| 1.6| [0.7,39.6,8.7]|| 7.3| 5.3| [5.4,29.9,9.4]|| 7.2| 6.6| [7.8,38.9,50.6]|| 8.64| 9.3|[17.2,45.9,69.3]|| 6.45| 6.7|[18.7,12.1,23.4]|+----------+-----+----------------+only showing top 5 rows 评估 123456789from pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.evaluation import RegressionEvaluator# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator(labelCol=&quot;label&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;)rmse = evaluator.evaluate(predictions)print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) 最终的均方根误差（RMSE）如下： 1Root Mean Squared Error (RMSE) on test data = 1.50999 123456y_true = predictions.select(&quot;label&quot;).toPandas()y_pred = predictions.select(&quot;prediction&quot;).toPandas()import sklearn.metricsr2_score = sklearn.metrics.r2_score(y_true, y_pred)print(&apos;r2_score: &#123;0&#125;&apos;.format(r2_score)) 之后你会得到 值： 1r2_score: 0.911024318967 你可以检查特征上的重要性 1model.stages[1].featureImportances 您将获得每个特征的权重 1SparseVector(3, &#123;0: 0.6811, 1: 0.3187, 2: 0.0002&#125;) 8.4. 随机森林回归8.4.1. 简介8.4.2. 如何求解8.4.3. 示例 Jupyter 笔记本可以从随机森林回归下载。 参数的更多信息请见随机森林回归 API。 建立 spark 上下文和 SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark RandomForest Regression example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() 加载数据集 1234567df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;).\ load(&quot;../data/Advertising.csv&quot;,header=True);df.show(5,True)df.printSchema() 12345678910111213141516+-----+-----+---------+-----+| TV|Radio|Newspaper|Sales|+-----+-----+---------+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5||180.8| 10.8| 58.4| 12.9|+-----+-----+---------+-----+only showing top 5 rowsroot |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 1234567891011df.describe().show()+-------+-----------------+------------------+------------------+------------------+|summary| TV| Radio| Newspaper| Sales|+-------+-----------------+------------------+------------------+------------------+| count| 200| 200| 200| 200|| mean| 147.0425|23.264000000000024|30.553999999999995|14.022500000000003|| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|| min| 0.7| 0.0| 0.3| 1.6|| max| 296.4| 49.6| 114.0| 27.0|+-------+-----------------+------------------+------------------+------------------+ 将数据转换为密集向量（特征和标签） 注意 强烈建议您尝试使用我的get_dummy函数来处理数据集中的分类数据。 监督学习版本： 12345678910111213141516171819202122232425def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(indexCol,&apos;features&apos;,&apos;label&apos;) 无监督学习版本： 1234567891011121314151617181920212223242526272829def get_dummy(df,indexCol,categoricalCols,continuousCols): &apos;&apos;&apos; Get dummy variables and concat with continuous variables for unsupervised learning. :param df: the dataframe :param categoricalCols: the name list of the categorical data :param continuousCols: the name list of the numerical data :return k: feature matrix :author: Wenqiang Feng :email: von198@gmail.com &apos;&apos;&apos; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) return data.select(indexCol,&apos;features&apos;) 1234567891011from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectors# convert the data to dense vector#def transData(row):# return Row(label=row[&quot;Sales&quot;],# features=Vectors.dense([row[&quot;TV&quot;],# row[&quot;Radio&quot;],# row[&quot;Newspaper&quot;]]))def transData(data): return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 将数据转换为密集向量 12transformed= transData(df)transformed.show(5) 12345678910+-----------------+-----+| features|label|+-----------------+-----+|[230.1,37.8,69.2]| 22.1|| [44.5,39.3,45.1]| 10.4|| [17.2,45.9,69.3]| 9.3||[151.5,41.3,58.5]| 18.5||[180.8,10.8,58.4]| 12.9|+-----------------+-----+only showing top 5 rows 处理类别变量 1234567891011from pyspark.ml import Pipelinefrom pyspark.ml.regression import LinearRegressionfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluatorfeatureIndexer = VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;,\ maxCategories=4).fit(transformed)data = featureIndexer.transform(transformed)data.show(5,True) 12345678910+-----------------+-----+-----------------+| features|label| indexedFeatures|+-----------------+-----+-----------------+|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|| [17.2,45.9,69.3]| 9.3| [17.2,45.9,69.3]||[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]||[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|+-----------------+-----+-----------------+only showing top 5 rows 将数据分割为训练和测试集（留出 40% 用于测试） 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = data.randomSplit([0.6, 0.4])trainingData.show(5)testData.show(5) 123456789101112131415161718192021+----------------+-----+----------------+| features|label| indexedFeatures|+----------------+-----+----------------+| [0.7,39.6,8.7]| 1.6| [0.7,39.6,8.7]|| [8.6,2.1,1.0]| 4.8| [8.6,2.1,1.0]|| [8.7,48.9,75.0]| 7.2| [8.7,48.9,75.0]||[11.7,36.9,45.2]| 7.3|[11.7,36.9,45.2]||[13.2,15.9,49.6]| 5.6|[13.2,15.9,49.6]|+----------------+-----+----------------+only showing top 5 rows+---------------+-----+---------------+| features|label|indexedFeatures|+---------------+-----+---------------+| [4.1,11.6,5.7]| 3.2| [4.1,11.6,5.7]|| [5.4,29.9,9.4]| 5.3| [5.4,29.9,9.4]||[7.3,28.1,41.4]| 5.5|[7.3,28.1,41.4]||[7.8,38.9,50.6]| 6.6|[7.8,38.9,50.6]|| [8.4,27.2,2.1]| 5.7| [8.4,27.2,2.1]|+---------------+-----+---------------+only showing top 5 rows 拟合随机森林回归模型 12345# Import LinearRegression classfrom pyspark.ml.regression import RandomForestRegressor# Define LinearRegression algorithmrf = RandomForestRegressor() # featuresCol=&quot;indexedFeatures&quot;,numTrees=2, maxDepth=2, seed=42 注意 如果你决定使用indexedFeatures特征，你需要添加参数featuresCol=&quot;indexedFeatures&quot;。 流水线架构 123# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, rf])model = pipeline.fit(trainingData) 做出预测 1234predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;, &quot;prediction&quot;).show(5) 12345678910+---------------+-----+------------------+| features|label| prediction|+---------------+-----+------------------+| [4.1,11.6,5.7]| 3.2| 8.155439814814816|| [5.4,29.9,9.4]| 5.3|10.412769901394899||[7.3,28.1,41.4]| 5.5| 12.13735648148148||[7.8,38.9,50.6]| 6.6|11.321796703296704|| [8.4,27.2,2.1]| 5.7|12.071421957671957|+---------------+-----+------------------+only showing top 5 rows 评估 12345# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator( labelCol=&quot;label&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;)rmse = evaluator.evaluate(predictions)print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) 1Root Mean Squared Error (RMSE) on test data = 2.35912 123import sklearn.metricsr2_score = sklearn.metrics.r2_score(y_true, y_pred)print(&apos;r2_score: &#123;:4.3f&#125;&apos;.format(r2_score)) 1r2_score: 0.831 特征重要性 1model.stages[-1].featureImportances 1SparseVector(3, &#123;0: 0.4994, 1: 0.3196, 2: 0.181&#125;) 1model.stages[-1].trees 1234567891011121314151617181920[DecisionTreeRegressionModel (uid=dtr_c75f1c75442c) of depth 5 with 43 nodes, DecisionTreeRegressionModel (uid=dtr_70fc2d441581) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_bc8464f545a7) of depth 5 with 31 nodes, DecisionTreeRegressionModel (uid=dtr_a8a7e5367154) of depth 5 with 59 nodes, DecisionTreeRegressionModel (uid=dtr_3ea01314fcbc) of depth 5 with 47 nodes, DecisionTreeRegressionModel (uid=dtr_be9a04ac22a6) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_38610d47328a) of depth 5 with 51 nodes, DecisionTreeRegressionModel (uid=dtr_bf14aea0ad3b) of depth 5 with 49 nodes, DecisionTreeRegressionModel (uid=dtr_cde24ebd6bb6) of depth 5 with 39 nodes, DecisionTreeRegressionModel (uid=dtr_a1fc9bd4fbeb) of depth 5 with 57 nodes, DecisionTreeRegressionModel (uid=dtr_37798d6db1ba) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_c078b73ada63) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_fd00e3a070ad) of depth 5 with 55 nodes, DecisionTreeRegressionModel (uid=dtr_9d01d5fb8604) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_8bd8bdddf642) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_e53b7bae30f8) of depth 5 with 49 nodes, DecisionTreeRegressionModel (uid=dtr_808a869db21c) of depth 5 with 47 nodes, DecisionTreeRegressionModel (uid=dtr_64d0916bceb0) of depth 5 with 33 nodes, DecisionTreeRegressionModel (uid=dtr_0891055fff94) of depth 5 with 55 nodes, DecisionTreeRegressionModel (uid=dtr_19c8bbad26c2) of depth 5 with 51 nodes] 8.5. 梯度提升树回归8.5.1. 简介8.5.2. 如何求解8.5.3. 示例 Jupyter 笔记本可以从梯度提升树回归。 参数的更多信息请见梯度提升树回归 API。 建立 spark 上下文和 SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark GBTRegressor example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() 加载数据集 1234567df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;).\ load(&quot;../data/Advertising.csv&quot;,header=True);df.show(5,True)df.printSchema() 12345678910111213141516+-----+-----+---------+-----+| TV|Radio|Newspaper|Sales|+-----+-----+---------+-----+|230.1| 37.8| 69.2| 22.1|| 44.5| 39.3| 45.1| 10.4|| 17.2| 45.9| 69.3| 9.3||151.5| 41.3| 58.5| 18.5||180.8| 10.8| 58.4| 12.9|+-----+-----+---------+-----+only showing top 5 rowsroot |-- TV: double (nullable = true) |-- Radio: double (nullable = true) |-- Newspaper: double (nullable = true) |-- Sales: double (nullable = true) 1234567891011df.describe().show()+-------+-----------------+------------------+------------------+------------------+|summary| TV| Radio| Newspaper| Sales|+-------+-----------------+------------------+------------------+------------------+| count| 200| 200| 200| 200|| mean| 147.0425|23.264000000000024|30.553999999999995|14.022500000000003|| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|| min| 0.7| 0.0| 0.3| 1.6|| max| 296.4| 49.6| 114.0| 27.0|+-------+-----------------+------------------+------------------+------------------+ 将数据转换为密集向量（特征和标签） 注意 强烈建议您尝试使用我的get_dummy函数来处理数据集中的分类数据。 监督学习版本： 12345678910111213141516171819202122232425def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(indexCol,&apos;features&apos;,&apos;label&apos;) 无监督学习版本： 1234567891011121314151617181920212223242526272829def get_dummy(df,indexCol,categoricalCols,continuousCols): &apos;&apos;&apos; Get dummy variables and concat with continuous variables for unsupervised learning. :param df: the dataframe :param categoricalCols: the name list of the categorical data :param continuousCols: the name list of the numerical data :return k: feature matrix :author: Wenqiang Feng :email: von198@gmail.com &apos;&apos;&apos; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) return data.select(indexCol,&apos;features&apos;) 1234567891011from pyspark.sql import Rowfrom pyspark.ml.linalg import Vectors# convert the data to dense vector#def transData(row):# return Row(label=row[&quot;Sales&quot;],# features=Vectors.dense([row[&quot;TV&quot;],# row[&quot;Radio&quot;],# row[&quot;Newspaper&quot;]]))def transData(data): return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 将数据转换为密集向量 12transformed= transData(df)transformed.show(5) 12345678910+-----------------+-----+| features|label|+-----------------+-----+|[230.1,37.8,69.2]| 22.1|| [44.5,39.3,45.1]| 10.4|| [17.2,45.9,69.3]| 9.3||[151.5,41.3,58.5]| 18.5||[180.8,10.8,58.4]| 12.9|+-----------------+-----+only showing top 5 rows 处理类别变量 1234567891011from pyspark.ml import Pipelinefrom pyspark.ml.regression import GBTRegressorfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluatorfeatureIndexer = VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;,\ maxCategories=4).fit(transformed)data = featureIndexer.transform(transformed)data.show(5,True) 12345678910+-----------------+-----+-----------------+| features|label| indexedFeatures|+-----------------+-----+-----------------+|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|| [17.2,45.9,69.3]| 9.3| [17.2,45.9,69.3]||[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]||[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|+-----------------+-----+-----------------+only showing top 5 rows 将数据分割为训练和测试集（留出 40% 用于测试） 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = data.randomSplit([0.6, 0.4])trainingData.show(5)testData.show(5) 123456789101112131415161718192021+----------------+-----+----------------+| features|label| indexedFeatures|+----------------+-----+----------------+| [0.7,39.6,8.7]| 1.6| [0.7,39.6,8.7]|| [8.6,2.1,1.0]| 4.8| [8.6,2.1,1.0]|| [8.7,48.9,75.0]| 7.2| [8.7,48.9,75.0]||[11.7,36.9,45.2]| 7.3|[11.7,36.9,45.2]||[13.2,15.9,49.6]| 5.6|[13.2,15.9,49.6]|+----------------+-----+----------------+only showing top 5 rows+---------------+-----+---------------+| features|label|indexedFeatures|+---------------+-----+---------------+| [4.1,11.6,5.7]| 3.2| [4.1,11.6,5.7]|| [5.4,29.9,9.4]| 5.3| [5.4,29.9,9.4]||[7.3,28.1,41.4]| 5.5|[7.3,28.1,41.4]||[7.8,38.9,50.6]| 6.6|[7.8,38.9,50.6]|| [8.4,27.2,2.1]| 5.7| [8.4,27.2,2.1]|+---------------+-----+---------------+only showing top 5 rows 拟合梯度提升树模型 12345# Import LinearRegression classfrom pyspark.ml.regression import GBTRegressor# Define LinearRegression algorithmrf = GBTRegressor() #numTrees=2, maxDepth=2, seed=42 注意 如果你决定使用indexedFeatures特征，你需要添加参数featuresCol=&quot;indexedFeatures&quot;。 流水线架构 123# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, rf])model = pipeline.fit(trainingData) 做出预测 1234predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;, &quot;prediction&quot;).show(5) 12345678910+----------------+-----+------------------+| features|label| prediction|+----------------+-----+------------------+| [7.8,38.9,50.6]| 6.6| 6.836040343319862|| [8.6,2.1,1.0]| 4.8| 5.652202764688849|| [8.7,48.9,75.0]| 7.2| 6.908750296855572|| [13.1,0.4,25.6]| 5.3| 5.784020210692574||[19.6,20.1,17.0]| 7.6|6.8678921062629295|+----------------+-----+------------------+only showing top 5 rows 评估 12345# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator( labelCol=&quot;label&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;rmse&quot;)rmse = evaluator.evaluate(predictions)print(&quot;Root Mean Squared Error (RMSE) on test data = %g&quot; % rmse) 1Root Mean Squared Error (RMSE) on test data = 1.36939 123import sklearn.metricsr2_score = sklearn.metrics.r2_score(y_true, y_pred)print(&apos;r2_score: &#123;:4.3f&#125;&apos;.format(r2_score)) 1r2_score: 0.932 特征重要性 1model.stages[-1].featureImportances 1SparseVector(3, &#123;0: 0.3716, 1: 0.3525, 2: 0.2759&#125;) 1model.stages[-1].trees 1234567891011121314151617181920[DecisionTreeRegressionModel (uid=dtr_7f5cd2ef7cb6) of depth 5 with 61 nodes, DecisionTreeRegressionModel (uid=dtr_ef3ab6baeac9) of depth 5 with 39 nodes, DecisionTreeRegressionModel (uid=dtr_07c6e3cf3819) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_ce724af79a2b) of depth 5 with 47 nodes, DecisionTreeRegressionModel (uid=dtr_d149ecc71658) of depth 5 with 55 nodes, DecisionTreeRegressionModel (uid=dtr_d3a79bdea516) of depth 5 with 43 nodes, DecisionTreeRegressionModel (uid=dtr_7abc1a337844) of depth 5 with 51 nodes, DecisionTreeRegressionModel (uid=dtr_480834b46d8f) of depth 5 with 33 nodes, DecisionTreeRegressionModel (uid=dtr_0cbd1eaa3874) of depth 5 with 39 nodes, DecisionTreeRegressionModel (uid=dtr_8088ac71a204) of depth 5 with 57 nodes, DecisionTreeRegressionModel (uid=dtr_2ceb9e8deb45) of depth 5 with 47 nodes, DecisionTreeRegressionModel (uid=dtr_cc334e84e9a2) of depth 5 with 57 nodes, DecisionTreeRegressionModel (uid=dtr_a665c562929e) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_2999b1ffd2dc) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_29965cbe8cfc) of depth 5 with 55 nodes, DecisionTreeRegressionModel (uid=dtr_731df51bf0ad) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_354cf33424da) of depth 5 with 51 nodes, DecisionTreeRegressionModel (uid=dtr_4230f200b1c0) of depth 5 with 41 nodes, DecisionTreeRegressionModel (uid=dtr_3279cdc1ce1d) of depth 5 with 45 nodes, DecisionTreeRegressionModel (uid=dtr_f474a99ff06e) of depth 5 with 55 nodes] 10. ClassificationChinese proverb Birds of a feather folock together. – old Chinese proverb 10.1. Binomial logistic regression10.1.1. Introduction10.1.2. Demo The Jupyter notebook can be download from Logistic Regression. For more details, please visit Logistic Regression API . Note In this demo, I introduced a new function get_dummy to deal with the categorical data. I highly recommend you to use my get_dummy function in the other cases. This function will save a lot of time for you. Set up spark context and SparkSession 1234567from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&quot;Python Spark Logistic Regression example&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() Load dataset 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;) \ .options(header=&apos;true&apos;, inferschema=&apos;true&apos;) \ .load(&quot;./data/bank.csv&quot;,header=True);df.drop(&apos;day&apos;,&apos;month&apos;,&apos;poutcome&apos;).show(5) 12345678910+---+------------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+---+|age| job|marital|education|default|balance|housing|loan|contact|duration|campaign|pdays|previous| y|+---+------------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+---+| 58| management|married| tertiary| no| 2143| yes| no|unknown| 261| 1| -1| 0| no|| 44| technician| single|secondary| no| 29| yes| no|unknown| 151| 1| -1| 0| no|| 33|entrepreneur|married|secondary| no| 2| yes| yes|unknown| 76| 1| -1| 0| no|| 47| blue-collar|married| unknown| no| 1506| yes| no|unknown| 92| 1| -1| 0| no|| 33| unknown| single| unknown| no| 1| no| no|unknown| 198| 1| -1| 0| no|+---+------------+-------+---------+-------+-------+-------+----+-------+--------+--------+-----+--------+---+only showing top 5 rows 1df.printSchema() 123456789101112131415161718root |-- age: integer (nullable = true) |-- job: string (nullable = true) |-- marital: string (nullable = true) |-- education: string (nullable = true) |-- default: string (nullable = true) |-- balance: integer (nullable = true) |-- housing: string (nullable = true) |-- loan: string (nullable = true) |-- contact: string (nullable = true) |-- day: integer (nullable = true) |-- month: string (nullable = true) |-- duration: integer (nullable = true) |-- campaign: integer (nullable = true) |-- pdays: integer (nullable = true) |-- previous: integer (nullable = true) |-- poutcome: string (nullable = true) |-- y: string (nullable = true) Note You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset. Supervised learning version: 123456789101112131415161718192021222324252627&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):&gt; &gt; from pyspark.ml import Pipeline&gt; from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler&gt; from pyspark.sql.functions import col&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; data = data.withColumn(&apos;label&apos;,col(labelCol))&gt; &gt; return data.select(indexCol,&apos;features&apos;,&apos;label&apos;)&gt; &gt; Unsupervised learning version: 12345678910111213141516171819202122232425262728293031&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols):&gt; &apos;&apos;&apos;&gt; Get dummy variables and concat with continuous variables for unsupervised learning.&gt; :param df: the dataframe&gt; :param categoricalCols: the name list of the categorical data&gt; :param continuousCols: the name list of the numerical data&gt; :return k: feature matrix&gt; &gt; :author: Wenqiang Feng&gt; :email: von198@gmail.com&gt; &apos;&apos;&apos;&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; return data.select(indexCol,&apos;features&apos;)&gt; &gt; 12345678910111213141516171819202122232425def get_dummy(df,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(&apos;features&apos;,&apos;label&apos;) Deal with categorical data and Convert the data to dense vector 12345678catcols = [&apos;job&apos;,&apos;marital&apos;,&apos;education&apos;,&apos;default&apos;, &apos;housing&apos;,&apos;loan&apos;,&apos;contact&apos;,&apos;poutcome&apos;]num_cols = [&apos;balance&apos;, &apos;duration&apos;,&apos;campaign&apos;,&apos;pdays&apos;,&apos;previous&apos;,]labelCol = &apos;y&apos;data = get_dummy(df,catcols,num_cols,labelCol)data.show(5) 12345678910+--------------------+-----+| features|label|+--------------------+-----+|(29,[1,11,14,16,1...| no||(29,[2,12,13,16,1...| no||(29,[7,11,13,16,1...| no||(29,[0,11,16,17,1...| no||(29,[12,16,18,20,...| no|+--------------------+-----+only showing top 5 rows Deal with Categorical Label and Variables 12345from pyspark.ml.feature import StringIndexer# Index labels, adding metadata to the label columnlabelIndexer = StringIndexer(inputCol=&apos;label&apos;, outputCol=&apos;indexedLabel&apos;).fit(data)labelIndexer.transform(data).show(5, True) 12345678910+--------------------+-----+------------+| features|label|indexedLabel|+--------------------+-----+------------+|(29,[1,11,14,16,1...| no| 0.0||(29,[2,12,13,16,1...| no| 0.0||(29,[7,11,13,16,1...| no| 0.0||(29,[0,11,16,17,1...| no| 0.0||(29,[12,16,18,20,...| no| 0.0|+--------------------+-----+------------+only showing top 5 rows 1234567from pyspark.ml.feature import VectorIndexer# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;, \ maxCategories=4).fit(data)featureIndexer.transform(data).show(5, True) 12345678910+--------------------+-----+--------------------+| features|label| indexedFeatures|+--------------------+-----+--------------------+|(29,[1,11,14,16,1...| no|(29,[1,11,14,16,1...||(29,[2,12,13,16,1...| no|(29,[2,12,13,16,1...||(29,[7,11,13,16,1...| no|(29,[7,11,13,16,1...||(29,[0,11,16,17,1...| no|(29,[0,11,16,17,1...||(29,[12,16,18,20,...| no|(29,[12,16,18,20,...|+--------------------+-----+--------------------+only showing top 5 rows Split the data to training and test data sets 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = data.randomSplit([0.6, 0.4])trainingData.show(5,False)testData.show(5,False) 123456789101112131415161718192021+-------------------------------------------------------------------------------------------------+-----+|features |label|+-------------------------------------------------------------------------------------------------+-----+|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-731.0,401.0,4.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-723.0,112.0,2.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-626.0,205.0,1.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-498.0,357.0,1.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-477.0,473.0,2.0,-1.0])|no |+-------------------------------------------------------------------------------------------------+-----+only showing top 5 rows+-------------------------------------------------------------------------------------------------+-----+|features |label|+-------------------------------------------------------------------------------------------------+-----+|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-648.0,280.0,2.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-596.0,147.0,1.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-529.0,416.0,4.0,-1.0])|no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-518.0,46.0,5.0,-1.0]) |no ||(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-470.0,275.0,2.0,-1.0])|no |+-------------------------------------------------------------------------------------------------+-----+only showing top 5 rows Fit Logistic Regression Model 12from pyspark.ml.classification import LogisticRegressionlogr = LogisticRegression(featuresCol=&apos;indexedFeatures&apos;, labelCol=&apos;indexedLabel&apos;) Pipeline Architecture 123# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;, labels=labelIndexer.labels) 12# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter]) 12# Train model. This also runs the indexers.model = pipeline.fit(trainingData) Make predictions 1234# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+--------------------+-----+--------------+| features|label|predictedLabel|+--------------------+-----+--------------+|(29,[0,11,13,16,1...| no| no||(29,[0,11,13,16,1...| no| no||(29,[0,11,13,16,1...| no| no||(29,[0,11,13,16,1...| no| no||(29,[0,11,13,16,1...| no| no|+--------------------+-----+--------------+only showing top 5 rows Evaluation 1234567from pyspark.ml.evaluation import MulticlassClassificationEvaluator# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %g&quot; % (1.0 - accuracy)) 1Test Error = 0.0987688 12345678910111213141516171819lrModel = model.stages[2]trainingSummary = lrModel.summary# Obtain the objective per iteration# objectiveHistory = trainingSummary.objectiveHistory# print(&quot;objectiveHistory:&quot;)# for objective in objectiveHistory:# print(objective)# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.trainingSummary.roc.show(5)print(&quot;areaUnderROC: &quot; + str(trainingSummary.areaUnderROC))# Set the model threshold to maximize F-MeasurefMeasure = trainingSummary.fMeasureByThresholdmaxFMeasure = fMeasure.groupBy().max(&apos;F-Measure&apos;).select(&apos;max(F-Measure)&apos;).head(5)# bestThreshold = fMeasure.where(fMeasure[&apos;F-Measure&apos;] == maxFMeasure[&apos;max(F-Measure)&apos;]) \# .select(&apos;threshold&apos;).head()[&apos;threshold&apos;]# lr.setThreshold(bestThreshold) You can use z.show() to get the data and plot the ROC curves: You can also register a TempTable data.registerTempTable(&#39;roc_data&#39;) and then use sql to plot the ROC curve: visualization 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as npimport itertoolsdef plot_confusion_matrix(cm, classes, normalize=False, title=&apos;Confusion matrix&apos;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&apos;float&apos;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&apos;Confusion matrix, without normalization&apos;) print(cm) plt.imshow(cm, interpolation=&apos;nearest&apos;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &apos;.2f&apos; if normalize else &apos;d&apos; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&apos;True label&apos;) plt.xlabel(&apos;Predicted label&apos;) 123456class_temp = predictions.select(&quot;label&quot;).groupBy(&quot;label&quot;)\ .count().sort(&apos;count&apos;, ascending=False).toPandas()class_temp = class_temp[&quot;label&quot;].values.tolist()class_names = map(str, class_temp)# # # print(class_name)class_names 1[&apos;no&apos;, &apos;yes&apos;] 123456789from sklearn.metrics import confusion_matrixy_true = predictions.select(&quot;label&quot;)y_true = y_true.toPandas()y_pred = predictions.select(&quot;predictedLabel&quot;)y_pred = y_pred.toPandas()cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_names)cnf_matrix 12array([[15657, 379], [ 1410, 667]]) 12345# Plot non-normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, title=&apos;Confusion matrix, without normalization&apos;)plt.show() 123Confusion matrix, without normalization[[15657 379] [ 1410 667]] 123456# Plot normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title=&apos;Normalized confusion matrix&apos;)plt.show() 123Normalized confusion matrix[[ 0.97636568 0.02363432] [ 0.67886375 0.32113625]] 10.2. Multinomial logistic regression10.2.1. Introduction10.2.2. Demo The Jupyter notebook can be download from Logistic Regression. For more details, please visit Logistic Regression API . Note In this demo, I introduced a new function get_dummy to deal with the categorical data. I highly recommend you to use my get_dummy function in the other cases. This function will save a lot of time for you. Set up spark context and SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark MultinomialLogisticRegression classification&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() Load dataset 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;) \ .options(header=&apos;true&apos;, inferschema=&apos;true&apos;) \ .load(&quot;./data/WineData2.csv&quot;,header=True);df.show(5) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| 5|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| 5|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| 6|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 1df.printSchema() 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) 12345678910111213141516171819202122# Convert to float formatdef string_to_float(x): return float(x)#def condition(r): if (0&lt;= r &lt;= 4): label = &quot;low&quot; elif(4&lt; r &lt;= 6): label = &quot;medium&quot; else: label = &quot;high&quot; return labelfrom pyspark.sql.functions import udffrom pyspark.sql.types import StringType, DoubleTypestring_to_float_udf = udf(string_to_float, DoubleType())quality_udf = udf(lambda x: condition(x), StringType())df = df.withColumn(&quot;quality&quot;, quality_udf(&quot;quality&quot;))df.show(5,True) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| medium|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| medium|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| medium|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 1df.printSchema() 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) Deal with categorical data and Convert the data to dense vector Note You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset. Supervised learning version: 123456789101112131415161718192021222324252627&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):&gt; &gt; from pyspark.ml import Pipeline&gt; from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler&gt; from pyspark.sql.functions import col&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; data = data.withColumn(&apos;label&apos;,col(labelCol))&gt; &gt; return data.select(indexCol,&apos;features&apos;,&apos;label&apos;)&gt; &gt; Unsupervised learning version: 12345678910111213141516171819202122232425262728293031&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols):&gt; &apos;&apos;&apos;&gt; Get dummy variables and concat with continuous variables for unsupervised learning.&gt; :param df: the dataframe&gt; :param categoricalCols: the name list of the categorical data&gt; :param continuousCols: the name list of the numerical data&gt; :return k: feature matrix&gt; &gt; :author: Wenqiang Feng&gt; :email: von198@gmail.com&gt; &apos;&apos;&apos;&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; return data.select(indexCol,&apos;features&apos;)&gt; &gt; 12345678910111213141516171819202122232425def get_dummy(df,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(&apos;features&apos;,&apos;label&apos;) Transform the dataset to DataFrame 12345678from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectorsfrom pyspark.ml import Pipelinefrom pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexerfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatordef transData(data):return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 12transformed = transData(df)transformed.show(5) 12345678910+--------------------+------+| features| label|+--------------------+------+|[7.4,0.7,0.0,1.9,...|medium||[7.8,0.88,0.0,2.6...|medium||[7.8,0.76,0.04,2....|medium||[11.2,0.28,0.56,1...|medium||[7.4,0.7,0.0,1.9,...|medium|+--------------------+------+only showing top 5 rows Deal with Categorical Label and Variables 1234# Index labels, adding metadata to the label columnlabelIndexer = StringIndexer(inputCol=&apos;label&apos;, outputCol=&apos;indexedLabel&apos;).fit(transformed)labelIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+------------+| features| label|indexedLabel|+--------------------+------+------------+|[7.4,0.7,0.0,1.9,...|medium| 0.0||[7.8,0.88,0.0,2.6...|medium| 0.0||[7.8,0.76,0.04,2....|medium| 0.0||[11.2,0.28,0.56,1...|medium| 0.0||[7.4,0.7,0.0,1.9,...|medium| 0.0|+--------------------+------+------------+only showing top 5 rows 123456# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;, \ maxCategories=4).fit(transformed)featureIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+--------------------+| features| label| indexedFeatures|+--------------------+------+--------------------+|[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...||[7.8,0.88,0.0,2.6...|medium|[7.8,0.88,0.0,2.6...||[7.8,0.76,0.04,2....|medium|[7.8,0.76,0.04,2....||[11.2,0.28,0.56,1...|medium|[11.2,0.28,0.56,1...||[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...|+--------------------+------+--------------------+only showing top 5 rows Split the data to training and test data sets 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = data.randomSplit([0.6, 0.4])trainingData.show(5,False)testData.show(5,False) 123456789101112131415161718192021+---------------------------------------------------------+------+|features |label |+---------------------------------------------------------+------+|[4.7,0.6,0.17,2.3,0.058,17.0,106.0,0.9932,3.85,0.6,12.9] |medium||[5.0,0.38,0.01,1.6,0.048,26.0,60.0,0.99084,3.7,0.75,14.0]|medium||[5.0,0.4,0.5,4.3,0.046,29.0,80.0,0.9902,3.49,0.66,13.6] |medium||[5.0,0.74,0.0,1.2,0.041,16.0,46.0,0.99258,4.01,0.59,12.5]|medium||[5.1,0.42,0.0,1.8,0.044,18.0,88.0,0.99157,3.68,0.73,13.6]|high |+---------------------------------------------------------+------+only showing top 5 rows+---------------------------------------------------------+------+|features |label |+---------------------------------------------------------+------+|[4.6,0.52,0.15,2.1,0.054,8.0,65.0,0.9934,3.9,0.56,13.1] |low ||[4.9,0.42,0.0,2.1,0.048,16.0,42.0,0.99154,3.71,0.74,14.0]|high ||[5.0,0.42,0.24,2.0,0.06,19.0,50.0,0.9917,3.72,0.74,14.0] |high ||[5.0,1.02,0.04,1.4,0.045,41.0,85.0,0.9938,3.75,0.48,10.5]|low ||[5.0,1.04,0.24,1.6,0.05,32.0,96.0,0.9934,3.74,0.62,11.5] |medium|+---------------------------------------------------------+------+only showing top 5 rows Fit Multinomial logisticRegression Classification Model 12from pyspark.ml.classification import LogisticRegressionlogr = LogisticRegression(featuresCol=&apos;indexedFeatures&apos;, labelCol=&apos;indexedLabel&apos;) Pipeline Architecture 123# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;, labels=labelIndexer.labels) 12# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter]) 12# Train model. This also runs the indexers.model = pipeline.fit(trainingData) Make predictions 1234# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+--------------------+------+--------------+| features| label|predictedLabel|+--------------------+------+--------------+|[4.6,0.52,0.15,2....| low| medium||[4.9,0.42,0.0,2.1...| high| high||[5.0,0.42,0.24,2....| high| high||[5.0,1.02,0.04,1....| low| medium||[5.0,1.04,0.24,1....|medium| medium|+--------------------+------+--------------+only showing top 5 rows Evaluation 1234567from pyspark.ml.evaluation import MulticlassClassificationEvaluator# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %g&quot; % (1.0 - accuracy)) 1Test Error = 0.181287 12345678910111213141516171819lrModel = model.stages[2]trainingSummary = lrModel.summary# Obtain the objective per iteration# objectiveHistory = trainingSummary.objectiveHistory# print(&quot;objectiveHistory:&quot;)# for objective in objectiveHistory:# print(objective)# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.trainingSummary.roc.show(5)print(&quot;areaUnderROC: &quot; + str(trainingSummary.areaUnderROC))# Set the model threshold to maximize F-MeasurefMeasure = trainingSummary.fMeasureByThresholdmaxFMeasure = fMeasure.groupBy().max(&apos;F-Measure&apos;).select(&apos;max(F-Measure)&apos;).head(5)# bestThreshold = fMeasure.where(fMeasure[&apos;F-Measure&apos;] == maxFMeasure[&apos;max(F-Measure)&apos;]) \# .select(&apos;threshold&apos;).head()[&apos;threshold&apos;]# lr.setThreshold(bestThreshold) You can use z.show() to get the data and plot the ROC curves: You can also register a TempTable data.registerTempTable(&#39;roc_data&#39;) and then use sql to plot the ROC curve: visualization 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as npimport itertoolsdef plot_confusion_matrix(cm, classes, normalize=False, title=&apos;Confusion matrix&apos;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&apos;float&apos;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&apos;Confusion matrix, without normalization&apos;) print(cm) plt.imshow(cm, interpolation=&apos;nearest&apos;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &apos;.2f&apos; if normalize else &apos;d&apos; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&apos;True label&apos;) plt.xlabel(&apos;Predicted label&apos;) 123456class_temp = predictions.select(&quot;label&quot;).groupBy(&quot;label&quot;)\ .count().sort(&apos;count&apos;, ascending=False).toPandas()class_temp = class_temp[&quot;label&quot;].values.tolist()class_names = map(str, class_temp)# # # print(class_name)class_names 1[&apos;medium&apos;, &apos;high&apos;, &apos;low&apos;] 123456789from sklearn.metrics import confusion_matrixy_true = predictions.select(&quot;label&quot;)y_true = y_true.toPandas()y_pred = predictions.select(&quot;predictedLabel&quot;)y_pred = y_pred.toPandas()cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_names)cnf_matrix 123array([[526, 11, 2], [ 73, 33, 0], [ 38, 0, 1]]) 12345# Plot non-normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, title=&apos;Confusion matrix, without normalization&apos;)plt.show() 1234Confusion matrix, without normalization[[526 11 2] [ 73 33 0] [ 38 0 1]] 123456# Plot normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title=&apos;Normalized confusion matrix&apos;)plt.show() 1234Normalized confusion matrix[[0.97588126 0.02040816 0.00371058] [0.68867925 0.31132075 0\. ] [0.97435897 0\. 0.02564103]] 10.3. Decision tree Classification10.3.1. Introduction10.3.2. Demo The Jupyter notebook can be download from Decision Tree Classification. For more details, please visit DecisionTreeClassifier API . Set up spark context and SparkSession 1234567from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&quot;Python Spark Decision Tree classification&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() Load dataset 12345df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;) \ .load(&quot;../data/WineData2.csv&quot;,header=True);df.show(5,True) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| 5|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| 5|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| 6|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 12345678910111213# Convert to float formatdef string_to_float(x): return float(x)#def condition(r): if (0&lt;= r &lt;= 4): label = &quot;low&quot; elif(4&lt; r &lt;= 6): label = &quot;medium&quot; else: label = &quot;high&quot; return label 1234from pyspark.sql.functions import udffrom pyspark.sql.types import StringType, DoubleTypestring_to_float_udf = udf(string_to_float, DoubleType())quality_udf = udf(lambda x: condition(x), StringType()) 123df = df.withColumn(&quot;quality&quot;, quality_udf(&quot;quality&quot;))df.show(5,True)df.printSchema() 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| medium|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| medium|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| medium|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) Convert the data to dense vector Note You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset. Supervised learning version: 123456789101112131415161718192021222324252627&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):&gt; &gt; from pyspark.ml import Pipeline&gt; from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler&gt; from pyspark.sql.functions import col&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; data = data.withColumn(&apos;label&apos;,col(labelCol))&gt; &gt; return data.select(indexCol,&apos;features&apos;,&apos;label&apos;)&gt; &gt; Unsupervised learning version: 12345678910111213141516171819202122232425262728293031&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols):&gt; &apos;&apos;&apos;&gt; Get dummy variables and concat with continuous variables for unsupervised learning.&gt; :param df: the dataframe&gt; :param categoricalCols: the name list of the categorical data&gt; :param continuousCols: the name list of the numerical data&gt; :return k: feature matrix&gt; &gt; :author: Wenqiang Feng&gt; :email: von198@gmail.com&gt; &apos;&apos;&apos;&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; return data.select(indexCol,&apos;features&apos;)&gt; &gt; 123456# !!!!caution: not from pyspark.mllib.linalg import Vectorsfrom pyspark.ml.linalg import Vectorsfrom pyspark.ml import Pipelinefrom pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexerfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator 12def transData(data): return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) Transform the dataset to DataFrame 12transformed = transData(df)transformed.show(5) 12345678910+--------------------+------+| features| label|+--------------------+------+|[7.4,0.7,0.0,1.9,...|medium||[7.8,0.88,0.0,2.6...|medium||[7.8,0.76,0.04,2....|medium||[11.2,0.28,0.56,1...|medium||[7.4,0.7,0.0,1.9,...|medium|+--------------------+------+only showing top 5 rows Deal with Categorical Label and Variables 1234# Index labels, adding metadata to the label columnlabelIndexer = StringIndexer(inputCol=&apos;label&apos;, outputCol=&apos;indexedLabel&apos;).fit(transformed)labelIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+------------+| features| label|indexedLabel|+--------------------+------+------------+|[7.4,0.7,0.0,1.9,...|medium| 0.0||[7.8,0.88,0.0,2.6...|medium| 0.0||[7.8,0.76,0.04,2....|medium| 0.0||[11.2,0.28,0.56,1...|medium| 0.0||[7.4,0.7,0.0,1.9,...|medium| 0.0|+--------------------+------+------------+only showing top 5 rows 123456 # Automatically identify categorical features, and index them. # Set maxCategories so features with &gt; 4 distinct values are treated as continuous. featureIndexer =VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;, \ maxCategories=4).fit(transformed)featureIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+--------------------+| features| label| indexedFeatures|+--------------------+------+--------------------+|[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...||[7.8,0.88,0.0,2.6...|medium|[7.8,0.88,0.0,2.6...||[7.8,0.76,0.04,2....|medium|[7.8,0.76,0.04,2....||[11.2,0.28,0.56,1...|medium|[11.2,0.28,0.56,1...||[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...|+--------------------+------+--------------------+only showing top 5 rows Split the data to training and test data sets 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = transformed.randomSplit([0.6, 0.4])trainingData.show(5)testData.show(5) 123456789101112131415161718192021+--------------------+------+| features| label|+--------------------+------+|[4.6,0.52,0.15,2....| low||[4.7,0.6,0.17,2.3...|medium||[5.0,1.02,0.04,1....| low||[5.0,1.04,0.24,1....|medium||[5.1,0.585,0.0,1....| high|+--------------------+------+only showing top 5 rows+--------------------+------+| features| label|+--------------------+------+|[4.9,0.42,0.0,2.1...| high||[5.0,0.38,0.01,1....|medium||[5.0,0.4,0.5,4.3,...|medium||[5.0,0.42,0.24,2....| high||[5.0,0.74,0.0,1.2...|medium|+--------------------+------+only showing top 5 rows Fit Decision Tree Classification Model 1234from pyspark.ml.classification import DecisionTreeClassifier# Train a DecisionTree modeldTree = DecisionTreeClassifier(labelCol=&apos;indexedLabel&apos;, featuresCol=&apos;indexedFeatures&apos;) Pipeline Architecture 123# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;, labels=labelIndexer.labels) 12# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree,labelConverter]) 12# Train model. This also runs the indexers.model = pipeline.fit(trainingData) Make predictions 1234# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+--------------------+------+--------------+| features| label|predictedLabel|+--------------------+------+--------------+|[4.9,0.42,0.0,2.1...| high| high||[5.0,0.38,0.01,1....|medium| medium||[5.0,0.4,0.5,4.3,...|medium| medium||[5.0,0.42,0.24,2....| high| medium||[5.0,0.74,0.0,1.2...|medium| medium|+--------------------+------+--------------+only showing top 5 rows Evaluation 12345678910from pyspark.ml.evaluation import MulticlassClassificationEvaluator# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %g&quot; % (1.0 - accuracy))rfModel = model.stages[-2]print(rfModel) # summary only 123Test Error = 0.45509DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4545ac8dca9c8438ef2a)of depth 5 with 59 nodes visualization 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as npimport itertoolsdef plot_confusion_matrix(cm, classes, normalize=False, title=&apos;Confusion matrix&apos;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&apos;float&apos;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&apos;Confusion matrix, without normalization&apos;) print(cm) plt.imshow(cm, interpolation=&apos;nearest&apos;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &apos;.2f&apos; if normalize else &apos;d&apos; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&apos;True label&apos;) plt.xlabel(&apos;Predicted label&apos;) 123456class_temp = predictions.select(&quot;label&quot;).groupBy(&quot;label&quot;)\ .count().sort(&apos;count&apos;, ascending=False).toPandas()class_temp = class_temp[&quot;label&quot;].values.tolist()class_names = map(str, class_temp)# # # print(class_name)class_names 1[&apos;medium&apos;, &apos;high&apos;, &apos;low&apos;] 123456789from sklearn.metrics import confusion_matrixy_true = predictions.select(&quot;label&quot;)y_true = y_true.toPandas()y_pred = predictions.select(&quot;predictedLabel&quot;)y_pred = y_pred.toPandas()cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_names)cnf_matrix 123array([[497, 29, 7], [ 40, 42, 0], [ 22, 0, 2]]) 12345# Plot non-normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, title=&apos;Confusion matrix, without normalization&apos;)plt.show() 1234Confusion matrix, without normalization[[497 29 7] [ 40 42 0] [ 22 0 2]] 123456# Plot normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title=&apos;Normalized confusion matrix&apos;)plt.show() 1234Normalized confusion matrix[[ 0.93245779 0.05440901 0.01313321] [ 0.48780488 0.51219512 0\. ] [ 0.91666667 0\. 0.08333333]] 10.4. Random forest Classification10.4.1. Introduction 10.4.2. Demo The Jupyter notebook can be download from Random forest Classification. For more details, please visit RandomForestClassifier API . Set up spark context and SparkSession 1234567from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&quot;Python Spark Decision Tree classification&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() Load dataset 12345df = spark.read.format(&apos;com.databricks.spark.csv&apos;).\ options(header=&apos;true&apos;, \ inferschema=&apos;true&apos;) \ .load(&quot;../data/WineData2.csv&quot;,header=True);df.show(5,True) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| 5|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| 5|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| 6|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 12345678910111213# Convert to float formatdef string_to_float(x): return float(x)#def condition(r): if (0&lt;= r &lt;= 4): label = &quot;low&quot; elif(4&lt; r &lt;= 6): label = &quot;medium&quot; else: label = &quot;high&quot; return label 1234from pyspark.sql.functions import udffrom pyspark.sql.types import StringType, DoubleTypestring_to_float_udf = udf(string_to_float, DoubleType())quality_udf = udf(lambda x: condition(x), StringType()) 123df = df.withColumn(&quot;quality&quot;, quality_udf(&quot;quality&quot;))df.show(5,True)df.printSchema() 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| medium|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| medium|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| medium|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) Convert the data to dense vector Note You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset. Supervised learning version: 123456789101112131415161718192021222324252627&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):&gt; &gt; from pyspark.ml import Pipeline&gt; from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler&gt; from pyspark.sql.functions import col&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; data = data.withColumn(&apos;label&apos;,col(labelCol))&gt; &gt; return data.select(indexCol,&apos;features&apos;,&apos;label&apos;)&gt; &gt; Unsupervised learning version: 12345678910111213141516171819202122232425262728293031&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols):&gt; &apos;&apos;&apos;&gt; Get dummy variables and concat with continuous variables for unsupervised learning.&gt; :param df: the dataframe&gt; :param categoricalCols: the name list of the categorical data&gt; :param continuousCols: the name list of the numerical data&gt; :return k: feature matrix&gt; &gt; :author: Wenqiang Feng&gt; :email: von198@gmail.com&gt; &apos;&apos;&apos;&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; return data.select(indexCol,&apos;features&apos;)&gt; &gt; 123456# !!!!caution: not from pyspark.mllib.linalg import Vectorsfrom pyspark.ml.linalg import Vectorsfrom pyspark.ml import Pipelinefrom pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexerfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator 12def transData(data): return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) Transform the dataset to DataFrame 12transformed = transData(df)transformed.show(5) 12345678910+--------------------+------+| features| label|+--------------------+------+|[7.4,0.7,0.0,1.9,...|medium||[7.8,0.88,0.0,2.6...|medium||[7.8,0.76,0.04,2....|medium||[11.2,0.28,0.56,1...|medium||[7.4,0.7,0.0,1.9,...|medium|+--------------------+------+only showing top 5 rows Deal with Categorical Label and Variables 1234# Index labels, adding metadata to the label columnlabelIndexer = StringIndexer(inputCol=&apos;label&apos;, outputCol=&apos;indexedLabel&apos;).fit(transformed)labelIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+------------+| features| label|indexedLabel|+--------------------+------+------------+|[7.4,0.7,0.0,1.9,...|medium| 0.0||[7.8,0.88,0.0,2.6...|medium| 0.0||[7.8,0.76,0.04,2....|medium| 0.0||[11.2,0.28,0.56,1...|medium| 0.0||[7.4,0.7,0.0,1.9,...|medium| 0.0|+--------------------+------+------------+only showing top 5 rows 123456 # Automatically identify categorical features, and index them. # Set maxCategories so features with &gt; 4 distinct values are treated as continuous. featureIndexer =VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;, \ maxCategories=4).fit(transformed)featureIndexer.transform(transformed).show(5, True) 12345678910+--------------------+------+--------------------+| features| label| indexedFeatures|+--------------------+------+--------------------+|[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...||[7.8,0.88,0.0,2.6...|medium|[7.8,0.88,0.0,2.6...||[7.8,0.76,0.04,2....|medium|[7.8,0.76,0.04,2....||[11.2,0.28,0.56,1...|medium|[11.2,0.28,0.56,1...||[7.4,0.7,0.0,1.9,...|medium|[7.4,0.7,0.0,1.9,...|+--------------------+------+--------------------+only showing top 5 rows Split the data to training and test data sets 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = transformed.randomSplit([0.6, 0.4])trainingData.show(5)testData.show(5) 123456789101112131415161718192021+--------------------+------+| features| label|+--------------------+------+|[4.6,0.52,0.15,2....| low||[4.7,0.6,0.17,2.3...|medium||[5.0,1.02,0.04,1....| low||[5.0,1.04,0.24,1....|medium||[5.1,0.585,0.0,1....| high|+--------------------+------+only showing top 5 rows+--------------------+------+| features| label|+--------------------+------+|[4.9,0.42,0.0,2.1...| high||[5.0,0.38,0.01,1....|medium||[5.0,0.4,0.5,4.3,...|medium||[5.0,0.42,0.24,2....| high||[5.0,0.74,0.0,1.2...|medium|+--------------------+------+only showing top 5 rows Fit Random Forest Classification Model 1234from pyspark.ml.classification import RandomForestClassifier# Train a RandomForest model.rf = RandomForestClassifier(labelCol=&quot;indexedLabel&quot;, featuresCol=&quot;indexedFeatures&quot;, numTrees=10) Pipeline Architecture 123# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;, labels=labelIndexer.labels) 12# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf,labelConverter]) 12# Train model. This also runs the indexers.model = pipeline.fit(trainingData) Make predictions 1234# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+--------------------+------+--------------+| features| label|predictedLabel|+--------------------+------+--------------+|[4.9,0.42,0.0,2.1...| high| high||[5.0,0.38,0.01,1....|medium| medium||[5.0,0.4,0.5,4.3,...|medium| medium||[5.0,0.42,0.24,2....| high| medium||[5.0,0.74,0.0,1.2...|medium| medium|+--------------------+------+--------------+only showing top 5 rows Evaluation 12345678910from pyspark.ml.evaluation import MulticlassClassificationEvaluator# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %g&quot; % (1.0 - accuracy))rfModel = model.stages[-2]print(rfModel) # summary only 12Test Error = 0.173502RandomForestClassificationModel (uid=rfc_a3395531f1d2) with 10 trees visualization 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as npimport itertoolsdef plot_confusion_matrix(cm, classes, normalize=False, title=&apos;Confusion matrix&apos;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&apos;float&apos;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&apos;Confusion matrix, without normalization&apos;) print(cm) plt.imshow(cm, interpolation=&apos;nearest&apos;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &apos;.2f&apos; if normalize else &apos;d&apos; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&apos;True label&apos;) plt.xlabel(&apos;Predicted label&apos;) 123456class_temp = predictions.select(&quot;label&quot;).groupBy(&quot;label&quot;)\ .count().sort(&apos;count&apos;, ascending=False).toPandas()class_temp = class_temp[&quot;label&quot;].values.tolist()class_names = map(str, class_temp)# # # print(class_name)class_names 1[&apos;medium&apos;, &apos;high&apos;, &apos;low&apos;] 123456789from sklearn.metrics import confusion_matrixy_true = predictions.select(&quot;label&quot;)y_true = y_true.toPandas()y_pred = predictions.select(&quot;predictedLabel&quot;)y_pred = y_pred.toPandas()cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_names)cnf_matrix 123array([[502, 9, 0], [ 73, 22, 0], [ 28, 0, 0]]) 12345# Plot non-normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, title=&apos;Confusion matrix, without normalization&apos;)plt.show() 1234Confusion matrix, without normalization[[502 9 0] [ 73 22 0] [ 28 0 0]] 123456# Plot normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title=&apos;Normalized confusion matrix&apos;)plt.show() 1234Normalized confusion matrix[[ 0.98238748 0.01761252 0\. ] [ 0.76842105 0.23157895 0\. ] [ 1\. 0\. 0\. ]] 10.5. Gradient-boosted tree Classification10.5.1. Introduction 10.5.2. Demo The Jupyter notebook can be download from Gradient boosted tree Classification. For more details, please visit GBTClassifier API . Warning Unfortunately, the GBTClassifier currently only supports binary labels. 10.6. XGBoost: Gradient-boosted tree Classification10.6.1. Introduction10.6.2. Demo The Jupyter notebook can be download from Gradient boosted tree Classification. For more details, please visit GBTClassifier API . Warning Unfortunately, I didn’t find a good way to setup the XGBoost directly in Spark. But I do get the XGBoost work with pysparkling on my machine. Start H2O cluster inside the Spark environment 12from pysparkling import *hc = H2OContext.getOrCreate(spark) 12345678910111213141516171819202122232425262728Connecting to H2O server at http://192.168.0.102:54323... successful.H2O cluster uptime: 07 secsH2O cluster timezone: America/ChicagoH2O data parsing timezone: UTCH2O cluster version: 3.22.1.3H2O cluster version age: 20 daysH2O cluster name: sparkling-water-dt216661_local-1550259209801H2O cluster total nodes: 1H2O cluster free memory: 848 MbH2O cluster total cores: 8H2O cluster allowed cores: 8H2O cluster status: accepting new members, healthyH2O connection url: http://192.168.0.102:54323H2O connection proxy: NoneH2O internal security: FalseH2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4Python version: 3.7.1 finalSparkling Water Context: * H2O name: sparkling-water-dt216661_local-1550259209801 * cluster size: 1 * list of used nodes: (executorId, host, port) ------------------------ (driver,192.168.0.102,54323) ------------------------ Open H2O Flow in browser: http://192.168.0.102:54323 (CMD + click in Mac OSX) Parse the data using H2O and convert them to Spark Frame 123import h2oframe = h2o.import_file(&quot;https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/prostate/prostate.csv&quot;)spark_frame = hc.as_spark_frame(frame) 1spark_frame.show(4) 123456789+---+-------+---+----+-----+-----+----+----+-------+| ID|CAPSULE|AGE|RACE|DPROS|DCAPS| PSA| VOL|GLEASON|+---+-------+---+----+-----+-----+----+----+-------+| 1| 0| 65| 1| 2| 1| 1.4| 0.0| 6|| 2| 0| 72| 1| 3| 2| 6.7| 0.0| 7|| 3| 0| 70| 1| 1| 2| 4.9| 0.0| 6|| 4| 0| 76| 2| 2| 1|51.2|20.0| 7|+---+-------+---+----+-----+-----+----+----+-------+only showing top 4 rows Train the model 123from pysparkling.ml import H2OXGBoostestimator = H2OXGBoost(predictionCol=&quot;AGE&quot;)model = estimator.fit(spark_frame) Run Predictions 12predictions = model.transform(spark_frame)predictions.show(4) 123456789+---+-------+---+----+-----+-----+----+----+-------+-------------------+| ID|CAPSULE|AGE|RACE|DPROS|DCAPS| PSA| VOL|GLEASON| prediction_output|+---+-------+---+----+-----+-----+----+----+-------+-------------------+| 1| 0| 65| 1| 2| 1| 1.4| 0.0| 6|[64.85852813720703]|| 2| 0| 72| 1| 3| 2| 6.7| 0.0| 7| [72.0611801147461]|| 3| 0| 70| 1| 1| 2| 4.9| 0.0| 6|[70.26496887207031]|| 4| 0| 76| 2| 2| 1|51.2|20.0| 7|[75.26521301269531]|+---+-------+---+----+-----+-----+----+----+-------+-------------------+only showing top 4 rows 10.7. Naive Bayes Classification10.7.1. Introduction10.7.2. Demo The Jupyter notebook can be download from Naive Bayes Classification. For more details, please visit NaiveBayes API . Set up spark context and SparkSession 1234567from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark Naive Bayes classification&quot;) \ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \ .getOrCreate() Load dataset 1234df = spark.read.format(&apos;com.databricks.spark.csv&apos;) \ .options(header=&apos;true&apos;, inferschema=&apos;true&apos;) \ .load(&quot;./data/WineData2.csv&quot;,header=True);df.show(5) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| 5|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| 5|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| 6|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| 5|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 1df.printSchema() 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) 1234567891011121314151617181920# Convert to float formatdef string_to_float(x): return float(x)#def condition(r): if (0&lt;= r &lt;= 6): label = &quot;low&quot; else: label = &quot;high&quot; return labelfrom pyspark.sql.functions import udffrom pyspark.sql.types import StringType, DoubleTypestring_to_float_udf = udf(string_to_float, DoubleType())quality_udf = udf(lambda x: condition(x), StringType())df = df.withColumn(&quot;quality&quot;, quality_udf(&quot;quality&quot;))df.show(5,True) 12345678910+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+|fixed|volatile|citric|sugar|chlorides|free|total|density| pH|sulphates|alcohol|quality|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|| 7.8| 0.88| 0.0| 2.6| 0.098|25.0| 67.0| 0.9968| 3.2| 0.68| 9.8| medium|| 7.8| 0.76| 0.04| 2.3| 0.092|15.0| 54.0| 0.997|3.26| 0.65| 9.8| medium|| 11.2| 0.28| 0.56| 1.9| 0.075|17.0| 60.0| 0.998|3.16| 0.58| 9.8| medium|| 7.4| 0.7| 0.0| 1.9| 0.076|11.0| 34.0| 0.9978|3.51| 0.56| 9.4| medium|+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+only showing top 5 rows 1df.printSchema() 12345678910111213root |-- fixed: double (nullable = true) |-- volatile: double (nullable = true) |-- citric: double (nullable = true) |-- sugar: double (nullable = true) |-- chlorides: double (nullable = true) |-- free: double (nullable = true) |-- total: double (nullable = true) |-- density: double (nullable = true) |-- pH: double (nullable = true) |-- sulphates: double (nullable = true) |-- alcohol: double (nullable = true) |-- quality: string (nullable = true) Deal with categorical data and Convert the data to dense vector Note You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset. Supervised learning version: 123456789101112131415161718192021222324252627&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):&gt; &gt; from pyspark.ml import Pipeline&gt; from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler&gt; from pyspark.sql.functions import col&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; data = data.withColumn(&apos;label&apos;,col(labelCol))&gt; &gt; return data.select(indexCol,&apos;features&apos;,&apos;label&apos;)&gt; &gt; Unsupervised learning version: 12345678910111213141516171819202122232425262728293031&gt; def get_dummy(df,indexCol,categoricalCols,continuousCols):&gt; &apos;&apos;&apos;&gt; Get dummy variables and concat with continuous variables for unsupervised learning.&gt; :param df: the dataframe&gt; :param categoricalCols: the name list of the categorical data&gt; :param continuousCols: the name list of the numerical data&gt; :return k: feature matrix&gt; &gt; :author: Wenqiang Feng&gt; :email: von198@gmail.com&gt; &apos;&apos;&apos;&gt; &gt; indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c))&gt; for c in categoricalCols ]&gt; &gt; # default setting: dropLast=True&gt; encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),&gt; outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol()))&gt; for indexer in indexers ]&gt; &gt; assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]&gt; + continuousCols, outputCol=&quot;features&quot;)&gt; &gt; pipeline = Pipeline(stages=indexers + encoders + [assembler])&gt; &gt; model=pipeline.fit(df)&gt; data = model.transform(df)&gt; &gt; return data.select(indexCol,&apos;features&apos;)&gt; &gt; 12345678910111213141516171819202122232425def get_dummy(df,categoricalCols,continuousCols,labelCol): from pyspark.ml import Pipeline from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler from pyspark.sql.functions import col indexers = [ StringIndexer(inputCol=c, outputCol=&quot;&#123;0&#125;_indexed&quot;.format(c)) for c in categoricalCols ] # default setting: dropLast=True encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=&quot;&#123;0&#125;_encoded&quot;.format(indexer.getOutputCol())) for indexer in indexers ] assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=&quot;features&quot;) pipeline = Pipeline(stages=indexers + encoders + [assembler]) model=pipeline.fit(df) data = model.transform(df) data = data.withColumn(&apos;label&apos;,col(labelCol)) return data.select(&apos;features&apos;,&apos;label&apos;) Transform the dataset to DataFrame 12345678from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectorsfrom pyspark.ml import Pipelinefrom pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexerfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatordef transData(data):return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF([&apos;features&apos;,&apos;label&apos;]) 12transformed = transData(df)transformed.show(5) 12345678910+--------------------+-----+| features|label|+--------------------+-----+|[7.4,0.7,0.0,1.9,...| low||[7.8,0.88,0.0,2.6...| low||[7.8,0.76,0.04,2....| low||[11.2,0.28,0.56,1...| low||[7.4,0.7,0.0,1.9,...| low|+--------------------+-----+only showing top 5 rows Deal with Categorical Label and Variables 1234# Index labels, adding metadata to the label columnlabelIndexer = StringIndexer(inputCol=&apos;label&apos;, outputCol=&apos;indexedLabel&apos;).fit(transformed)labelIndexer.transform(transformed).show(5, True) 12345678910+--------------------+-----+------------+| features|label|indexedLabel|+--------------------+-----+------------+|[7.4,0.7,0.0,1.9,...| low| 0.0||[7.8,0.88,0.0,2.6...| low| 0.0||[7.8,0.76,0.04,2....| low| 0.0||[11.2,0.28,0.56,1...| low| 0.0||[7.4,0.7,0.0,1.9,...| low| 0.0|+--------------------+-----+------------+only showing top 5 rows 123456# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =VectorIndexer(inputCol=&quot;features&quot;, \ outputCol=&quot;indexedFeatures&quot;, \ maxCategories=4).fit(transformed)featureIndexer.transform(transformed).show(5, True) 12345678910+--------------------+-----+--------------------+| features|label| indexedFeatures|+--------------------+-----+--------------------+|[7.4,0.7,0.0,1.9,...| low|[7.4,0.7,0.0,1.9,...||[7.8,0.88,0.0,2.6...| low|[7.8,0.88,0.0,2.6...||[7.8,0.76,0.04,2....| low|[7.8,0.76,0.04,2....||[11.2,0.28,0.56,1...| low|[11.2,0.28,0.56,1...||[7.4,0.7,0.0,1.9,...| low|[7.4,0.7,0.0,1.9,...|+--------------------+-----+--------------------+only showing top 5 rows Split the data to training and test data sets 12345# Split the data into training and test sets (40% held out for testing)(trainingData, testData) = data.randomSplit([0.6, 0.4])trainingData.show(5,False)testData.show(5,False) 123456789101112131415161718192021+---------------------------------------------------------+-----+|features |label|+---------------------------------------------------------+-----+|[5.0,0.38,0.01,1.6,0.048,26.0,60.0,0.99084,3.7,0.75,14.0]|low ||[5.0,0.42,0.24,2.0,0.06,19.0,50.0,0.9917,3.72,0.74,14.0] |high ||[5.0,0.74,0.0,1.2,0.041,16.0,46.0,0.99258,4.01,0.59,12.5]|low ||[5.0,1.02,0.04,1.4,0.045,41.0,85.0,0.9938,3.75,0.48,10.5]|low ||[5.0,1.04,0.24,1.6,0.05,32.0,96.0,0.9934,3.74,0.62,11.5] |low |+---------------------------------------------------------+-----+only showing top 5 rows+---------------------------------------------------------+-----+|features |label|+---------------------------------------------------------+-----+|[4.6,0.52,0.15,2.1,0.054,8.0,65.0,0.9934,3.9,0.56,13.1] |low ||[4.7,0.6,0.17,2.3,0.058,17.0,106.0,0.9932,3.85,0.6,12.9] |low ||[4.9,0.42,0.0,2.1,0.048,16.0,42.0,0.99154,3.71,0.74,14.0]|high ||[5.0,0.4,0.5,4.3,0.046,29.0,80.0,0.9902,3.49,0.66,13.6] |low ||[5.2,0.49,0.26,2.3,0.09,23.0,74.0,0.9953,3.71,0.62,12.2] |low |+---------------------------------------------------------+-----+only showing top 5 rows Fit Naive Bayes Classification Model 12from pyspark.ml.classification import NaiveBayesnb = NaiveBayes(featuresCol=&apos;indexedFeatures&apos;, labelCol=&apos;indexedLabel&apos;) Pipeline Architecture 123# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol=&quot;prediction&quot;, outputCol=&quot;predictedLabel&quot;, labels=labelIndexer.labels) 12# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, nb,labelConverter]) 12# Train model. This also runs the indexers.model = pipeline.fit(trainingData) Make predictions 1234# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select(&quot;features&quot;,&quot;label&quot;,&quot;predictedLabel&quot;).show(5) 12345678910+--------------------+-----+--------------+| features|label|predictedLabel|+--------------------+-----+--------------+|[4.6,0.52,0.15,2....| low| low||[4.7,0.6,0.17,2.3...| low| low||[4.9,0.42,0.0,2.1...| high| low||[5.0,0.4,0.5,4.3,...| low| low||[5.2,0.49,0.26,2....| low| low|+--------------------+-----+--------------+only showing top 5 rows Evaluation 1234567from pyspark.ml.evaluation import MulticlassClassificationEvaluator# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %g&quot; % (1.0 - accuracy)) 1Test Error = 0.307339 12345678910111213141516171819lrModel = model.stages[2]trainingSummary = lrModel.summary# Obtain the objective per iteration# objectiveHistory = trainingSummary.objectiveHistory# print(&quot;objectiveHistory:&quot;)# for objective in objectiveHistory:# print(objective)# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.trainingSummary.roc.show(5)print(&quot;areaUnderROC: &quot; + str(trainingSummary.areaUnderROC))# Set the model threshold to maximize F-MeasurefMeasure = trainingSummary.fMeasureByThresholdmaxFMeasure = fMeasure.groupBy().max(&apos;F-Measure&apos;).select(&apos;max(F-Measure)&apos;).head(5)# bestThreshold = fMeasure.where(fMeasure[&apos;F-Measure&apos;] == maxFMeasure[&apos;max(F-Measure)&apos;]) \# .select(&apos;threshold&apos;).head()[&apos;threshold&apos;]# lr.setThreshold(bestThreshold) You can use z.show() to get the data and plot the ROC curves: You can also register a TempTable data.registerTempTable(&#39;roc_data&#39;) and then use sql to plot the ROC curve: visualization 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltimport numpy as npimport itertoolsdef plot_confusion_matrix(cm, classes, normalize=False, title=&apos;Confusion matrix&apos;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&apos;float&apos;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&apos;Confusion matrix, without normalization&apos;) print(cm) plt.imshow(cm, interpolation=&apos;nearest&apos;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &apos;.2f&apos; if normalize else &apos;d&apos; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&apos;True label&apos;) plt.xlabel(&apos;Predicted label&apos;) 123456class_temp = predictions.select(&quot;label&quot;).groupBy(&quot;label&quot;)\ .count().sort(&apos;count&apos;, ascending=False).toPandas()class_temp = class_temp[&quot;label&quot;].values.tolist()class_names = map(str, class_temp)# # # print(class_name)class_names 1[&apos;low&apos;, &apos;high&apos;] 123456789from sklearn.metrics import confusion_matrixy_true = predictions.select(&quot;label&quot;)y_true = y_true.toPandas()y_pred = predictions.select(&quot;predictedLabel&quot;)y_pred = y_pred.toPandas()cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_names)cnf_matrix 12array([[392, 169], [ 32, 61]]) 12345# Plot non-normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, title=&apos;Confusion matrix, without normalization&apos;)plt.show() 123Confusion matrix, without normalization[[392 169] [ 32 61]] 123456# Plot normalized confusion matrixplt.figure()plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title=&apos;Normalized confusion matrix&apos;)plt.show() 123Normalized confusion matrix[[0.69875223 0.30124777] [0.34408602 0.65591398]]]]></content>
      <tags>
        <tag>PySpark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池-饿了么智慧物流]]></title>
    <url>%2F2020%2F04%2F11%2F%E5%A4%A9%E6%B1%A0-%E9%A5%BF%E4%BA%86%E4%B9%88%2F</url>
    <content type="text"><![CDATA[top 5% 1.feature 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333#!/usr/bin/env python# coding: utf-8import osimport timefrom tqdm import tqdmimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn.utils import shufflepd.set_option('display.max_columns', None)import warningswarnings.filterwarnings('ignore')def task1_featuretask(train_path,test_path): print('----------------------文件读取----------------------') # 读取数据并加入date列 def read_datafile(rootpath, selct): file_path = rootpath + selct + '/' data_list = [] for f in os.listdir(file_path): date = f.split('.')[0].split('_')[1] if selct == 'action': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str&#125;) elif selct == 'order': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str&#125;) elif selct == 'courier': df = pd.read_csv(file_path + f) elif selct == 'distance': df = pd.read_csv(file_path + f, converters=&#123;'tracking_id': str, 'target_tracking_id': str&#125;) df['date'] = date data_list.append(df) return pd.concat(data_list) def majorid(df): df['majorid'] = df['date'].map(str) + df['courier_id'].map(str) + '_' + df['wave_index'].map(str) return df def dropdate(df): df.drop(['date', 'courier_id', 'wave_index'], axis=1, inplace=True) return df print('----------------------TASK1~特征工程----------------------') print('----------------------action的操作(1/4)----------------------') ## action的操作 # courier_id 骑士id # wave_index 波次index # tracking_id 订单id # courier_wave_start_lng 波次起始时刻骑士位置 # courier_wave_start_lat 波次起始时刻骑士位置 # action_type 行为类型 # expect_time 行为对应时刻 action_train = read_datafile(train_path, 'action') action_test = read_datafile(test_path, 'action') action_train = majorid(action_train) action_test = majorid(action_test) def action_train_group(df): groups = df.groupby(['majorid']) df_future = [] df_last = [] for name, group in tqdm(groups): # future_data代表后面55%个 future_data = group.tail(int(group.shape[0] * 0.55)) last_data = group.drop(future_data.index) # last操作 last_data = last_data.tail(1) # 此时last_data 为最后一组数 last_data.reset_index(drop=True, inplace=True) # 对future处理:把第一个样本标记为正样本 # (一般是把(二分类)任务中要查找(识别)出来的类别称做正类) future_data['label'] = 0 future_data.reset_index(drop=True, inplace=True) future_data.loc[0, 'label'] = 1 # 标记正负样本 df_future.append(future_data) df_last.append(last_data) return_last = pd.concat(df_last) return_future = pd.concat(df_future) return_last.rename(&#123;'expect_time': 'last_time'&#125;, axis=1, inplace=True) # 把expecttime列重命名 return_future = shuffle(return_future) # 随机打乱顺序 return return_last, return_future def action_test_group(df): groups = df.groupby(['majorid']) df_future = [] df_last = [] for name, group in tqdm(groups): future_data = group[group['expect_time'] == 0] last_data = group.drop(future_data.index) # last操作 last_data = last_data.tail(1) last_data.reset_index(drop=True, inplace=True) # future操作 future_data['label'] = None df_future.append(future_data) df_last.append(last_data) return_last = pd.concat(df_last) return_future = pd.concat(df_future) return_last.rename(&#123;'expect_time': 'last_time'&#125;, axis=1, inplace=True) return return_last, return_future action_train_last, action_train_future = action_train_group(action_train) action_test_last, action_test_future = action_test_group(action_test) print('----------------------distance操作(2/4)----------------------') # ## distance操作 # courier_id 骑士id # wave_index 波次id # tracking_id 源订单id # source_type 源点类型（Assign/PickFood/DeliverFood） # source_lng 源点经度 # source_lat 源点纬度 # target_tracking_id 目标订单id # target_type 目标点类型（Assign/PickFood/DeliverFood） # target_lng 目标点经度 # target_lat 目标点纬度 # grid_distance 源点与目标点的高德距离 distance_train = read_datafile(train_path, 'distance') distance_test = read_datafile(test_path, 'distance') distance_train = majorid(distance_train) distance_test = majorid(distance_test) distance_train = dropdate(distance_train) distance_test = dropdate(distance_test) # 计算曼哈顿距离 def tanlism_distance(df): df['target_tan'] = (df['source_lat'] - df['target_lat']) / ( df['source_lng'] - df['target_lng']) # df会自动处理出正无穷和负无穷，很秀 df['target_tan'] = np.arctan(df['target_tan']) df['target_tan'] = np.degrees(df['target_tan']) df['target_MHD'] = abs(df['source_lat'] - df['target_lat']) + abs( df['source_lng'] - df['target_lng']) # 加入曼哈顿距离 return df distance_train = tanlism_distance(distance_train) distance_test = tanlism_distance(distance_test) rename_rule = &#123;'source_type': 'action_type'&#125; distance_test.rename(rename_rule, axis=1, inplace=True) distance_train.rename(rename_rule, axis=1, inplace=True) feature_train = pd.merge(left=action_train_last, right=distance_train, on=['majorid', 'tracking_id', 'action_type'], how='left') feature_test = pd.merge(left=action_test_last, right=distance_test, on=['majorid', 'tracking_id', 'action_type'], how='left') rename_rule = &#123;'tracking_id': 'last_tracking_id', 'action_type': 'last_action_type', 'target_tracking_id': 'tracking_id', 'target_type': 'action_type'&#125; feature_test.rename(rename_rule, axis=1, inplace=True) feature_train.rename(rename_rule, axis=1, inplace=True) feature_test.drop(['courier_wave_start_lng', 'courier_wave_start_lat'], axis=1, inplace=True) feature_train.drop(['courier_wave_start_lng', 'courier_wave_start_lat'], axis=1, inplace=True) feature_train = dropdate(feature_train) feature_test = dropdate(feature_test) feature_train = pd.merge(left=action_train_future, right=feature_train, on=['majorid', 'tracking_id', 'action_type'], how='left') feature_test = pd.merge(left=action_test_future, right=feature_test, on=['majorid', 'tracking_id', 'action_type'], how='left') del action_test, action_test_future, action_test_last del action_train, action_train_future, action_train_last del distance_test, distance_train print('----------------------order操作(3/4)----------------------') # ## order操作 # courier_id 骑士id # wave_index 波次id # tracking_id 订单id # weather_grade 天气状况(正常天气/轻微恶劣天气/恶劣天气/极恶劣天气/罕见恶劣天气) # pick_lng 取餐经度 # pick_lat 取餐纬度 # deliver_lng 送餐经度 # deliver_lat 送餐纬度 # create_time 订单创建时间 # confirm_time 订单确认时间 # assigned_time 订单分配时间 # time 时间戳 # promise_deliver_time 承诺送达时间 # estimate_pick_time 预计取餐时间 # aoi_id 送餐点所在aoi id，aoi_id是可以唯一标识一个小区，写字楼园区，学校，医院等 # shop_id 商户id，可以唯一标识一个商户 order_train = read_datafile(train_path, 'order') order_test = read_datafile(test_path, 'order') order_test = majorid(order_test) order_train = majorid(order_train) order_test = dropdate(order_test) order_train = dropdate(order_train) def tanlism_order(df): df['delivery_tan'] = (df['deliver_lat'] - df['pick_lat']) / (df['deliver_lng'] - df['pick_lng']) df['delivery_tan'] = np.arctan(df['delivery_tan']) df['delivery_tan'] = np.degrees(df['delivery_tan']) df['delivery_MHD'] = abs(df['deliver_lat'] - df['pick_lat']) + abs( df['deliver_lng'] - df['pick_lng']) # 加入曼哈顿距离 df.drop(['deliver_lat', 'pick_lat', 'deliver_lng', 'pick_lng'], axis=1, inplace=True) return df order_test = tanlism_order(order_test) order_train = tanlism_order(order_train) feature_test = pd.merge(left=feature_test, right=order_test, on=['majorid', 'tracking_id'], how='left') feature_train = pd.merge(left=feature_train, right=order_train, on=['majorid', 'tracking_id'], how='left') print('----------------------courier操作(4/4)----------------------') # ## courier操作 # courier_id 骑士id # level 新老骑士 # speed 骑士速度 # max_load 背单能力 courier_train = read_datafile(train_path, 'courier') courier_test = read_datafile(test_path, 'courier') feature_test = pd.merge(left=feature_test, right=courier_test, on=['courier_id', 'date'], how='left') feature_train = pd.merge(left=feature_train, right=courier_train, on=['courier_id', 'date'], how='left') # 加入新的特征：id, rush和road feature_train['id'] = range(feature_train.shape[0]) feature_test['id'] = range(feature_test.shape[0]) def add_rush(df): df['rush'] = (df['last_time'] - df['create_time']) / (df['promise_deliver_time'] - df['create_time']) return df feature_train = add_rush(feature_train) feature_test = add_rush(feature_test) a = time.strftime('%a_%H', time.localtime(1582094246)) # 根据日期转化 分为 工作日/周末 高峰/非高峰 def add_road(df): df['now'] = df['last_time'].apply(lambda x: time.strftime('%a_%H', time.localtime(x))) # 格式化时间戳为本地的时间 df['is_holiday'] = df['now'].apply(lambda x: 1 if x.split('_')[0] in ['Sat', 'Sun'] else 0) busytime = ['7', '8', '11', '12', '17', '18'] normtime = ['5', '6', '9', '10', '13', '14', '15', '16', '19', '20', '21', '22'] df['road'] = df['now'].apply( lambda x: 1 if x.split('_')[1] in busytime else 2 if x.split('_')[1] in normtime else 3) df.drop(['now'], axis=1, inplace=True) return df feature_test = add_road(feature_test) feature_train = add_road(feature_train) # weather_grade转化为特征 def weather(x): if x == '正常天气': x = 4 elif x == '轻微恶劣天气': x = 3 elif x == '恶劣天气': x = 2 elif x == '极恶劣天气': x = 1 else: x = 0 return x feature_train['weather_grade'] = feature_train['weather_grade'].apply(lambda x: weather(x)) feature_test['weather_grade'] = feature_test['weather_grade'].apply(lambda x: weather(x)) # 加入expect_time-create_time和promise_deliver_time-expect_time作为预测的目标值 # 一个时间是 从 下订单 到 这一action 之间的时间--&gt;已花费时间 # 另一方 是 从 这一action 到 承诺送达的时间--&gt;剩余时间 # expect_time就相当于当前时间 feature_train['expect_used_time'] = feature_train['expect_time'] - feature_train['create_time'] feature_train['will_residue_time'] = feature_train['promise_deliver_time'] - feature_train['expect_time'] feature_test['expect_used_time'] = 0 feature_test['will_residue_time'] = 0 # feature_train.info() print('----------------------异常值处理----------------------') # # 异常值处理 # 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数 def deal_outliers(df, col): # df = feature_train # col = 'expect_used_time' def Box_outliers(data_ser): iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) val_low = data_ser.quantile(0.25) - iqr val_up = data_ser.quantile(0.75) + iqr return val_low, val_up data_ser = df[col] val_low, val_up = Box_outliers(df[col]) std_data = df[col].quantile(0.5) df[col] = df[col].apply(lambda x: std_data if x &lt; val_low else std_data if x &gt; val_up else x) sns.boxplot(y=df[col], data=df, palette="Set1") return df[col] # 下边是实际需要运行的 feature_train['expect_used_time'] = deal_outliers(feature_train, 'expect_used_time') feature_train['will_residue_time'] = deal_outliers(feature_train, 'will_residue_time') feature_train['expect_used_time'].describe() feature_test.to_pickle('../user_data/feature_test_reg.pkl') feature_train.to_pickle('../user_data/feature_train_reg.pkl') print('******************特征工程完毕******************')if __name__ == '__main__': train_path = '../data/eleme_round1_train_20200313/' test_path = '../data/eleme_round1_testB_20200413/' task1_featuretask(train_path,test_path) 2.model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478#!/usr/bin/env python# coding: utf-8import osimport timeimport numpy as npimport pandas as pdimport seaborn as snsfrom sklearn import metricsfrom sklearn.externals import joblibfrom sklearn.preprocessing import LabelEncoderfrom sklearn.model_selection import GroupKFold, KFoldimport gcimport lightgbm as lgbpd.set_option('display.max_columns', None)import warningswarnings.filterwarnings('ignore')def task2_regressortask(): print('----------------------TASK2~回归问题12----------------------') feature_test = pd.read_pickle('../user_data/feature_test_reg.pkl') feature_train = pd.read_pickle('../user_data/feature_train_reg.pkl') reg_prediction = feature_test reg_prediction['expect_time'] = 0 # 注意这边的 'expect_time' # 为什么是0，是因为expect_time 早已变成last_time reg_prediction['expect_used_time'] = 0 reg_prediction['will_residue_time'] = 0 print('----------------------回归问题1预测expect_used_time----------------------') # # 建立时间预测的回归任务1 # print('----------------------回归1调参----------------------') # y_col = 'expect_used_time' # # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123; # 'n_estimators': [10000], # 'learning_rate': [0.05], # 'min_child_weight': [0], # 'max_delta_step': [1], # 'colsample_bytree': [0.5,0.8], # 'reg_alpha': [0.8,1,1.2], # 'reg_lambda': [0.8,1,1.2], # 'scale_pos_weight': [0.8,1,1.2], # 'num_leaves': [340,350,400], # 'max_depth': [90,100,130], # 'subsample': [ 0.1,0.12,0.14], # 'min_child_samples': [1,2,4,3], # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='mae', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMRegressor(objective = 'regression_l1',metric = 'mae') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) # 建立时间预测的回归任务1 y_col = 'expect_used_time' # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] t0 = time.time() model = lgb.LGBMRegressor(objective='regression_l1', metric='mae', subsample=0.14, scale_pos_weight=1, reg_lambda=1.2, reg_alpha=1, num_leaves=400, n_estimators=10000, min_child_weight=0, min_child_samples=2, max_depth=130, max_delta_step=1, learning_rate=0.05, colsample_bytree=0.5 ) valueK = 10 oof = [] df_importance_list = [] kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020) for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold&#123;&#125; Training ======================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='mae', early_stopping_rounds=100 ) pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_) df_oof = feature_train.iloc[val_idx][['id', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_) reg_prediction['expect_used_time'] += (pred_test / valueK) df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_ &#125;) df_importance_list.append(df_importance) # break joblib.dump(lgb_model, '../user_data/model_data/model1&#123;&#125;.dat'.format(fold_id)) # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val gc.collect() t1 = time.time() print('end train, use&#123;&#125; second'.format(t1 - t0)) df_oof = pd.concat(oof) mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred']) print('mae:', mae) print('----------------------回归问题2预测will_residue_time----------------------') # # 建立时间预测的回归任务2 # print('----------------------回归2调参----------------------') # y_col = 'will_residue_time' # # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', 'target_MHD'] # # x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD'] # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123;'n_estimators':[10000], # 'num_leaves':[300], # 'max_depth':[10,20], # 'learning_rate': [0.05], # # 'bagging_fraction': [0.5,0.8,1], # # 'feature_fraction': [0.8], # 'reg_alpha' : [0,0.2,0.5], # 'reg_lambda' : [0,0.2,0.5] # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='mae', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMRegressor(objective = 'regression_l1',metric = 'mae') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) # 建立时间预测的回归任务2 y_col = 'will_residue_time' # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'target_tan', 'delivery_tan', 'delivery_MHD', 'target_MHD']# 加入expect_used_time和will_residue_time之后的x_col # x_col = ['weather_grade', 'level', 'speed', 'max_load', 'is_holiday', 'rush', 'road', 'grid_distance', 'delivery_MHD', # 'target_MHD'] x_col = ['speed', 'rush', 'grid_distance', 'delivery_MHD', 'target_MHD'] t0 = time.time() model = lgb.LGBMRegressor( metric='mae', num_leaves=64, max_depth=7, n_estimators=300, learning_rate=0.05, bagging_fraction=1, feature_fraction=0.8, reg_alpha=0, reg_lambda=0 ) valueK = 10 oof = [] df_importance_list = [] kfold = KFold(n_splits=valueK, shuffle=True, random_state=2020) for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold&#123;&#125; Training ======================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='mae', early_stopping_rounds=100 ) pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_) df_oof = feature_train.iloc[val_idx][['id', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict(feature_test[x_col], num_iteration=lgb_model.best_iteration_) reg_prediction['will_residue_time'] += (pred_test / valueK) df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_ &#125;) df_importance_list.append(df_importance) # break joblib.dump(lgb_model, '../user_data/model_data/model2&#123;&#125;.dat'.format(fold_id)) # del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val # gc.collect() t1 = time.time() print('end train, use&#123;&#125; second'.format(t1 - t0)) df_oof = pd.concat(oof) mae = metrics.mean_absolute_error(df_oof[y_col], df_oof['pred']) print('mae:', mae) print('----------------------计算expect_time----------------------') # 还原expect_time reg_prediction['expect_time'] = reg_prediction['promise_deliver_time'] - reg_prediction[ 'will_residue_time'] # 只使用后一个 feature_train.to_pickle('../user_data/regfuture_train_regend.pkl') reg_prediction.to_pickle('../user_data/regfuture_test_regend.pkl') print('******************回归问题完毕******************')def task3_classifiertask(): print('----------------------TASK3~分类问题----------------------') feature_train = pd.read_pickle('../user_data/regfuture_train_regend.pkl') feature_test = pd.read_pickle('../user_data/regfuture_test_regend.pkl') # 特征 deadline--&gt;还剩下的时间 def deadLine(df): df['deadline'] = df['promise_deliver_time'] - df['expect_time'] df['need_speed'] = df['grid_distance'] / df['deadline'] return df feature_train = deadLine(feature_train) feature_test = deadLine(feature_test) # 重新包装了一个异常值处理函数，把异常值清洗设为50%中位数 def deal_outliers(df, col): # df = feature_train # col = 'expect_used_time' def Box_outliers(data_ser): iqr = 3 * (data_ser.quantile(0.75) - data_ser.quantile(0.25)) val_low = data_ser.quantile(0.25) - iqr val_up = data_ser.quantile(0.75) + iqr return val_low, val_up data_ser = df[col] val_low, val_up = Box_outliers(df[col]) std_data = df[col].quantile(0.5) df[col] = df[col].apply(lambda x: std_data if x &lt; val_low else std_data if x &gt; val_up else x) sns.boxplot(y=df[col], data=df, palette="Set1") return df[col] feature_train['need_speed'] = deal_outliers(feature_train, 'need_speed') feature_train['deadline'] = deal_outliers(feature_train, 'deadline') feature_train['deadline'].plot.hist() def is_Picked(df): df['is_picked'] = df['last_time'] - df['estimate_pick_time'] df['is_picked'] = df['is_picked'].apply(lambda x: 0 if x &lt; 0 else 1) return df feature_test = is_Picked(feature_test) feature_train = is_Picked(feature_train) feature_train.to_pickle('../user_data/future_train_clf.pkl') feature_test.to_pickle('../user_data/future_test_clf.pkl') # # 分类模型 # 目标：提高回归任务的mae，以提高expect_time预测精准度。 feature_train = pd.read_pickle('../user_data/future_train_clf.pkl') feature_test = pd.read_pickle('../user_data/future_test_clf.pkl') # 临时特征工程 feature_test['label'] = 0 feature_train['train'] = 1 feature_test['train'] = 0 feature = feature_train.append(feature_test) # 将送餐点和商户的信息 标签编码LabelEncoder for f in ['aoi_id', 'shop_id']: print(f) lbl = LabelEncoder() feature[f] = lbl.fit_transform(feature[f].astype(str)) feature_train = feature[feature['train'] == 1].copy() feature_test = feature[feature['train'] == 0].copy() def total_Residue_Time(df): df['total_residue_time'] = df['promise_deliver_time'] - df['estimate_pick_time'] return df feature_train = total_Residue_Time(feature_train) feature_test = total_Residue_Time(feature_test) # 回到模型 prediction = feature_test[ ['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date', 'id', 'majorid', 'label']] prediction['label'] = 0 # print('----------------------分类调参----------------------') # y_col = 'label' # # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked'] # # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']# 939-911 # x_col = ['grid_distance', 'target_MHD', 'weather_grade', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'deadline', 'need_speed', 'is_picked']# # # x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked']# 783-772-718 # # x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked', 'aoi_id', 'shop_id', 'courier_id']# 797-767-716 # t0 = time.time() # X_train, X_val, Y_train, Y_val = train_test_split(feature_train[x_col], feature_train[y_col], test_size=0.25) # def find_best_param(model): # lgb_param_test = &#123; # 'n_estimators': [10000], # 'learning_rate': [0.1], # 'num_leaves': [10,6,7], # 'max_depth': [300,32,64,128], # 'subsample': [0.5,0.8,1], # 'reg_alpha': [0.8,1,0,0.5], # 'reg_lambda': [0.8,1,0,0.5], # 'feature_fraction': [0.5,0.8,1], # &#125; # # # # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同 # grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='accuracy', cv=3) # # grid_search = GridSearchCV(estimator=model, param_grid=lgb_param_test, scoring='accuracy', cv=5) # grid_search.fit(X_train,Y_train, # eval_names=['train', 'valid'], # eval_set=[(X_train, Y_train), (X_val, Y_val)], # verbose=500, # eval_metric='auc', # early_stopping_rounds=100) # print(grid_search.best_params_) # # # lgbreg = lgb.LGBMClassifier(metric = 'auc') # find_best_param(lgbreg) # gc.collect() # t1 = time.time() # print('end train, use&#123;&#125; second'.format(t1-t0)) y_col = 'label' # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'aoi_id', 'shop_id', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked'] # x_col = ['grid_distance', 'target_tan', 'target_MHD', 'weather_grade', 'delivery_tan', 'delivery_MHD', 'level', 'speed', 'max_load', 'rush', 'road', 'expect_used_time', 'will_residue_time', 'deadline', 'need_speed', 'is_picked']# 939-911 # x_col = ['grid_distance', 'target_MHD', 'weather_grade', 'delivery_MHD', 'level', 'speed', 'max_load', 'r# 783-772-718 x_col = ['rush', 'target_MHD', 'delivery_MHD', 'grid_distance', 'max_load', 'is_picked', 'aoi_id', 'shop_id', 'courier_id'] model = lgb.LGBMClassifier(metric='auc', num_leaves=7, max_depth=128, learning_rate=0.05, n_estimators=10000, subsample=0.8, feature_fraction=0.8, reg_alpha=1, reg_lambda=0.5, ) oof = [] df_importance_list = [] kfold = GroupKFold(n_splits=10) for fold_id, (trn_idx, val_idx) in enumerate( kfold.split(feature_train[x_col], feature_train[y_col], feature_train['majorid'])): X_train = feature_train.iloc[trn_idx][x_col] Y_train = feature_train.iloc[trn_idx][y_col] X_val = feature_train.iloc[val_idx][x_col] Y_val = feature_train.iloc[val_idx][y_col] print('\nFold_&#123;&#125; Training ================================\n'.format(fold_id + 1)) lgb_model = model.fit( X_train, Y_train, eval_names=['train', 'valid'], eval_set=[(X_train, Y_train), (X_val, Y_val)], verbose=500, eval_metric='auc', early_stopping_rounds=100 ) pred_val = lgb_model.predict_proba(X_val, num_iteration=lgb_model.best_iteration_)[:, 1] df_oof = feature_train.iloc[val_idx][['id', 'majorid', y_col]].copy() df_oof['pred'] = pred_val oof.append(df_oof) pred_test = lgb_model.predict_proba(feature_test[x_col], num_iteration=lgb_model.best_iteration_)[:, 1] prediction['label'] += pred_test / 10 df_importance = pd.DataFrame(&#123; 'column': x_col, 'importance': lgb_model.feature_importances_, &#125;) df_importance_list.append(df_importance) joblib.dump(lgb_model, '../user_data/model_data/model3&#123;&#125;.dat'.format(fold_id)) # 验证recall，对feature_train 和 oof def result_func(majorid): majorid = majorid.values.tolist() max_index = majorid.index(max(majorid)) result = np.zeros(len(majorid)) result[max_index] = 1 return result def Acc(df): TP = df[(df['label'] == 1) &amp; (df['result'] == 1)].shape[0] # 预测正确，预测的值为真 TN = df[(df['label'] == 0) &amp; (df['result'] == 0)].shape[0] # 预测正确，预测的值为假 FP = df[(df['label'] == 0) &amp; (df['result'] == 1)].shape[0] # 预测错误，预测的值为真 FN = df[(df['label'] == 1) &amp; (df['result'] == 0)].shape[0] # 预测错误，预测的值为假 TP_FN = df[df['label'] == 1].shape[0] TP_FP = df[df['result'] == 1].shape[0] print('recall:&#123;&#125;'.format(TP / TP_FN)) print('acc:&#123;&#125;'.format(TP / TP_FP)) feature_train['pred_val'] = lgb_model.predict_proba(feature_train[x_col], num_iteration=lgb_model.best_iteration_)[ :, 1] feature_train['result'] = feature_train.groupby(['majorid'])['pred_val'].transform(result_func) Acc(feature_train) print('******************分类问题完毕******************') print('----------------------zip输出----------------------') # 输出 prediction['rusult'] = prediction.groupby(['majorid'])['label'].transform(result_func) subfile = prediction[prediction['rusult'] == 1] result = subfile[ ['courier_id', 'wave_index', 'tracking_id', 'courier_wave_start_lng', 'courier_wave_start_lat', 'action_type', 'expect_time', 'date']] import zipfile os.makedirs('../action_predict', exist_ok=True) # f = zipfile.ZipFile('./action_predict/&#123;&#125;.zip'.format('result'), 'w', zipfile.ZIP_DEFLATED) for date in result['date'].unique(): df_temp = result[result['date'] == date] del df_temp['date'] df_temp.to_csv('../action_predict/action_&#123;&#125;.txt'.format(date), index=False) # f.write('./action_predict/action_&#123;&#125;.txt'.format(date), 'action_&#123;&#125;.txt'.format(date)) # f.close()if __name__ == '__main__': task2_regressortask() task3_classifiertask() 3.main 12345678from feature.feature import task1_featuretaskfrom model.model import task2_regressortask,task3_classifiertasktrain_path = '../data/eleme_round1_train_20200313/'test_path = '../data/eleme_round1_testB_20200413/'task1_featuretask(train_path,test_path)task2_regressortask()task3_classifiertask()]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML，天池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据清洗时的一些指标]]></title>
    <url>%2F2020%2F04%2F01%2F%E5%A4%A9%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[算法常见的评估指标分类算法常见的评估指标如下： 对于二类分类器/分类算法，评价指标主要有accuracy， [Precision，Recall，F-score，Pr曲线]，ROC-AUC曲线。 对于多类分类器/分类算法，评价指标主要有accuracy， [宏平均和微平均，F-score]。 预测（横） 实际（纵） + - + tp fn - fp tn tp tn 代表预测正确 准确率(Accuracy) accuracy是最常见也是最基本的评价指标。但是，在二分类且正负样本不平衡的情况下，尤其是对于较少数样本类感兴趣时，accuracy基本无参考价值。如欺诈检测、癌症检测等，100个样例中，99个负例，1个正例。模型将任意样本都分为负例，accuracy值为0.99。但是，拿这个模型去检测新样本，一个正例也分不出来。 精确率(Precision)、召回率(Recall) precision是相对于模型预测而言的，可以理解为模型做出新预测时的自信度得分是多少或做出这个预测是对的可能性是多少。自信度。 recall是相对于真实标签而言的，可以理解为模型预测出的正例占实际正例的比例。覆盖率。 如果模型很贪婪，想要覆盖更多的样本，那么它就有可能会犯错。这个时候的recall值常很高，precision常很低。如果模型很保守，只对很确定的样本做出预测，则precision值常很高，recall值常很低。我们可以选择只看我们感兴趣的样本类，也就是较少数样本类的precision和recall来评价模型的好坏。 疾病检测、反垃圾等，是在保证精确率的条件下提升召回率；搜索等是在保证召回率的情况下提升精确率。 F1值(F1-score) F1值是个综合考虑precision值和recall值的指标。 多类别分类时，有宏平均(macro-average)和微平均(micro-average)两种。 宏平均是指先对每个类别单独计算F1值。取这些值的算术平均值作为全局指标。这种方式平等地对待每个类别，所以其值主要受稀有类别的影响，更能体现模型在稀有类别上的表现。 微平均是指先累加各个类别的tp、fp、tn、fn值，再由这些值来计算F1值。这种方式平等地对待每个样本，所以其值主要受到常见类别的影响。 ROC-AUC 无论的真实概率是多少，都不会影响sensitivity和specificity。也就是说，这两个指标是不会受到不平衡数据的影响的。而是会受到数据集中正负比例的影响的。 ROC曲线(Receiver Operating Characteristic Curve)是一个以fpr为轴，tpr为轴，取不同的score threshold画出来的。 基本上，ROC曲线下面积即AUC越大，或者说曲线越接近于左上角(fpr=0, tpr=1)，那么模型的分类效果就越好。一般来说，最优score threshold就是ROC曲线离基准线最远的一点或者说是ROC曲线离左上角最近的一点对应的阈值，再或者是根据用户自定义的cost function来决定的。 AUC就是从所有正例样本中随机选择出一个样本，在所有负例样本中随机选择出一个样本，使用分类器进行预测。将正例样本预测为正的概率记作，将负例样本预测为负的概率记作，的概率就等于AUC值。因此，AUC反映的是分类器对于样本的排序能力。根据这个解释，如果我们完全随机地对样本进行分类，那么AUC应该接近于0.5。另外，AUC值对于样本类别是否均衡并不敏感，这也是不均衡样本通常使用AUC评价分类器性能的一个原因。通常使用AUC的目的，一是为了比较不同模型性能的好坏，二是为了找到得到最佳指标值的那个阈值点。 PR-AUC PR曲线，是以P为轴，以R为轴，取不同的概率阈值得到不同的(p,r)点后画成的线。 为了解决P、R、F-Measure(即)的单点局限性，得到一个能够反映全局的指标，使用PR-AUC/AP。同样地，PR-AUC值越大，或者说曲线越接近右上角(p=1, r=1)，那么模型就越理想、越好。 AAP(Approximated Average Precision) AAP将PR-AUC面积分割成不同的长方形然后求面积和。 IAP(Interpolated Average Precision) 如果存在r’&gt;r且p’&gt;p，使用p’代替p参与面积计算。AAP会比IAP离实际的PR-AUC更近，面积也会小点。 PASCAL VOC中使用IAP作为AP值，认为这一方法能够有效地减少PR曲线中的抖动。然后对于多类别进行AP取平均操作后得mAP值。 算法倾向如果是“宁可错杀一千，不可放过一个”，可以设定在合理的precision值下，最高的recall值作为最优点，找到这个点对应的阈值。总之，我们可以根据具体的应用或者是偏好，在曲线上找到最优的点，去调整模型的阈值，从而得到一个符合具体应用的模型。 对于回归预测类常见的评估指标如下: 平均绝对误差（Mean Absolute Error，MAE），均方误差（Mean Squared Error，MSE），平均绝对百分误差（Mean Absolute Percentage Error，MAPE），均方根误差（Root Mean Squared Error）， R2（R-Square） 平均绝对误差 平均绝对误差（Mean Absolute Error，MAE）:平均绝对误差，其能更好地反映预测值与真实值误差的实际情况，其计算公式如下：$$MAE=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}{i}\right|$$均方误差 均方误差（Mean Squared Error，MSE）,均方误差,其计算公式为：$$MSE=1NN∑i=1(yi−^yi)2MSE=1N∑i=1N(yi−y^i)2$$R2（R-Square）的公式为： 残差平方和：$$SSres=∑(yi−^yi)2SSres=∑(yi−y^i)2$$总平均值:$$SStot=∑(yi−¯¯¯yi)2SStot=∑(yi−y¯i)2$$其中¯¯¯yy¯表示yy的平均值 得到R2R2表达式为：$$R^{2}=1-\frac{SS{res}}{SS_{tot}}=1-\frac{\sum\left(y_{i}-\hat{y}{i}\right)^{2}}{\sum\left(y{i}-\overline{y}\right)^{2}}$$R2用于度量因变量的变异中可由自变量解释部分所占的比例，取值范围是 0~1，R2越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以R2也称为拟合优度（Goodness of Fit）的统计量。 yi表示真实值，^yi表示预测值，¯yii表示样本均值。得分越高拟合效果越好。 偏度和峰度]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池-二手车交易价格预测]]></title>
    <url>%2F2020%2F03%2F30%2F%E5%A4%A9%E6%B1%A0%20-%20%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[作为第一次参加数据分析的比赛，之前一直把知识都停留在表面，等到自己真正写起来，才发现很多地方都不会！！最后成绩197，虽然很低，但是也是自己努力的成果，从一个连基础的机器学习算法都不了解的小白，到一个可以进行简单预测，并调参的中白 :) 得益于这次比赛是由 Datawhale与天池联合发起的，在论坛提供了很多学习的知识，并且有大佬分析自己的baseline和思路，这种学习的氛围真的挺好的，否则自己太容易变成闷葫芦了~~防止闭门造车！！ 先上个代码~ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import numpy as npimport pandas as pdfrom sklearn import preprocessingfrom scipy.special import boxcox, inv_boxcoximport matplotlibimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings('ignore')data_train = pd.read_csv("./Train_data.csv", sep=",")# print(data_train.info())# print(data_train.describe())data_test = pd.read_csv("./TestA_data.csv", sep=",")train_num = data_train.shape[0]y_train = data_train['price']## 注意这里对y进行了boxcox变换y_train = boxcox(y_train, 0)all_data = pd.concat((data_train, data_test)).reset_index(drop=True)all_data.drop(['price'], axis=1, inplace=True)def processing_data(X): number_X = X.select_dtypes(exclude=object) ## 1.将数值型feature里的缺失值填补为他们每个featuer的均值 my_imputer = preprocessing.Imputer(strategy='mean') new_number_X = my_imputer.fit_transform(number_X) ## （偏度）绝对值大于0.75的特征进行一个boxcox变换 for i in number_X.columns: if abs(number_X[i].skew()) &gt; 0.75: number_X[i] = boxcox(number_X[i], 0) number_X = pd.DataFrame(new_number_X, columns=number_X.columns) # print('--------------------数字型特征已处理完毕--------------------') category_X = X.select_dtypes(include=object) for i in category_X.columns: category_X[i] = category_X[i].fillna('noinfo') # print('--------------------类别特征已处理完毕--------------------') X = pd.concat([number_X, category_X], axis=1) return X# all_data = processing_data(all_data)all_data = pd.get_dummies(all_data, drop_first=True)from xgboost import XGBRegressorfrom lightgbm import LGBMRegressorfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, train_test_splitfrom sklearn import preprocessingfrom sklearn.metrics import mean_squared_log_errorimport lightgbm as lgbfrom sklearn.pipeline import make_pipelinefrom scipy.special import boxcox, inv_boxcoxfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom sklearn.svm import SVRfrom sklearn.linear_model import Lasso, LassoCV, RidgeCVX_train = all_data[:150000]X_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)X_test = all_data[150000:]# 第一次# &#123;'subsample': 1, 'scale_pos_weight': 0.5, 'reg_lambda': 1, 'reg_alpha': 0.5,# 'n_estimators': 450, 'min_child_weight': 1, 'max_depth': 6, 'max_delta_step': 3,# 'learning_rate': 0.08, 'colsample_bytree': 0.7&#125;# &#123;'subsample': 0.1, 'num_leaves': 350, 'n_estimators': 300, 'min_child_samples': 4, 'max_depth': 90,# 'learning_rate': 0.15&#125;# 0.11190316301363079## def find_best_param(model):# xgb_param_test = &#123;# 'n_estimators': [450],# 'max_depth': [6, 7, 8],# 'learning_rate': [0.02, 0.05, 0.08],# 'min_child_weight': [0, 1,2],# 'max_delta_step': [ 3,4,5],# 'subsample': [0.5, 0.8, 1],# 'colsample_bytree': [0.5, 0.7, 1],# 'reg_alpha': [0.2, 0.5, 1],# 'reg_lambda': [0.3, 0.5, 1],# 'scale_pos_weight': [0.3, 0.5, 1]# &#125;# # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同# grid_search = RandomizedSearchCV(estimator=model, param_distributions=xgb_param_test, scoring='mean_squared_error',# cv=3)# # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5)# grid_search.fit(X_train, y_train)# print(grid_search.best_params_)### ## xgbreg = XGBRegressor()# find_best_param(xgbreg)### def find_best_param1(model):# lgb_param_test = &#123;# 'num_leaves': [250, 300, 350],# 'max_depth': [90, 100, 130],# 'subsample': [0.1, 0.12, 0.15],# 'min_child_samples': [3, 4, 5],# 'learning_rate': [0.05,0.1,0.15],# 'n_estimators': [100,200,300]# &#125;## # 注意RandomizedSearchCV跟GridSearchCV的参数 param_distributions跟param_grid作用相同，但名字不同# grid_search = RandomizedSearchCV(estimator=model, param_distributions=lgb_param_test, scoring='mean_squared_error',# cv=3)# # grid_search = GridSearchCV(estimator=model, param_grid=xgb_param_test, scoring='mean_squared_error', cv=5)# grid_search.fit(X_train, y_train)# print(grid_search.best_params_)### lgbreg = LGBMRegressor(objective='regression_l1')# find_best_param1(lgbreg)# # ======================================================print('--------------------正在进行模型训练--------------------')# 最终模型与测试集答案相比的误差：0.10836896959704842，n=10000，0.05xgbreg1 = XGBRegressor(subsample=1, scale_pos_weight=0.5, reg_lambda=1, reg_alpha=0.5, n_estimators=10000, min_child_weight=1, max_depth=6, max_delta_step=3, learning_rate=0.05, colsample_bytree=0.7)xgbreg1.fit(X_train, y_train)xgb_train_pred = xgbreg1.predict(X_train)xgb_test_pred = xgbreg1.predict(x_val)#lgbreg = LGBMRegressor(objective='regression_l1', subsample=0.1, num_leaves=350, min_child_samples=4, max_depth=90,n_estimators=10000,learning_rate=0.05)lgbreg.fit(X_train, y_train)lgb_train_pred = lgbreg.predict(X_train)lgb_test_pred = lgbreg.predict(x_val)# def rmsle(y, y_pred):# return np.sqrt(mean_squared_log_error(y, y_pred))from sklearn.metrics import mean_absolute_error, mean_squared_log_error# y_train = inv_boxcox(y_train,0)# lgb_train_pred = inv_boxcox(lgb_train_pred,0)# xgb_train_pred = inv_boxcox(xgb_train_pred,0)print('LGBM与训练集答案相比的对数误差：&#123;&#125;'.format(mean_absolute_error(y_train, lgb_train_pred)))print('Xgboost与训练集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_train, xgb_train_pred)))## # y_val = inv_boxcox(y_val,0)# # lgb_test_pred = inv_boxcox(lgb_test_pred,0)# # xgb_test_pred = inv_boxcox(xgb_test_pred,0)ensemble_pred = 0.5 * xgb_test_pred + 0.5 * lgb_test_predprint('lgb与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, lgb_test_pred)))print('xgb与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, xgb_test_pred)))print('最终模型与测试集答案相比的误差：&#123;&#125;'.format(mean_absolute_error(y_val, ensemble_pred)))p1 = lgbreg.predict(X_test)p2 = xgbreg1.predict(X_test)p = 0.5*p1+0.5*p2p = inv_boxcox(p,0)sub = pd.DataFrame()sub['SaleID'] =X_test.indexsub['price'] = psub.to_csv('./house3.csv',index=False)# xgbreg1 = XGBRegressor(subsample=0.8, scale_pos_weight=0.5, reg_lambda=0.5, reg_alpha=1,# n_estimators=4000, min_child_weight=1, max_depth=7, max_delta_step=2,# learning_rate=0.08, colsample_bytree=0.7)# lgbreg = LGBMRegressor(objective='regression_l1', subsample=0.15, num_leaves=350, min_child_samples=4, max_depth=90,n_estimators=4000)]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML，天池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统知识学习]]></title>
    <url>%2F2020%2F03%2F16%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐系统基础推荐系统简介推荐的概念 信息过滤系统 解决 信息过载 用户需求不明确的问题 利用一定的规则将物品排序 展示给需求不明确的用户 推荐 搜索区别 推荐个性化较强，用户被动的接受，希望能够提供持续的服务 搜索个性化弱，用户主动搜索，快速满足用户的需求 推荐和 web项目区别 构建稳定的信息流通通道 推荐 信息过滤系统 web 对结果有明确预期 推荐 结果是概率问题 Lambda 架构介绍 离线计算和实时计算共同提供服务的问题 离线计算优缺点 优点 能够处理的数据量可以很大 比如pb级别 缺点 速度比较慢 分钟级别的延迟 实时计算 优点 响应快 来一条数据处理一条 ms级别响应 缺点 处理的数据量小一些 离线计算的框架 hadoop hdfs mapreduce spark core , spark sql hive 实时计算框架 spark streaming storm flink 消息中间件 flume 日志采集系统 kafka 消息队列 存储相关 hbase nosql数据库 hive sql操作hdfs数据 推荐算法架构 召回 协同过滤 算相似度 memory base 基于模型的 model base 矩阵分解 基于内容 分词 词权重（提取关键词） tf-idf word2Vec 词向量 物品向量 排序 逻辑回归 策略调整 整体流程 lambda内 推荐算法 推荐模型构建流程 推荐算法概述 基于协同过滤的推荐算法 协同过滤实现 一 推荐模型构建流程Data(数据)-&gt;Features(特征)-&gt;ML Algorithm(机器学习算法)-&gt;Prediction Output(预测输出) 数据清洗/数据处理 数据来源 显性数据 Rating 打分 Comments 评论/评价 隐形数据 Order history 历史订单 Cart events 加购物车 Page views 页面浏览 Click-thru 点击 Search log 搜索记录 数据量/数据能否满足要求 特征工程 从数据中筛选特征 一个给定的商品，可能被拥有类似品味或需求的用户购买 使用用户行为数据描述商品 用数据表示特征 将所有用户行为合并在一起 ，形成一个user-item 矩阵 选择合适的算法 产生推荐结果 二 最经典的推荐算法：协同过滤推荐算法（Collaborative Filtering）协同过滤(CF)思路介绍 CF 物以类聚人以群分 做协同过滤的话 首先特征工程把 用户-物品的评分矩阵创建出来 基于用户的协同过滤 给用户A 找到最相似的N个用户 N个用户消费过哪些物品 N个用户消费过的物品中-A用户消费过的就是推荐结果 基于物品的协同过滤 给物品A 找到最相似的N个物品 A用户消费记录 找到这些物品的相似物品 从这些相似物品先去重-A用户消费过的就是推荐结果 三 相似度计算(Similarity Calculation)相似度计算 余弦相似度、皮尔逊相关系数 向量的夹角余弦值 皮尔逊会对向量的每一个分量做中心化 余弦只考虑方向 不考虑向量长度 如果评分数据是连续的数值比较适合中余弦、皮尔逊计算相似度 杰卡德相似度 交集/并集 计算评分是0 1 布尔值的相似度 使用不同相似度计算方式实现协同过滤 如果 买/没买 点/没点数据 0/1 适合使用杰卡德相似度 from sklearn.metrics import jaccard_similarity_score jaccard_similarity_score(df[‘Item A’],df[‘Item B’]) from sklearn.metrics.pairwise import pairwise_distances user_similar = 1-pairwise_distances(df,metric=’jaccard’) 一般用评分去做协同过滤 推荐使用皮尔逊相关系数(sim是交集除并集) 评分预测 $$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$ 基于用户和基于物品的协同过滤 严格上说，属于两种算法，实践中可以都做出来，对比效果，选择最靠谱的 协同过滤推荐算法代码实现： 构建数据集： 12345678910users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 构建数据集datasets = [ ["buy",None,"buy","buy",None], ["buy",None,None,"buy","buy"], ["buy",None,"buy",None,None], [None,"buy",None,"buy","buy"], ["buy","buy","buy",None,"buy"],] 计算时我们数据通常都需要对数据进行处理，或者编码，目的是为了便于我们对数据进行运算处理，比如这里是比较简单的情形，我们用1、0分别来表示用户的是否购买过该物品，则我们的数据集其实应该是这样的： 12345678910111213141516users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]import pandas as pddf = pd.DataFrame(datasets, columns=items, index=users)print(df) 有了数据集，接下来我们就可以进行相似度的计算，不过对于相似度的计算其实是有很多专门的相似度计算方法的，比如余弦相似度、皮尔逊相关系数、杰卡德相似度等等。这里我们选择使用杰卡德相似系数[0,1] 12345678910111213141516171819# 直接计算某两项的杰卡德相似系数from sklearn.metrics import jaccard_similarity_score# 计算Item A 和Item B的相似度print(jaccard_similarity_score(df["Item A"], df["Item B"]))# 计算所有的数据两两的杰卡德相似系数(相似度=1-距离)from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar) 有了两两的相似度，接下来就可以筛选TOP-N相似结果，并进行推荐了 User-Based CF 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)topN_users = &#123;&#125;# 遍历每一行数据for i in user_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = user_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_users[i] = top2print("Top2相似用户：")pprint(topN_users)rs_results = &#123;&#125;# 构建推荐结果for user, sim_users in topN_users.items(): rs_result = set() # 存储推荐结果 for sim_user in sim_users: # 构建初始的推荐结果 #dropna()的使用 b=a.dropna()，则直接将a中的nan值给删除掉了 # 使用loc:只能指定行列索引的名字 # 使用iloc可以通过索引的下标去获取 # 使用ix进行下表和名称组合做引 rs_result = rs_result.union(set(df.ix[sim_user].replace(0,np.nan).dropna().index)) # 过滤掉已经购买过的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results) Item-Based CF 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [1,0,1,1,0], [1,0,0,1,1], [1,0,1,0,0], [0,1,0,1,1], [1,1,1,0,1],]df = pd.DataFrame(datasets, columns=items, index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar)topN_items = &#123;&#125;# 遍历每一行数据for i in item_similar.index: # 取出每一列数据，并删除自身，然后排序数据 _df = item_similar.loc[i].drop([i]) _df_sorted = _df.sort_values(ascending=False) top2 = list(_df_sorted.index[:2]) topN_items[i] = top2print("Top2相似物品：")pprint(topN_items)rs_results = &#123;&#125;# 构建推荐结果for user in df.index: # 遍历所有用户 rs_result = set() for item in df.ix[user].replace(0,np.nan).dropna().index: # 取出每个用户当前已购物品列表 # 根据每个物品找出最相似的TOP-N物品，构建初始推荐结果 rs_result = rs_result.union(topN_items[item]) # 过滤掉用户已购的物品 rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index) # 添加到结果中 rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results) 关于协同过滤推荐算法使用的数据集 在前面的demo中，我们只是使用用户对物品的一个购买记录，类似也可以是比如浏览点击记录、收听记录等等。这样数据我们预测的结果其实相当于是在预测用户是否对某物品感兴趣，对于喜好程度不能很好的预测。 因此在协同过滤推荐算法中其实会更多的利用用户对物品的“评分”数据来进行预测，通过评分数据集，我们可以预测用户对于他没有评分过的物品的评分。其实现原理和思想和都是一样的，只是使用的数据集是用户-物品的评分数据。 关于用户-物品评分矩阵 用户-物品的评分矩阵，根据评分矩阵的稀疏程度会有不同的解决方案 稠密评分矩阵 稀疏评分矩阵 这里先介绍稠密评分矩阵的处理，稀疏矩阵的处理相对会复杂一些，我们到后面再来介绍。 使用协同过滤推荐算法对用户进行评分预测 数据集： 目的：预测用户1对物品E的评分 构建数据集：注意这里构建评分数据时，对于缺失的部分我们需要保留为None，如果设置为0那么会被当作评分值为0去对待 12345678910users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [ [5,3,4,4,None], [3,1,2,3,3], [4,3,4,3,5], [3,3,1,5,4], [1,5,5,2,1],] 计算相似度：对于评分数据这里我们采用皮尔逊相关系数[-1,1]来计算，-1表示强负相关，+1表示强正相关 pandas中corr方法可直接用于计算皮尔逊相关系数 12345678910111213df = pd.DataFrame(datasets, columns=items, index=users)print("用户之间的两两相似度：")# 直接计算皮尔逊相关系数# 默认是按列进行计算，因此如果计算用户间的相似度，当前需要进行转置user_similar = df.T.corr()print(user_similar.round(4))print("物品之间的两两相似度：")item_similar = df.corr()print(item_similar.round(4)) 123456789101112131415# 运行结果：用户之间的两两相似度： User1 User2 User3 User4 User5User1 1.0000 0.8528 0.7071 0.0000 -0.7921User2 0.8528 1.0000 0.4677 0.4900 -0.9001User3 0.7071 0.4677 1.0000 -0.1612 -0.4666User4 0.0000 0.4900 -0.1612 1.0000 -0.6415User5 -0.7921 -0.9001 -0.4666 -0.6415 1.0000物品之间的两两相似度： Item A Item B Item C Item D Item EItem A 1.0000 -0.4767 -0.1231 0.5322 0.9695Item B -0.4767 1.0000 0.6455 -0.3101 -0.4781Item C -0.1231 0.6455 1.0000 -0.7206 -0.4276Item D 0.5322 -0.3101 -0.7206 1.0000 0.5817Item E 0.9695 -0.4781 -0.4276 0.5817 1.0000 可以看到与用户1最相似的是用户2和用户3；与物品A最相似的物品分别是物品E和物品D。 注意：我们在预测评分时，往往是通过与其有正相关的用户或物品进行预测，如果不存在正相关的情况，那么将无法做出预测。这一点尤其是在稀疏评分矩阵中尤为常见，因为稀疏评分矩阵中很难得出正相关系数。 评分预测： User-Based CF 评分预测：使用用户间的相似度进行预测 关于评分预测的方法也有比较多的方案，下面介绍一种效果比较好的方案，该方案考虑了用户本身的评分评分以及近邻用户的加权平均相似度打分来进行预测：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$我们要预测用户1对物品E的评分，那么可以根据与用户1最近邻的用户2和用户3进行预测，计算如下： ​$$pred(u_1, i_5) =\cfrac{0.853+0.715}{0.85+0.71} = 3.91$$最终预测出用户1对物品5的评分为3.91 Item-Based CF 评分预测：使用物品间的相似度进行预测 这里利用物品相似度预测的计算同上，同样考虑了用户自身的平均打分因素，结合预测物品与相似物品的加权平均相似度打分进行来进行预测$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{j\in I_{rated}}sim(i,j)r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}$$我们要预测用户1对物品E的评分，那么可以根据与物品E最近邻的物品A和物品D进行预测，计算如下：$$pred(u_1, i_5) = \cfrac {0.975+0.58*4}{0.97+0.58} = 4.63$$对比可见，User-Based CF预测评分和Item-Based CF的评分结果也是存在差异的，因为严格意义上他们其实应当属于两种不同的推荐算法，各自在不同的领域不同场景下，都会比另一种的效果更佳，但具体哪一种更佳，必须经过合理的效果评估，因此在实现推荐系统时这两种算法往往都是需要去实现的，然后对产生的推荐效果进行评估分析选出更优方案。 基于模型的方法–解决用户和物品矩阵比较稀疏的情况 思想 通过机器学习算法，在数据中找出模式，并将用户与物品间的互动方式模式化 基于模型的协同过滤方式是构建协同过滤更高级的算法 近邻模型的问题 物品之间存在相关性, 信息量并不随着向量维度增加而线性增加 矩阵元素稀疏, 计算结果不稳定,增减一个向量维度, 导致近邻结果差异很大的情况存在 算法分类 基于图的模型 基于矩阵分解的方法 基于图的模型 基于邻域的模型看做基于图的模型的简单形式 原理 将用户的行为数据表示为二分图 基于二分图为用户进行推荐 根据两个顶点之间的路径数、路径长度和经过的顶点数来评价两个顶点的相关性 基于矩阵分解的模型–降维 原理 根据用户与物品的潜在表现，我们就可以预测用户对未评分的物品的喜爱程度 把原来的大矩阵, 近似分解成两个小矩阵的乘积, 在实际推荐计算时不再使用大矩阵, 而是使用分解得到的两个小矩阵 用户-物品评分矩阵A是M X N维, 即一共有M个用户, n个物品 我们选一个很小的数 K (K&lt;&lt; M, K&lt;&lt;N) 通过计算得到两个矩阵U V U是M * K矩阵 , 矩阵V是 N * K $U_{mk} V^{T}_{nk} 约等于 A_{m*n}$ 类似这样的计算过程就是矩阵分解 基于矩阵分解的方法 ALS交替最小二乘 ALS-WR(加权正则化交替最小二乘法): alternating-least-squares with weighted-λ –regularization 将用户(user)对商品(item)的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。在这个矩阵分解的过程中，评分缺失项得到了填充，也就是说我们可以基于这个填充的评分来给用户做商品推荐了。 SVD奇异值分解矩阵 ALS方法–交替最小二乘法来优化损失 ALS的矩阵分解算法常应用于推荐系统中，将用户(user)对商品(item)的评分矩阵，分解为用户对商品隐含特征的偏好矩阵，和商品在隐含特征上的映射矩阵。 与传统的矩阵分解SVD方法来分解矩阵R(R∈ℝm×n)不同的是，ALS(alternating least squares)希望找到两个低维矩阵，以 R̃ =XY 来逼近矩阵R，其中 ，X∈ℝm×d，Y∈ℝd×n，这样，将问题的复杂度由O(mn)转换为O((m+n)d)。 计算X和Y过程：首先用一个小于1的随机数初始化Y，并根据公式求X，此时就可以得到初始的XY矩阵了，根据平方差和得到的X，重新计算并覆盖Y，计算差平方和，反复进行以上两步的计算，直到差平方和小于一个预设的数，或者迭代次数满足要求则停止 四 推荐系统评估 好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢 显示反馈和隐式反馈 显式反馈 隐式反馈 例子 电影/书籍评分 是否喜欢这个推荐 播放/点击 评论 下载 购买 准确性 高 低 数量 少 多 获取成本 高 低 常用评估指标 • 准确性 • 信任度• 满意度 • 实时性• 覆盖率 • 鲁棒性• 多样性 • 可扩展性• 新颖性 • 商业⽬标• 惊喜度 • ⽤户留存 准确性 (理论角度) Netflix 美国录像带租赁 评分预测–线性回归 RMSE MAE topN推荐 召回率 精准率 准确性 (业务角度) 覆盖度 信息熵 对于推荐越大越好 覆盖率 多样性&amp;新颖性&amp;惊喜性 多样性：推荐列表中两两物品的不相似性。（相似性如何度量？ 新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度 惊喜性：历史不相似（惊）但很满意（喜） 往往需要牺牲准确性 使⽤历史⾏为预测⽤户对某个物品的喜爱程度 系统过度强调实时性 Exploitation &amp; Exploration 探索与利用问题 Exploitation(开发 利用)：选择现在可能最佳的⽅案 Exploration(探测 搜索)：选择现在不确定的⼀些⽅案，但未来可能会有⾼收益的⽅案 在做两类决策的过程中，不断更新对所有决策的不确定性的认知，优化长期的⽬标 EE问题实践 兴趣扩展: 相似话题, 搭配推荐 人群算法: userCF 用户聚类 平衡个性化推荐和热门推荐比例 随机丢弃用户行为历史 随机扰动模型参数 EE可能带来的问题 探索伤害用户体验, 可能导致用户流失 探索带来的长期收益(留存率)评估周期长, KPI压力大 如何平衡实时兴趣和长期兴趣 如何平衡短期产品体验和长期系统生态 如何平衡大众口味和小众需求 评估方法 问卷调查: 成本高 离线评估: 只能在用户看到过的候选集上做评估, 且跟线上真实效果存在偏差 只能评估少数指标 速度快, 不损害用户体验 在线评估: 灰度发布 &amp; A/B测试 50% 全量上线 实践: 离线评估和在线评估结合, 定期做问卷调查 五 推荐系统的冷启动问题 推荐系统冷启动概念 ⽤户冷启动：如何为新⽤户做个性化推荐 物品冷启动：如何将新物品推荐给⽤户（协同过滤） 系统冷启动：⽤户冷启动+物品冷启动 本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好 用户冷启动 1.收集⽤户特征 ⽤户注册信息：性别、年龄、地域 设备信息：定位、⼿机型号、app列表 社交信息、推⼴素材、安装来源 2 引导用户填写兴趣 3 使用其它站点的行为数据, 例如腾讯视频&amp;QQ音乐 今日头条&amp;抖音 4 新老用户推荐策略的差异 新⽤户在冷启动阶段更倾向于热门排⾏榜，⽼⽤户会更加需要长尾推荐 Explore Exploit⼒度 使⽤单独的特征和模型预估 举例 性别与电视剧的关系 物品冷启动 给物品打标签–构建物品画像 利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。 系统冷启动 基于内容的推荐 系统早期 基于内容的推荐逐渐过渡到协同过滤 基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果 案例–基于协同过滤的电影推荐前面我们已经基本掌握了协同过滤推荐算法，以及其中两种最基本的实现方案：User-Based CF和Item-Based CF，下面我们将利用真是的数据来进行实战演练。 案例需求 演示效果 分析案例 数据集下载MovieLens Latest Datasets Small 建议下载ml-latest-small.zip，数据量小，便于我们单机使用和运行 目标：根据ml-latest-small/ratings.csv（用户-电影评分数据），分别实现User-Based CF和Item-Based CF，并进行电影评分的预测，然后为用户实现电影推荐 数据集加载 加载ratings.csv，并转换为用户-电影评分矩阵 123456789101112131415161718192021222324252627282930313233343536373839import osimport pandas as pdimport numpy as npDATA_PATH = "./datasets/ml-latest-small/ratings.csv"CACHE_DIR = "./datasets/cache/"def load_data(data_path): ''' 加载数据 :param data_path: 数据集路径 :param cache_path: 数据集缓存路径 :return: 用户-物品评分矩阵 ''' # 数据集缓存地址 cache_path = os.path.join(CACHE_DIR, "ratings_matrix.cache") print("开始加载数据集...") if os.path.exists(cache_path): # 判断是否存在缓存文件 print("加载缓存中...") ratings_matrix = pd.read_pickle(cache_path) # 转换成pickle加快操作 print("从缓存加载数据集完毕") else: print("加载新数据中...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) # 读取前三列 # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵 ratings_matrix = ratings.pivot_table(index=["userId"], columns=["movieId"], values="rating") # 存入缓存文件 ratings_matrix.to_pickle(cache_path) print("数据集加载完毕") return ratings_matrixif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) print(ratings_matrix) (https://www.cnblogs.com/Yanjy-OnlyOne/p/11195621.html) 透视表知识点补充 相似度计算 计算用户或物品两两相似度： 12345678910111213141516171819202122232425262728293031323334353637383940414243# ......def compute_pearson_similarity(ratings_matrix, based="user"): ''' 计算皮尔逊相关系数 :param ratings_matrix: 用户-物品评分矩阵 :param based: "user" or "item" :return: 相似度矩阵 ''' user_similarity_cache_path = os.path.join(CACHE_DIR, "user_similarity.cache") item_similarity_cache_path = os.path.join(CACHE_DIR, "item_similarity.cache") # 基于皮尔逊相关系数计算相似度 # 用户相似度 if based == "user": if os.path.exists(user_similarity_cache_path): print("正从缓存加载用户相似度矩阵") similarity = pd.read_pickle(user_similarity_cache_path) else: print("开始计算用户相似度矩阵") similarity = ratings_matrix.T.corr() similarity.to_pickle(user_similarity_cache_path) elif based == "item": if os.path.exists(item_similarity_cache_path): print("正从缓存加载物品相似度矩阵") similarity = pd.read_pickle(item_similarity_cache_path) else: print("开始计算物品相似度矩阵") similarity = ratings_matrix.corr() similarity.to_pickle(item_similarity_cache_path) else: raise Exception("Unhandled 'based' Value: %s"%based) print("相似度矩阵计算/加载完毕") return similarityif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") print(user_similar) item_similar = compute_pearson_similarity(ratings_matrix, based="item") print(item_similar) 注意以上实现，仅用于实验阶段，因为工业上、或生产环境中，数据量是远超过我们本例中使用的数据量的，而pandas是无法支撑起大批量数据的运算的，因此工业上通常会使用spark、mapReduce等分布式计算框架来实现，我们后面的课程中也是建立在此基础上进行实践的。 但是正如前面所说，推荐算法的思想和理念都是统一的，不论使用什么平台工具、有多大的数据体量，其背后的实现原理都是不变的。 所以在本节，大家要深刻去学习的是推荐算法的业务流程，以及在具体的业务场景中，如本例的电影推荐，如何实现出推荐算法，并产生推荐结果。 案例–算法实现：User-Based CF 预测评分评分预测公式：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}$$ 算法实现 实现评分预测方法：predict 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ......def predict(uid, iid, ratings_matrix, user_similar): ''' 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param user_similar: 用户两两相似度矩阵 :return: 预测的评分值 ''' print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."%(uid, iid)) # 1. 找出uid用户的相似用户 similar_users = user_similar[uid].drop([uid]).dropna() # 相似用户筛选规则：正相关的用户 similar_users = similar_users.where(similar_users&gt;0).dropna() if similar_users.empty is True: raise Exception("用户&lt;%d&gt;没有相似的用户" % uid) # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户 ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index) finally_similar_users = similar_users.ix[list(ids)] # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 for sim_uid, similarity in finally_similar_users.iteritems(): # 近邻用户的评分数据 sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna() # 近邻用户对iid物品的评分 sim_user_rating_for_item = sim_user_rated_movies[iid] # 计算分子的值 sum_up += similarity * sim_user_rating_for_item # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up/sum_down print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating)) return round(predict_rating, 2)if __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") # 预测用户1对物品1的评分 predict(1, 1, ratings_matrix, user_similar) # 预测用户1对物品2的评分 predict(1, 2, ratings_matrix, user_similar) 实现预测全部评分方法：predict_all 123456789101112131415161718192021222324252627# ......def predict_all(uid, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 准备要预测的物品的id列表 item_ids = ratings_matrix.columns # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingif __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") for i in predict_all(1, ratings_matrix, user_similar): pass 添加过滤规则 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def _predict_all(uid, item_ids, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param item_ids: 要预测的物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 逐个预测 for iid in item_ids: try: rating = predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, ratingdef predict_all(uid, ratings_matrix, user_similar, filter_rule=None): ''' 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None :return: 生成器，逐个返回预测评分 ''' if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == "unhot": '''过滤非热门电影''' # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count&gt;10).dropna().index elif isinstance(filter_rule, str) and filter_rule == "rated": '''过滤用户评分过的电影''' # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings&lt;6 item_ids = _.where(_==False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]): '''过滤非热门和用户已经评分过的电影''' count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1)&amp;set(ids2) else: raise Exception("无效的过滤参数") yield from _predict_all(uid, item_ids, ratings_matrix, user_similar)if __name__ == '__main__': ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") for result in predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]): print(result) 根据预测评分为指定用户进行TOP-N推荐： 123456789101112# ......def top_k_rs_result(k): ratings_matrix = load_data(DATA_PATH) user_similar = compute_pearson_similarity(ratings_matrix, based="user") results = predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]) return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__': from pprint import pprint result = top_k_rs_result(20) pprint(result) 案例–算法实现：Item-Based CF 预测评分评分预测公式：$$pred(u,i)=\hat{r}{ui}=\cfrac{\sum{j\in I_{rated}}sim(i,j)*r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}$$(与U-B一样) Model-Based 协同过滤算法随着机器学习技术的逐渐发展与完善，推荐系统也逐渐运用机器学习的思想来进行推荐。将机器学习应用到推荐系统中的方案真是不胜枚举。以下对Model-Based CF算法做一个大致的分类： 基于分类算法、回归算法、聚类算法 基于矩阵分解的推荐 基于神经网络算法 基于图模型算法 接下来我们重点学习以下几种应用较多的方案： 基于K最近邻的协同过滤推荐 基于回归模型的协同过滤推荐 基于矩阵分解的协同过滤推荐 基于K最近邻的协同过滤推荐基于K最近邻的协同过滤推荐其实本质上就是MemoryBased CF，只不过在选取近邻的时候，加上K最近邻的限制。 这里我们直接根据MemoryBased CF的代码实现 修改以下地方 123456789101112131415class CollaborativeFiltering(object): based = None def __init__(self, k=40, rules=None, use_cache=False, standard=None): ''' :param k: 取K个最近邻来进行预测 :param rules: 过滤规则，四选一，否则将抛异常："unhot", "rated", ["unhot","rated"], None :param use_cache: 相似度计算结果是否开启缓存 :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化 ''' self.k = 40 self.rules = rules self.use_cache = use_cache self.standard = standard 修改所有的选取近邻的地方的代码，根据相似度来选取K个最近邻 123similar_users = self.similar[uid].drop([uid]).dropna().sort_values(ascending=False)[:self.k]similar_items = self.similar[iid].drop([iid]).dropna().sort_values(ascending=False)[:self.k] 最终代码汇总 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251import osimport pandas as pdimport numpy as npDATA_PATH = "../datasets/ml-latest-small/ratings.csv"CACHE_DIR = "../datasets/cache/"class CollaborativeFiltering(object): based = None def __init__(self, k=40, rules=None, use_cache=False, standard=None): ''' :param k: 取K个最近邻来进行预测 :param rules: 过滤规则，四选一，否则将抛异常："unhot", "rated", ["unhot","rated"], None :param use_cache: 相似度计算结果是否开启缓存 :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化 ''' self.k = 40 self.rules = rules self.use_cache = use_cache self.standard = standard # 加载数据集 def load_data(self, data_path): ''' 加载数据 :param data_path: 数据集路径 :param cache_path: 数据集缓存路径 :return: 用户-物品评分矩阵 ''' # 数据集缓存地址 cache_path = os.path.join(CACHE_DIR, "ratings_matrix.cache") print("开始加载数据集...") if os.path.exists(cache_path): # 判断是否存在缓存文件 print("加载缓存中...") ratings_matrix = pd.read_pickle(cache_path) # 转换成pickle加快操作 print("从缓存加载数据集完毕") else: print("加载新数据中...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) # 读取前三列 # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵 ratings_matrix = ratings.pivot_table(index=["userId"], columns=["movieId"], values="rating") # 存入缓存文件 ratings_matrix.to_pickle(cache_path) print("数据集加载完毕") return ratings_matrix # 计算相似度 def compute_pearson_similarity(self, ratings_matrix, based="user"): ''' 计算皮尔逊相关系数 :param ratings_matrix: 用户-物品评分矩阵 :param based: "user" or "item" :return: 相似度矩阵 ''' user_similarity_cache_path = os.path.join(CACHE_DIR, "user_similarity.cache") item_similarity_cache_path = os.path.join(CACHE_DIR, "item_similarity.cache") # 基于皮尔逊相关系数计算相似度 # 用户相似度 if based == "user": if os.path.exists(user_similarity_cache_path): print("正从缓存加载用户相似度矩阵") similarity = pd.read_pickle(user_similarity_cache_path) else: print("开始计算用户相似度矩阵") similarity = ratings_matrix.T.corr() similarity.to_pickle(user_similarity_cache_path) elif based == "item": if os.path.exists(item_similarity_cache_path): print("正从缓存加载物品相似度矩阵") similarity = pd.read_pickle(item_similarity_cache_path) else: print("开始计算物品相似度矩阵") similarity = ratings_matrix.corr() similarity.to_pickle(item_similarity_cache_path) else: raise Exception("Unhandled 'based' Value: %s" % based) print("相似度矩阵计算/加载完毕") return similarity # 预测指定用户指定商品的评分 def predict(self, uid, iid, ratings_matrix, user_similar): ''' 预测给定用户对给定物品的评分值 :param uid: 用户ID :param iid: 物品ID :param ratings_matrix: 用户-物品评分矩阵 :param user_similar: 用户两两相似度矩阵 :return: 预测的评分值 ''' print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..." % (uid, iid)) # 1. 找出uid用户的相似用户 similar_users = user_similar[uid].drop([uid]).dropna().sort_values(ascending=False)[:self.k] # 相似用户筛选规则：正相关的用户 similar_users = similar_users.where(similar_users &gt; 0).dropna() if similar_users.empty is True: raise Exception("用户&lt;%d&gt;没有相似的用户" % uid) # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户 # (其中就是筛选出来的用户以下条件:1.是uid的近邻用户 2. 且这些用户对iid有评分) ids = set(ratings_matrix[iid].dropna().index) &amp; set(similar_users.index) finally_similar_users = similar_users.ix[list(ids)] # ids内是索引 # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分 sum_up = 0 # 评分预测公式的分子部分的值 sum_down = 0 # 评分预测公式的分母部分的值 # iteritems()与itemgetter()函数作用(https://www.cnblogs.com/SpringFull/p/10168533.html) for sim_uid, similarity in finally_similar_users.iteritems(): # 近邻用户的评分数据 sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna() # 近邻用户对iid物品的评分 sim_user_rating_for_item = sim_user_rated_movies[iid] # 计算分子的值 sum_up += similarity * sim_user_rating_for_item # 计算分母的值 sum_down += similarity # 计算预测的评分值并返回 predict_rating = sum_up / sum_down print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating)) return round(predict_rating, 2) # 预测指定用户的全部商品评分 # def predict_all(uid, ratings_matrix, user_similar): # ''' # 预测全部评分 # :param uid: 用户id # :param ratings_matrix: 用户-物品打分矩阵 # :param user_similar: 用户两两间的相似度 # :return: 生成器，逐个返回预测评分 # ''' # # 准备要预测的物品的id列表 # item_ids = ratings_matrix.columns # # 逐个预测 # for iid in item_ids: # try: # rating = predict(uid, iid, ratings_matrix, user_similar) # except Exception as e: # print(e) # else: # yield uid, iid, rating # 添加过滤条件 def _predict_all(self, uid, item_ids, ratings_matrix, user_similar): ''' 预测全部评分 :param uid: 用户id :param item_ids: 要预测的物品id列表 :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :return: 生成器，逐个返回预测评分 ''' # 逐个预测 for iid in item_ids: try: rating = self.predict(uid, iid, ratings_matrix, user_similar) except Exception as e: print(e) else: yield uid, iid, rating # 预测指定用户的全部商品评分 def predict_all(self, uid, ratings_matrix, user_similar, filter_rule=None): ''' 预测全部评分，并可根据条件进行前置过滤 :param uid: 用户ID :param ratings_matrix: 用户-物品打分矩阵 :param user_similar: 用户两两间的相似度 :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None :return: 生成器，逐个返回预测评分 ''' if not filter_rule: item_ids = ratings_matrix.columns elif isinstance(filter_rule, str) and filter_rule == "unhot": '''过滤非热门电影''' # 统计每部电影的评分数 count = ratings_matrix.count() # 过滤出评分数高于10的电影，作为热门电影 item_ids = count.where(count &gt; 10).dropna().index elif isinstance(filter_rule, str) and filter_rule == "rated": '''过滤用户评分过的电影''' # 获取用户对所有电影的评分记录 user_ratings = ratings_matrix.ix[uid] # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的 _ = user_ratings &lt; 6 # 这里的 _ 存的是判断结果(T/F) item_ids = _.where(_ == False).dropna().index elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]): '''过滤非热门和用户已经评分过的电影''' count = ratings_matrix.count() ids1 = count.where(count &gt; 10).dropna().index user_ratings = ratings_matrix.ix[uid] _ = user_ratings &lt; 6 ids2 = _.where(_ == False).dropna().index # 取二者交集 item_ids = set(ids1) &amp; set(ids2) else: raise Exception("无效的过滤参数") yield from self._predict_all(uid, item_ids, ratings_matrix, user_similar) # 根据预测评分为指定用户进行TOP-N推荐 def top_k_rs_result(self, result, k): return sorted(result, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__': U_I = CollaborativeFiltering(k=5, rules='unhot', use_cache=False, standard=None) # 1.数据集加载 ratings_matrix = U_I.load_data(DATA_PATH) print(ratings_matrix) # 2.相似度计算-皮尔逊相似系数 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # print(user_similar) # item_similar = compute_pearson_similarity(ratings_matrix, based="item") # print(item_similar) # 3. 预测给定用户对给定物品的评分值 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # # 预测用户1对物品1的评分 # predict(1, 1, ratings_matrix, user_similar) # # 预测用户1对物品2的评分 # predict(1, 2, ratings_matrix, user_similar) # 4. 预测全部评分 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # for i in predict_all(1, ratings_matrix, user_similar): # pass # 5. 添加过滤规则后,预测指定用户的全部商品评分 # user_similar = compute_pearson_similarity(ratings_matrix, based="user") # # for result in predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]): # print(result) # 6.根据预测评分为指定用户进行TOP-N推荐 from pprint import pprint user_similar = U_I.compute_pearson_similarity(ratings_matrix, based="user") result = U_I.predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]) top_k = U_I.top_k_rs_result(result, 20) pprint(top_k) 但由于我们的原始数据较少，这里我们的KNN方法的效果会比纯粹的MemoryBasedCF要差 基于回归模型的协同过滤推荐如果我们将评分看作是一个连续的值而不是离散的值，那么就可以借助线性回归思想来预测目标用户对某物品的评分。其中一种实现策略被称为Baseline（基准预测）。 Baseline：基准预测Baseline设计思想基于以下的假设： 有些用户的评分普遍高于其他用户，有些用户的评分普遍低于其他用户。比如有些用户天生愿意给别人好评，心慈手软，比较好说话，而有的人就比较苛刻，总是评分不超过3分（5分满分） 一些物品的评分普遍高于其他物品，一些物品的评分普遍低于其他物品。比如一些物品一被生产便决定了它的地位，有的比较受人们欢迎，有的则被人嫌弃。 这个用户或物品普遍高于或低于平均值的差值，我们称为偏置(bias) Baseline目标： 找出每个用户普遍高于或低于他人的偏置值 $b_u$ 找出每件物品普遍高于或低于其他物品的偏置值$b_i$ 我们的目标也就转化为寻找最优的$b_u$和$b_i$ 使用Baseline的算法思想预测评分的步骤如下： 计算所有电影的平均评分$\mu$（即全局平均评分） 计算每个用户评分与平均评分$\mu$的偏置值$b_u$ 计算每部电影所接受的评分与平均评分$\mu$的偏置值$b_i$ 预测用户对电影的评分：$$\hat{r}{ui} = b{ui} = \mu + b_u + b_i$$ 举例： ​ 比如我们想通过Baseline来预测用户A对电影“阿甘正传”的评分，那么首先计算出整个评分数据集的平均评分$\mu$是3.5分；而用户A是一个比较苛刻的用户，他的评分比较严格，普遍比平均评分低0.5分，即用户A的偏置值$b_i$是-0.5；而电影“阿甘正传”是一部比较热门而且备受好评的电影，它的评分普遍比平均评分要高1.2分，那么电影“阿甘正传”的偏置值$b_i$是+1.2，因此就可以预测出用户A对电影“阿甘正传”的评分为：$3.5+(-0.5)+1.2$，也就是4.2分。 对于所有电影的平均评分$\mu$是直接能计算出的，因此问题在于要测出每个用户的$b_u$值和每部电影的$b_i$的值。对于线性回归问题，我们可以利用平方差构建损失函数如下：$$\begin{split}Cost &amp;= \sum_{u,i\in R}(r_{ui}-\hat{r}{ui})^2\&amp;=\sum{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2\end{split}$$ 加入L2正则化：$$Cost=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)$$公式解析： 公式第一部分$ \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2$是用来寻找与已知评分数据拟合最好的$b_u$和$b_i$ 公式第二部分$\lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)$是正则化项，用于避免过拟合现象 对于最小过程的求解，我们一般采用随机梯度下降法或者交替最小二乘法来优化实现。 方法一：随机梯度下降法优化梯度下降知识点补充 使用随机梯度下降优化算法预测Baseline偏置值 step 1：梯度下降法推导 –(要推出来b_u和b_i的表达式)损失函数：$$\begin{split}&amp;J(\theta)=Cost=f(b_u, b_i)\\&amp;J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)\end{split}$$梯度下降参数更新原始公式：$$\theta_j:=\theta_j-\alpha\cfrac{\partial }{\partial \theta_j}J(\theta)$$(对$b_u$和$b_i$求偏导) 梯度下降更新$b_u$: ​ 损失函数偏导推导：$$\begin{split}\cfrac{\partial}{\partial b_u} J(\theta)&amp;=\cfrac{\partial}{\partial b_u} f(b_u, b_i)\&amp;=2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)(-1) + 2\lambda{b_u}\&amp;=-2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambdab_u\end{split}$$​ $b_u$更新(因为alpha可以人为控制，所以2可以省略掉)：$$\begin{split}b_u&amp;:=b_u - \alpha(-\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + \lambda * b_u)\&amp;:=b_u + \alpha(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) - \lambda b_u)\end{split}$$同理可得，梯度下降更新$b_i$:$$b_i:=b_i + \alpha(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) -\lambdab_i)$$ step 2：随机梯度下降由于随机梯度下降法本质上利用每个样本的损失来更新参数，而不用每次求出全部的损失和，因此使用SGD时：(随机梯度下降每次只考虑一个样本，所以不用上面的求和) 单样本损失值：$$\begin{split}error &amp;=r_{ui}-\hat{r}{ui}\&amp;= r{ui}-(\mu+b_u+b_i)\&amp;= r_{ui}-\mu-b_u-b_i\end{split}$$参数更新：$$\begin{split}b_u&amp;:=b_u + \alpha((r_{ui}-\mu-b_u-b_i) -\lambda*b_u) \&amp;:=b_u + \alpha(error - \lambdab_u) \\b_i&amp;:=b_i + \alpha((r_{ui}-\mu-b_u-b_i) -\lambdab_i)\&amp;:=b_i + \alpha(error -\lambda*b_i)\end{split}$$ step 3：算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pdimport numpy as npclass BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): # for是为了把全部数据遍历一遍，上面的for保证了每个数据遍历20次 error = real_rating - (self.global_mean + bu[uid] + bi[iid]) # 这边的bu和bi都是每个用户每个商品都不一样，相当于每个数都计算了20次 bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFBySGD(20, 0.1, 0.1, ["userId", "movieId", "rating"]) bcf.fit(dataset) while True: uid = int(input("uid: ")) iid = int(input("iid: ")) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估 添加test方法，然后使用之前实现accuary方法计算准确性指标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): ''' 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 ''' print("开始切分数据集...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合 for uid in ratings.groupby("userId").any().index: user_rating_data = ratings.where(ratings["userId"]==uid).dropna() if random: # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print("完成数据集切分...") return trainset, testsetdef accuray(predict_results, method="all"): ''' 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: ''' def rmse(predict_results): ''' rmse评估指标 :param predict_results: :return: rmse ''' length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): ''' mae评估指标 :param predict_results: :return: mae ''' length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): ''' rmse和mae评估指标 :param predict_results: :return: rmse, mae ''' length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == "rmse": rmse(predict_results) elif method.lower() == "mae": mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFBySGD(object): def __init__(self, number_epochs, alpha, reg, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # 学习率 self.alpha = alpha # 正则参数 self.reg = reg # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.sgd() def sgd(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for uid, iid, real_rating in self.dataset.itertuples(index=False): error = real_rating - (self.global_mean + bu[uid] + bi[iid]) bu[uid] += self.alpha * (error - self.reg * bu[uid]) bi[iid] += self.alpha * (error - self.reg * bi[iid]) return bu, bi def predict(self, uid, iid): '''评分预测''' if iid not in self.items_ratings.index: raise Exception("无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据".format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': trainset, testset = data_split("datasets/ml-latest-small/ratings.csv", random=True) bcf = BaselineCFBySGD(20, 0.1, 0.1, ["userId", "movieId", "rating"]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print("rmse: ", rmse, "mae: ", mae) 方法二：交替最小二乘法优化使用交替最小二乘法优化算法预测Baseline偏置值 step 1: 交替最小二乘法推导最小二乘法和梯度下降法一样，可以用于求极值。 最小二乘法思想：对损失函数求偏导，然后再使偏导为0 同样，损失函数：$$J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda(\sum_u {b_u}^2 + \sum_i {b_i}^2)$$对损失函数求偏导：$$\cfrac{\partial}{\partial b_u} f(b_u, b_i) =-2 \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda * b_u$$令偏导为0，则可得：$$\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) = \lambda b_u\\sum_{u,i\in R}(r_{ui}-\mu-b_i) = \sum_{u,i\in R} b_u+\lambda * b_u$$为了简化公式，这里令$\sum_{u,i\in R} b_u \approx |R(u)|*b_u$，即直接假设每一项的偏置都相等，可得：$$b_u := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_i)}{\lambda_1 + |R(u)|}$$其中$|R(u)|$表示用户$u$的有过评分数量 同理可得：$$b_i := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_u)}{\lambda_2 + |R(i)|}$$其中$|R(i)|$表示物品$i$收到的评分数量 $b_u$和$b_i$分别属于用户和物品的偏置，因此他们的正则参数可以分别设置两个独立的参数 step 2: 交替最小二乘法应用通过最小二乘推导，我们最终分别得到了$b_u$和$b_i$的表达式，但他们的表达式中却又各自包含对方，因此这里我们将利用一种叫交替最小二乘的方法来计算他们的值： 计算其中一项，先固定其他未知参数，即看作其他未知参数为已知 如求$b_u$时，将$b_i$看作是已知；求$b_i$时，将$b_u$看作是已知；如此反复交替，不断更新二者的值，求得最终的结果。这就是交替最小二乘法（ALS） step 3: 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import pandas as pdimport numpy as npclass BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) bcf = BaselineCFByALS(20, 25, 15, ["userId", "movieId", "rating"]) bcf.fit(dataset) while True: uid = int(input("uid: ")) iid = int(input("iid: ")) print(bcf.predict(uid, iid)) Step 4: 准确性指标评估123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174import pandas as pdimport numpy as npdef data_split(data_path, x=0.8, random=False): ''' 切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分 :param data_path: 数据集路径 :param x: 训练集的比例，如x=0.8，则0.2是测试集 :param random: 是否随机切分，默认False :return: 用户-物品评分矩阵 ''' print("开始切分数据集...") # 设置要加载的数据字段的类型 dtype = &#123;"userId": np.int32, "movieId": np.int32, "rating": np.float32&#125; # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分 ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3)) testset_index = [] # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合 for uid in ratings.groupby("userId").any().index: user_rating_data = ratings.where(ratings["userId"]==uid).dropna() if random: # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表 index = list(user_rating_data.index) np.random.shuffle(index) # 打乱列表 _index = round(len(user_rating_data) * x) testset_index += list(index[_index:]) else: # 将每个用户的x比例的数据作为训练集，剩余的作为测试集 index = round(len(user_rating_data) * x) testset_index += list(user_rating_data.index.values[index:]) testset = ratings.loc[testset_index] trainset = ratings.drop(testset_index) print("完成数据集切分...") return trainset, testsetdef accuray(predict_results, method="all"): ''' 准确性指标计算方法 :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列 :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae :return: ''' def rmse(predict_results): ''' rmse评估指标 :param predict_results: :return: rmse ''' length = 0 _rmse_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 return round(np.sqrt(_rmse_sum / length), 4) def mae(predict_results): ''' mae评估指标 :param predict_results: :return: mae ''' length = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _mae_sum += abs(pred_rating - real_rating) return round(_mae_sum / length, 4) def rmse_mae(predict_results): ''' rmse和mae评估指标 :param predict_results: :return: rmse, mae ''' length = 0 _rmse_sum = 0 _mae_sum = 0 for uid, iid, real_rating, pred_rating in predict_results: length += 1 _rmse_sum += (pred_rating - real_rating) ** 2 _mae_sum += abs(pred_rating - real_rating) return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4) if method.lower() == "rmse": rmse(predict_results) elif method.lower() == "mae": mae(predict_results) else: return rmse_mae(predict_results)class BaselineCFByALS(object): def __init__(self, number_epochs, reg_bu, reg_bi, columns=["uid", "iid", "rating"]): # 梯度下降最高迭代次数 self.number_epochs = number_epochs # bu的正则参数 self.reg_bu = reg_bu # bi的正则参数 self.reg_bi = reg_bi # 数据集中user-item-rating字段的名称 self.columns = columns def fit(self, dataset): ''' :param dataset: uid, iid, rating :return: ''' self.dataset = dataset # 用户评分数据 self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] # 物品评分数据 self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] # 计算全局平均分 self.global_mean = self.dataset[self.columns[2]].mean() # 调用sgd方法训练模型参数 self.bu, self.bi = self.als() def als(self): ''' 利用随机梯度下降，优化bu，bi的值 :return: bu, bi ''' # 初始化bu、bi的值，全部设为0 bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings)))) bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings)))) for i in range(self.number_epochs): print("iter%d" % i) for iid, uids, ratings in self.items_ratings.itertuples(index=True): _sum = 0 for uid, rating in zip(uids, ratings): _sum += rating - self.global_mean - bu[uid] bi[iid] = _sum / (self.reg_bi + len(uids)) for uid, iids, ratings in self.users_ratings.itertuples(index=True): _sum = 0 for iid, rating in zip(iids, ratings): _sum += rating - self.global_mean - bi[iid] bu[uid] = _sum / (self.reg_bu + len(iids)) return bu, bi def predict(self, uid, iid): '''评分预测''' if iid not in self.items_ratings.index: raise Exception("无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据".format(uid=uid, iid=iid)) predict_rating = self.global_mean + self.bu[uid] + self.bi[iid] return predict_rating def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': trainset, testset = data_split("datasets/ml-latest-small/ratings.csv", random=True) bcf = BaselineCFByALS(20, 25, 15, ["userId", "movieId", "rating"]) bcf.fit(trainset) pred_results = bcf.test(testset) rmse, mae = accuray(pred_results) print("rmse: ", rmse, "mae: ", mae) 基于矩阵分解的CF算法矩阵分解发展史Traditional SVD: 通常SVD矩阵分解指的是SVD（奇异值）分解技术，在这我们姑且将其命名为Traditional SVD（传统并经典着）其公式如下： Traditional SVD分解的形式为3个矩阵相乘，中间矩阵为奇异值矩阵。如果想运用SVD分解的话，有一个前提是要求矩阵是稠密的，即矩阵里的元素要非空，否则就不能运用SVD分解。 很显然我们的数据其实绝大多数情况下都是稀疏的，因此如果要使用Traditional SVD，一般的做法是先用均值或者其他统计学方法来填充矩阵，然后再运用Traditional SVD分解降维，但这样做明显对数据的原始性造成一定影响。 FunkSVD（LFM） 刚才提到的Traditional SVD首先需要填充矩阵，然后再进行分解降维，同时存在计算复杂度高的问题，因为要分解成3个矩阵，所以后来提出了Funk SVD的方法，它不在将矩阵分解为3个矩阵，而是分解为2个用户-隐含特征，项目-隐含特征的矩阵，Funk SVD也被称为最原始的LFM模型 借鉴线性回归的思想，通过最小化观察数据的平方来寻求最优的用户和项目的隐含向量表示。同时为了避免过度拟合（Overfitting）观测数据，又提出了带有L2正则项的FunkSVD，上公式： 以上两种最优化函数都可以通过梯度下降或者随机梯度下降法来寻求最优解。 BiasSVD: 在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解： 它基于的假设和Baseline基准预测是一样的，但这里将Baseline的偏置引入到了矩阵分解中 SVD++: 人们后来又提出了改进的BiasSVD，被称为SVD++，该算法是在BiasSVD的基础上添加了用户的隐式反馈信息： 显示反馈指的用户的评分这样的行为，隐式反馈指用户的浏览记录、购买记录、收听记录等。 SVD++是基于这样的假设：在BiasSVD基础上，认为用户对于项目的历史浏览记录、购买记录、收听记录等可以从侧面反映用户的偏好。 基于矩阵分解的CF算法实现（一）：LFMLFM也就是前面提到的Funk SVD矩阵分解 LFM原理解析LFM(latent factor model)隐语义模型核心思想是通过隐含特征联系用户和物品，如下图： P矩阵是User-LF矩阵，即用户和隐含特征矩阵。LF有三个，表示共总有三个隐含特征。 Q矩阵是LF-Item矩阵，即隐含特征和物品的矩阵 R矩阵是User-Item矩阵，有P*Q得来 能处理稀疏评分矩阵 利用矩阵分解技术，将原始User-Item的评分矩阵（稠密/稀疏）分解为P和Q矩阵，然后利用$P*Q$还原出User-Item评分矩阵$R$。整个过程相当于降维处理，其中： 矩阵值$P_{11}$表示用户1对隐含特征1的权重值 矩阵值$Q_{11}$表示隐含特征1在物品1上的权重值 矩阵值$R_{11}$就表示预测的用户1对物品1的评分，且$R_{11}=\vec{P_{1,k}}\cdot \vec{Q_{k,1}}$ 利用LFM预测用户对物品的评分，$k$表示隐含特征数量：$$\begin{split}\hat {r}{ui} &amp;=\vec {p{uk}}\cdot \vec {q_{ik}}\&amp;={\sum_{k=1}}^k p_{uk}q_{ik}\end{split}$$因此最终，我们的目标也就是要求出P矩阵和Q矩阵及其当中的每一个值，然后再对用户-物品的评分进行预测。 损失函数同样对于评分预测我们利用平方差来构建损失函数：$$\begin{split}Cost &amp;= \sum_{u,i\in R} (r_{ui}-\hat{r}{ui})^2\&amp;=\sum{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2\end{split}$$加入L2正则化：$$Cost = \sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)$$对损失函数求偏导：$$\begin{split}\cfrac {\partial}{\partial p_{uk}}Cost &amp;= \cfrac {\partial}{\partial p_{uk}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}\\\cfrac {\partial}{\partial q_{ik}}Cost &amp;= \cfrac {\partial}{\partial q_{ik}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\lambda q_{ik}\end{split}$$ 随机梯度下降法优化梯度下降更新参数$p_{uk}$：$$\begin{split}p_{uk}&amp;:=p_{uk} - \alpha\cfrac {\partial}{\partial p_{uk}}Cost\&amp;:=p_{uk}-\alpha [2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}]\&amp;:=p_{uk}+\alpha [\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda p_{uk}]\end{split}$$ 同理：$$\begin{split}q_{ik}&amp;:=q_{ik} + \alpha[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda q_{ik}]\end{split}$$随机梯度下降： 向量乘法 每一个分量相乘 求和$$\begin{split}&amp;p_{uk}:=p_{uk}+\alpha [(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda_1 p_{uk}]\&amp;q_{ik}:=q_{ik} + \alpha[(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda_2 q_{ik}]\end{split}$$由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\lambda_1$和$\lambda_2$ 算法实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113'''LFM Model'''import pandas as pdimport numpy as np# 评分预测 1-5class LFM(object): def __init__(self, alpha, reg_p, reg_q, number_LatentFactors=10, number_epochs=10, columns=["uid", "iid", "rating"]): self.alpha = alpha # 学习率 self.reg_p = reg_p # P矩阵正则 self.reg_q = reg_q # Q矩阵正则 self.number_LatentFactors = number_LatentFactors # 隐式类别数量 self.number_epochs = number_epochs # 最大迭代次数 self.columns = columns def fit(self, dataset): ''' fit dataset :param dataset: uid, iid, rating :return: ''' self.dataset = pd.DataFrame(dataset) self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]] self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]] self.globalMean = self.dataset[self.columns[2]].mean() self.P, self.Q = self.sgd() def _init_matrix(self): ''' 初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值--防止×完全是0 :return: ''' # User-LF # random.rand(x)取值范围是 [0,1) P = dict(zip( self.users_ratings.index, np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32) )) # Item-LF Q = dict(zip( self.items_ratings.index, np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32) )) return P, Q def sgd(self): ''' 使用随机梯度下降，优化结果 :return: ''' P, Q = self._init_matrix() for i in range(self.number_epochs): print("iter%d"%i) error_list = [] for uid, iid, r_ui in self.dataset.itertuples(index=False): # User-LF P ## Item-LF Q v_pu = P[uid] #用户向量！！这是一个向量，PQ是矩阵 v_qi = Q[iid] #物品向量 err = np.float32(r_ui - np.dot(v_pu, v_qi)) v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu) v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi) P[uid] = v_pu Q[iid] = v_qi # 或者不从向量的角度考虑 # for k in range(self.number_of_LatentFactors): # v_pu[k] += self.alpha*(err*v_qi[k] - self.reg_p*v_pu[k]) # v_qi[k] += self.alpha*(err*v_pu[k] - self.reg_q*v_qi[k]) error_list.append(err ** 2) print(np.sqrt(np.mean(error_list))) return P, Q def predict(self, uid, iid): # 如果uid或iid不在，我们使用全剧平均分作为预测结果返回 if uid not in self.users_ratings.index or iid not in self.items_ratings.index: return self.globalMean p_u = self.P[uid] q_i = self.Q[iid] return np.dot(p_u, q_i) def test(self,testset): '''预测测试集数据''' for uid, iid, real_rating in testset.itertuples(index=False): try: pred_rating = self.predict(uid, iid) except Exception as e: print(e) else: yield uid, iid, real_rating, pred_ratingif __name__ == '__main__': dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)] dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype)) lfm = LFM(0.02, 0.01, 0.01, 10, 100, ["userId", "movieId", "rating"]) lfm.fit(dataset) while True: uid = input("uid: ") iid = input("iid: ") print(lfm.predict(int(uid), int(iid))) 基于内容的推荐算法（Content-Based）简介基于内容的推荐方法是非常直接的，它以物品的内容描述信息为依据来做出的推荐，本质上是基于对物品和用户自身的特征或属性的直接分析和计算。 例如，假设已知电影A是一部喜剧，而恰巧我们得知某个用户喜欢看喜剧电影，那么我们基于这样的已知信息，就可以将电影A推荐给该用户。 基于内容的推荐实现步骤 画像构建。顾名思义，画像就是刻画物品或用户的特征。本质上就是给用户或物品贴标签。 物品画像：例如给电影《战狼2》贴标签，可以有哪些？ “动作”、”吴京”、”吴刚”、”张翰”、”大陆电影”、”国产”、”爱国”、”军事”等等一系列标签是不是都可以贴上 用户画像：例如已知用户的观影历史是：”《战狼1》”、”《战狼2》”、”《建党伟业》”、”《建军大业》”、”《建国大业》”、”《红海行动》”、”《速度与激情1-8》”等，我们是不是就可以分析出该用户的一些兴趣特征如：”爱国”、”战争”、”赛车”、”动作”、”军事”、”吴京”、”韩三平”等标签。 问题：物品的标签来自哪儿？ PGC 物品画像–冷启动 物品自带的属性（物品一产生就具备的）：如电影的标题、导演、演员、类型等等 服务提供方设定的属性（服务提供方为物品附加的属性）：如短视频话题、微博话题（平台拟定） 其他渠道：如爬虫 UGC 冷启动问题 用户在享受服务过程中提供的物品的属性：如用户评论内容，微博话题（用户拟定） 根据PGC内容构建的物品画像的可以解决物品的冷启动问题 基于内容推荐的算法流程： 根据PGC/UGC内容构建物品画像 根据用户行为记录生成用户画像 根据用户画像从物品中寻找最匹配的TOP-N物品进行推荐 物品冷启动处理： 根据PGC内容构建物品画像 利用物品画像计算物品间两两相似情况 为每个物品产生TOP-N最相似的物品进行相关推荐：如与该商品相似的商品有哪些？与该文章相似文章有哪些？ 基于内容的电影推荐：物品画像物品画像构建步骤： 利用tags.csv中每部电影的标签作为电影的候选关键词 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签 将电影的分类词直接作为每部电影的画像标签 基于TF-IDF的特征提取技术(算一下关键程度)前面提到，物品画像的特征标签主要都是指的如电影的导演、演员、图书的作者、出版社等结构话的数据，也就是他们的特征提取，尤其是体征向量的计算是比较简单的，如直接给作品的分类定义0或者1的状态。 但另外一些特征，比如电影的内容简介、电影的影评、图书的摘要等文本数据，这些被称为非结构化数据，首先他们本应该也属于物品的一个特征标签，但是这样的特征标签进行量化时，也就是计算它的特征向量时是很难去定义的。 因此这时就需要借助一些自然语言处理、信息检索等技术，将如用户的文本评论或其他文本内容信息的非结构化数据进行量化处理，从而实现更加完善的物品画像/用户画像。 TF-IDF算法便是其中一种在自然语言处理领域中应用比较广泛的一种算法。可用来提取目标文档中，并得到关键词用于计算对于目标文档的权重，并将这些权重组合到一起得到特征向量。 算法原理TF-IDF自然语言处理领域中计算文档中词或短语的权值的方法，是词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）的乘积。TF指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被正规化，以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。IDF是一个词语普遍重要性的度量，某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。 TF-IDF算法基于一个这样的假设：若一个词语在目标文档中出现的频率高而在其他文档中出现的频率低，那么这个词语就可以用来区分出目标文档。这个假设需要掌握的有两点： 在本文档出现的频率高； 在其他文档出现的频率低。 因此，TF-IDF算法的计算可以分为词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）两部分，由TF和IDF的乘积来设置文档词语的权重。 TF指的是一个词语在文档中的出现频率。假设文档集包含的文档数为$$N$$，文档集中包含关键词$$k_i$$的文档数为$$n_i$$，$$f_{ij}$$表示关键词$$k_i$$在文档$$d_j$$中出现的次数，$$f_{dj}$$表示文档$$d_j$$中出现的词语总数，$$k_i$$在文档dj中的词频$$TF_{ij}$$定义为：$$TF_{ij}=\frac {f_{ij}}{f_{dj}}$$。并且注意，这个数字通常会被正规化，以防止它偏向长的文件（指同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。 IDF是一个词语普遍重要性的度量。表示某一词语在整个文档集中出现的频率，由它计算的结果取对数得到关键词$$k_i$$的逆文档频率$$IDF_i$$：$$IDF_i=log\frac {N}{n_i}$$ 由TF和IDF计算词语的权重为：$$w_{ij}=TF_{ij}$$·$$IDF_{i}=\frac {f_{ij}}{f_{dj}}$$·$$log\frac {N}{n_i}$$ 结论：TF-IDF与词语在文档中的出现次数成正比，与该词在整个文档集中的出现次数成反比。 用途：在目标文档中，提取关键词(特征标签)的方法就是将该文档所有词语的TF-IDF计算出来并进行对比，取其中TF-IDF值最大的k个数组成目标文档的特征向量用以表示文档。 注意：文档中存在的停用词（Stop Words），如“是”、“的”之类的，对于文档的中心思想表达没有意义的词，在分词时需要先过滤掉再计算其他词语的TF-IDF值。 算法举例对于计算影评的TF-IDF，以电影“加勒比海盗：黑珍珠号的诅咒”为例，假设它总共有1000篇影评，其中一篇影评的总词语数为200，其中出现最频繁的词语为“海盗”、“船长”、“自由”，分别是20、15、10次，并且这3个词在所有影评中被提及的次数分别为1000、500、100，就这3个词语作为关键词的顺序计算如下。 将影评中出现的停用词过滤掉，计算其他词语的词频。以出现最多的三个词为例进行计算如下： “海盗”出现的词频为20/200＝0.1 “船长”出现的词频为15/200=0.075 “自由”出现的词频为10/200=0.05； 计算词语的逆文档频率如下： “海盗”的IDF为：log(1000/1000)=0 “船长”的IDF为：log(1000/500)=0.3“自由”的IDF为：log(1000/100)=1 由1和2计算的结果求出词语的TF-IDF结果，“海盗”为0，“船长”为0.0225，“自由”为0.05。 通过对比可得，该篇影评的关键词排序应为：“自由”、“船长”、“海盗”。把这些词语的TF-IDF值作为它们的权重按照对应的顺序依次排列，就得到这篇影评的特征向量，我们就用这个向量来代表这篇影评，向量中每一个维度的分量大小对应这个属性的重要性。 将总的影评集中所有的影评向量与特定的系数相乘求和，得到这部电影的综合影评向量，与电影的基本属性结合构建视频的物品画像，同理构建用户画像，可采用多种方法计算物品画像和用户画像之间的相似度，为用户做出推荐。 加载数据集1234567891011121314151617181920212223242526272829303132333435363738import pandas as pdimport numpy as np'''- 利用tags.csv中每部电影的标签作为电影的候选关键词- 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签- 并将电影的分类词直接作为每部电影的画像标签'''def get_movie_dataset(): # 加载基于所有电影的标签 # all-tags.csv来自ml-latest数据集中 # 由于ml-latest-small中标签数据太多，因此借助其来扩充 _tags = pd.read_csv("datasets/ml-latest-small/all-tags.csv", usecols=range(1, 3)).dropna() tags = _tags.groupby("movieId").agg(list) # 加载电影列表数据集 movies = pd.read_csv("datasets/ml-latest-small/movies.csv", index_col="movieId") # 将类别词分开 movies["genres"] = movies["genres"].apply(lambda x: x.split("|")) # 为每部电影匹配对应的标签数据，如果没有将会是NAN movies_index = set(movies.index) &amp; set(tags.index) new_tags = tags.loc[list(movies_index)] ret = movies.join(new_tags) # 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段 # 如果电影没有标签数据，那么就替换为空列表 # map(fun,可迭代对象) movie_dataset = pd.DataFrame( map( lambda x: (x[0], x[1], x[2], x[2]+x[3]) if x[3] is not np.nan else (x[0], x[1], x[2], []), ret.itertuples()) , columns=["movieId", "title", "genres","tags"] ) movie_dataset.set_index("movieId", inplace=True) return movie_datasetmovie_dataset = get_movie_dataset()print(movie_dataset) 基于TF·IDF提取TOP-N关键词，构建电影画像1234567891011121314151617181920212223242526272829303132333435363738from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): ''' 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: ''' dataset = movie_dataset["tags"].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) movie_profile = &#123;&#125; for i, mid in enumerate(movie_dataset.index): # 根据每条数据返回，向量 vector = model[corpus[i]] # 按照TF-IDF值得到top-n的关键词 movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] # 根据关键词提取对应的名称--dct[x[0]]是找 在movie_tags中的索引 movie_profile[mid] = dict(map(lambda x:(dct[x[0]], x[1]), movie_tags)) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 完善画像关键词123456789101112131415161718192021222324252627282930313233343536373839404142434445from gensim.models import TfidfModelimport pandas as pdimport numpy as npfrom pprint import pprint# ......def create_movie_profile(movie_dataset): ''' 使用tfidf，分析提取topn关键词 :param movie_dataset: :return: ''' dataset = movie_dataset["tags"].values from gensim.corpora import Dictionary # 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取 dct = Dictionary(dataset) # 根据将每条数据，返回对应的词索引和词频 corpus = [dct.doc2bow(line) for line in dataset] # 训练TF-IDF模型，即计算TF-IDF值 model = TfidfModel(corpus) _movie_profile = [] for i, data in enumerate(movie_dataset.itertuples()): mid = data[0] title = data[1] genres = data[2] vector = model[corpus[i]] movie_tags = sorted(vector, key=lambda x: x[1], reverse=True)[:30] topN_tags_weights = dict(map(lambda x: (dct[x[0]], x[1]), movie_tags)) # 将类别词的添加进去，并设置权重值为1.0 for g in genres: topN_tags_weights[g] = 1.0 topN_tags = [i[0] for i in topN_tags_weights.items()] _movie_profile.append((mid, title, topN_tags, topN_tags_weights)) movie_profile = pd.DataFrame(_movie_profile, columns=["movieId", "title", "profile", "weights"]) movie_profile.set_index("movieId", inplace=True) return movie_profilemovie_dataset = get_movie_dataset()pprint(create_movie_profile(movie_dataset)) 为了根据指定关键词迅速匹配到对应的电影，因此需要对物品画像的标签词，建立倒排索引 倒排索引介绍 通常数据存储数据，都是以物品的ID作为索引，去提取物品的其他信息数据 而倒排索引就是用物品的其他数据作为索引，去提取它们对应的物品的ID列表 123456789101112131415161718# ......'''建立tag-物品的倒排索引'''def create_inverted_table(movie_profile): inverted_table = &#123;&#125; for mid, weights in movie_profile["weights"].iteritems(): for tag, weight in weights.items(): #到inverted_table dict 用tag作为Key去取值 如果取不到就返回[] _ = inverted_table.get(tag, []) _.append((mid, weight)) inverted_table.setdefault(tag, _) return inverted_tableinverted_table = create_inverted_table(movie_profile)pprint(inverted_table) 基于内容的电影推荐：用户画像用户画像构建步骤： 根据用户的评分历史，结合物品画像，将有观影记录的电影的画像标签作为初始标签反打到用户身上 通过对用户观影标签的次数进行统计，计算用户的每个初始标签的权重值，排序后选取TOP-N作为用户最终的画像标签 用户画像建立1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdimport numpy as npfrom gensim.models import TfidfModelfrom functools import reduceimport collectionsfrom pprint import pprint# ......'''user profile画像建立：1. 提取用户观看列表2. 根据观看列表和物品画像为用户匹配关键词，并统计词频3. 根据词频排序，最多保留TOP-k个词，这里K设为100，作为用户的标签'''def create_user_profile(): watch_record = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(2), dtype=&#123;"userId":np.int32, "movieId": np.int32&#125;) watch_record = watch_record.groupby("userId").agg(list) # print(watch_record) movie_dataset = get_movie_dataset() movie_profile = create_movie_profile(movie_dataset) user_profile = &#123;&#125; for uid, mids in watch_record.itertuples(): record_movie_prifole = movie_profile.loc[list(mids)] counter = collections.Counter(reduce(lambda x, y: list(x)+list(y), record_movie_prifole["profile"].values)) # 兴趣词 interest_words = counter.most_common(50) maxcount = interest_words[0][1] interest_words = [(w,round(c/maxcount, 4)) for w,c in interest_words] user_profile[uid] = interest_words return user_profileuser_profile = create_user_profile()pprint(user_profile) 基于内容的电影推荐：为用户产生TOP-N推荐结果1234567891011121314151617181920212223242526272829# ......user_profile = create_user_profile()watch_record = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(2),dtype=&#123;"userId": np.int32, "movieId": np.int32&#125;)watch_record = watch_record.groupby("userId").agg(list)for uid, interest_words in user_profile.items(): result_table = &#123;&#125; # 电影id:[0.2,0.5,0.7] for interest_word, interest_weight in interest_words: related_movies = inverted_table[interest_word] for mid, related_weight in related_movies: _ = result_table.get(mid, []) _.append(interest_weight) # 只考虑用户的兴趣程度 # _.append(related_weight) # 只考虑兴趣词与电影的关联程度 # _.append(interest_weight*related_weight) # 二者都考虑 result_table.setdefault(mid, _) # 为什么要加起来是因为一个电影有多个标签 rs_result = map(lambda x: (x[0], sum(x[1])), result_table.items()) rs_result = sorted(rs_result, key=lambda x:x[1], reverse=True)[:100] print(uid) pprint(rs_result) break # 历史数据 ==&gt; 历史兴趣程度 ==&gt; 历史推荐结果 离线推荐 离线计算 # 在线推荐 ===&gt; 娱乐(王思聪) ===&gt; 我 ==&gt; 王思聪 100% # 近线：最近1天、3天、7天 实时计算 基于内容推荐流程总结① 建立物品画像 来源：①用户打tag ②电影的分类值 根据电影的id 把tag和分类值合并起来 求tf-idf 根据tf-idf的结果 为每一部电影筛选出 top-n（tf-idf比较大的）个关键词 电影id-关键词([&quot;profile&quot;])-关键词权重([&quot;weigts&quot;]) ② 建立倒排索引 通过关键词找到电影 遍历 电影id-关键词-关键词权重 数据， 读取每一个关键词，用关键词作为key [(关键词对应的电影id和tf-idf)] 作为value 保存到dict当中 ③ 用户画像 看用户看过那些电影， 到电影的 电影id-关键词-关键词权重 数据中 找到电影所对应的关键词 把用户看过的所有的关键词放到一起 统计词频 每个词出现了几次 出现次数多的关键词 作为用户的兴趣词，这个兴趣词实际上就是用户画像的关键词 ④ 根据用户的兴趣词 找到兴趣词对应的电影 多个兴趣词可能对应一个电影 {电影id：[关键词1权重，关键词2权重]} 把每一个部电影对应的关键词权重求和之后 排序 权重比较高的排在前面 推荐给用户 介绍一下各种命令hadoop/hdfsHDFS shell操作 调用文件系统(FS)Shell命令应使用 bin/hadoop fs 的形式 ls使用方法：hadoop fs -ls 如果是文件，则按照如下格式返回文件信息：文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：目录名 修改日期 修改时间 权限 用户ID 组ID示例：hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile返回值：成功返回0，失败返回-1。 text使用方法：hadoop fs -text 将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。 mv使用方法：hadoop fs -mv URI [URI …] 将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。示例： hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1 返回值： 成功返回0，失败返回-1。 put使用方法：hadoop fs -put … 从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。 hadoop fs -put localfile /user/hadoop/hadoopfile hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile hadoop fs -put - hdfs://host:port/hadoop/hadoopfile从标准输入中读取输入。 返回值： 成功返回0，失败返回-1。 rm使用方法：hadoop fs -rm URI [URI …] 删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。示例： hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir 返回值： 成功返回0，失败返回-1。 http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html HDFS shell操作练习 在centos 中创建 test.txt 1touch test.txt 在centos中为test.txt 添加文本内容 1vi test.txt 在HDFS中创建 hadoop001/test 文件夹 1hadoop fs -mkdir -p /hadoop001/test 把text.txt文件上传到HDFS中 1hadoop fs -put test.txt /hadoop001/test/ 查看hdfs中 hadoop001/test/test.txt 文件内容 1hadoop fs -cat /hadoop001/test/test.txt 将hdfs中 hadoop001/test/test.txt文件下载到centos 1hadoop fs -get /hadoop001/test/test.txt test.txt 删除HDFS中 hadoop001/test/ hadoop fs -rm -r /hadoop001]]></content>
      <tags>
        <tag>python</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy&pandas&机器学习算法]]></title>
    <url>%2F2020%2F03%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[机器学习工作流程学习目标 了解机器学习的定义 知道机器学习的工作流程 掌握获取到的数据集的特性 1 什么是机器学习机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。 2 机器学习工作流程 机器学习工作流程总结 1.获取数据 2.数据基本处理 3.特征工程 4.机器学习(模型训练) 5.模型评估 结果达到要求，上线服务 没有达到要求，重新上面步骤 2.1 获取到的数据集介绍 数据简介 在数据集中一般： 一行数据我们称为一个样本 一列数据我们成为一个特征 有些数据有目标值（标签值），有些数据没有目标值（如上表中，电影类型就是这个数据集的目标值） 数据类型构成： 数据类型一：特征值+目标值（目标值是连续的和离散的） 数据类型二：只有特征值，没有目标值 数据分割： 机器学习一般的数据集会划分为两个部分： 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 划分比例： 训练集：70% 80% 75% 测试集：30% 20% 25% 2.2 数据基本处理 即对数据进行缺失值、去除异常值等处理 2.3 特征工程2.3.1什么是特征工程特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。 意义：会直接影响机器学习的效果 2.3.2 为什么需要特征工程(Feature Engineering) 机器学习领域的大神Andrew Ng(吴恩达)老师说“Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. ” 注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 2.3.3 特征工程包含内容 特征提取 特征预处理 特征降维 2.3.4 各概念具体解释 特征提取 将任意数据（如文本或图像）转换为可用于机器学习的数字特征 特征预处理 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 特征降维 指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程 2.4 机器学习选择合适的算法对模型进行训练 2.5 模型评估对训练好的模型进行评估 3 小结 机器学习定义【掌握】 机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测 机器学习工作流程总结【掌握】 1.获取数据 2.数据基本处理 3.特征工程 4.机器学习(模型训练) 5.模型评估 结果达到要求，上线服务 没有达到要求，重新上面步骤 获取到的数据集介绍【掌握】 数据集中一行数据一般称为一个样本，一列数据一般称为一个特征。 数据集的构成： 由特征值+目标值（部分数据集没有）构成 为了模型的训练和测试，把数据集分为： 训练数据（70%-80%）和测试数据（20%-30%） 特征工程包含内容【了解】 特征提取 特征预处理 特征降维 1.5 机器学习算法分类学习目标 了解机器学习常用算法的分类 根据数据集组成不同，可以把机器学习算法分为： 监督学习 无监督学习 半监督学习 强化学习 1 监督学习 定义： 输入数据是由输入特征值和目标值所组成。 函数的输出可以是一个连续的值(称为回归）， 或是输出是有限个离散值（称作分类）。 1.1 回归问题例如：预测房价，根据样本集拟合出一条连续曲线。 1.2 分类问题例如：根据肿瘤特征判断良性还是恶性，得到的是结果是“良性”或者“恶性”，是离散的。 2 无监督学习 定义： 输入数据是由输入特征值组成，没有目标值 输入数据没有被标记，也没有确定的结果。样本数据类别未知； 需要根据样本间的相似性对样本集进行类别划分。 有监督，无监督算法对比： 3 半监督学习 定义： 训练集同时包含有标记样本数据和未标记样本数据。 举例： 监督学习训练方式： 半监督学习训练方式(少) 4 强化学习(动态过程–上一步的输出是下一步的输入) 定义： 实质是make decisions 问题，即自动进行决策，并且可以做连续决策。 举例： 小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。 小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。 主要包含五个元素：agent, action, reward, environment, observation； 强化学习的目标就是获得最多的累计奖励。 监督学习和强化学习的对比 监督学习 强化学习 反馈映射 输出的是之间的关系，可以告诉算法什么样的输入对应着什么样的输出。 输出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。 反馈时间 做了比较坏的选择会立刻反馈给算法。 结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏。 输入特征 输入是独立同分布的。 面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入。 5 小结 In Out 目的 案例 监督学习 (supervised learning) 有标签 有反馈 预测结果 猫狗分类 房价预测 无监督学习 (unsupervised learning) 无标签 无反馈 发现潜在结构 “物以类聚，人以群分” 半监督学习 (Semi-Supervised Learning) 部分有标签，部分无标签 有反馈 降低数据标记的难度 强化学习 (reinforcement learning) 决策流程及激励系统 一系列行动 长期利益最大化 学下棋 1.6 模型评估学习目标 目标 了解机器学习中模型评估的方法 知道过拟合、欠拟合发生情况 模型评估是模型开发过程不可或缺的一部分。它有助于发现表达数据的最佳模型和所选模型将来工作的性能如何。 按照数据集的目标值不同，可以把模型评估分为分类模型评估和回归模型评估。 1 分类模型评估 准确率 预测正确的数占样本总数的比例。 其他评价指标：精确率、召回率、F1-score、AUC指标等 2 回归模型评估均方根误差（Root Mean Squared Error，RMSE） RMSE是一个衡量回归模型误差率的常用公式。 不过，它仅能比较误差是相同单位的模型。 举例： 123假设上面的房价预测，只有五个样本，对应的真实值为：100,120,125,230,400预测值为：105,119,120,230,410 那么使用均方根误差求解得：$$RMSE=\sqrt[2]{\frac{[(100-105)^2+(120-119)^2+5^2+0^2+10^2]}{5}} =5.495$$ 其他评价指标：相对平方误差（Relative Squared Error，RSE）、平均绝对误差（Mean Absolute Error，MAE)、相对绝对误差（Relative Absolute Error，RAE)3 拟合模型评估用于评价训练好的的模型的表现效果，其表现效果大致可以分为两类：过拟合、欠拟合。 在训练过程中，你可能会遇到如下问题： 训练数据训练的很好啊，误差也不大，为什么在测试集上面有问题呢？当算法在某个数据集当中出现这种情况，可能就出现了拟合问题。 3.1 欠拟合 因为机器学习到的天鹅特征太少了，导致区分标准太粗糙，不能准确识别出天鹅。 欠拟合（under-fitting）：模型学习的太过粗糙，连训练集中的样本数据特征关系都没有学出来。 3.2 过拟合 机器已经基本能区别天鹅和其他动物了。然后，很不巧已有的天鹅图片全是白天鹅的，于是机器经过学习后，会认为天鹅的羽毛都是白的，以后看到羽毛是黑的天鹅就会认为那不是天鹅。 过拟合（over-fitting）：所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在测试数据集中表现不佳。 上问题解答： 训练数据训练的很好啊，误差也不大，为什么在测试集上面有问题呢？ 4 小结 分类模型评估【了解】 准确率 回归模型评估【了解】 RMSE – 均方根误差 拟合【知道】 举例 – 判断是否是人 欠拟合 学习到的东西太少 模型学习的太过粗糙 过拟合 学习到的东西太多 学习到的特征多，不好泛化 Numpy4.1 Numpy优势 1 Numpy介绍Numpy（Numerical Python）是一个开源的Python科学计算库，用于快速处理任意维度的数组。 Numpy支持常见的数组和矩阵操作。对于同样的数值计算任务，使用Numpy比直接使用Python要简洁的多。 Numpy使用ndarray对象来处理多维数组，该对象是一个快速而灵活的大数据容器。 2 ndarray介绍12NumPy provides an N-dimensional array type, the ndarray, which describes a collection of “items” of the same type. NumPy提供了一个N维数组类型ndarray，它描述了相同类型的“items”的集合。 用ndarray进行存储： 1234567891011121314import numpy as np# 创建ndarrayscore = np.array([[80, 89, 86, 67, 79],[78, 97, 89, 67, 81],[90, 94, 78, 67, 74],[91, 91, 90, 67, 69],[76, 87, 75, 67, 86],[70, 79, 84, 67, 84],[94, 92, 93, 67, 64],[86, 85, 83, 67, 80]])score 返回结果： 12345678array([[80, 89, 86, 67, 79], [78, 97, 89, 67, 81], [90, 94, 78, 67, 74], [91, 91, 90, 67, 69], [76, 87, 75, 67, 86], [70, 79, 84, 67, 84], [94, 92, 93, 67, 64], [86, 85, 83, 67, 80]]) 提问: 使用Python列表可以存储一维数组，通过列表的嵌套可以实现多维数组，那么为什么还需要使用Numpy的ndarray呢？ 3 ndarray与Python原生list运算效率对比在这里我们通过一段代码运行来体会到ndarray的好处 12345678910111213import randomimport timeimport numpy as npa = []for i in range(100000000): a.append(random.random())# 通过%time魔法方法, 查看当前行的代码运行一次所花费的时间%time sum1=sum(a)b=np.array(a)%time sum2=np.sum(b) 其中第一个时间显示的是使用原生Python计算时间,第二个内容是使用numpy计算时间: 1234CPU times: user 852 ms, sys: 262 ms, total: 1.11 sWall time: 1.13 sCPU times: user 133 ms, sys: 653 µs, total: 133 msWall time: 134 ms 从中我们看到ndarray的计算速度要快很多，节约了时间。 机器学习的最大特点就是大量的数据运算，那么如果没有一个快速的解决方案，那可能现在python也在机器学习领域达不到好的效果。 Numpy专门针对ndarray的操作和运算进行了设计，所以数组的存储效率和输入输出性能远优于Python中的嵌套列表，数组越大，Numpy的优势就越明显。 4 ndarray的优势4.1 内存块风格ndarray到底跟原生python列表有什么不同呢，请看一张图： 从图中我们可以看出ndarray在存储数据的时候，数据与数据的地址都是连续的，这样就给使得批量操作数组元素时速度更快。 这是因为ndarray中的所有元素的类型都是相同的，而Python列表中的元素类型是任意的，所以ndarray在存储元素时内存可以连续，而python原生list就只能通过寻址方式找到下一个元素，这虽然也导致了在通用性能方面Numpy的ndarray不及Python原生list，但在科学计算中，Numpy的ndarray就可以省掉很多循环语句，代码使用方面比Python原生list简单的多。 4.2 ndarray支持并行化运算（向量化运算）numpy内置了并行运算功能，当系统有多个核心时，做某种计算时，numpy会自动做并行计算 4.3 效率远高于纯Python代码Numpy底层使用C语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制，所以，其效率远高于纯Python代码。 5 小结 numpy介绍【了解】 一个开源的Python科学计算库 计算起来要比python简洁高效 Numpy使用ndarray对象来处理多维数组 ndarray介绍【了解】 NumPy提供了一个N维数组类型ndarray，它描述了相同类型的“items”的集合。 生成numpy对象:np.array() ndarray的优势【掌握】 内存块风格 list – 分离式存储,存储内容多样化 ndarray – 一体式存储,存储类型必须一样 ndarray支持并行化运算（向量化运算） ndarray底层是用C语言写的,效率更高,释放了GIL 4.2 N维数组-ndarray1 ndarray的属性数组属性反映了数组本身固有的信息。 属性名字 属性解释 ndarray.shape 数组维度的元组 ndarray.ndim 数组维数 ndarray.size 数组中的元素数量 ndarray.itemsize 一个数组元素的长度（字节） ndarray.dtype 数组元素的类型 2 ndarray的形状首先创建一些数组。 1234# 创建不同形状的数组&gt;&gt;&gt; a = np.array([[1,2,3],[4,5,6]])&gt;&gt;&gt; b = np.array([1,2,3,4])&gt;&gt;&gt; c = np.array([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]]) 分别打印出形状 1234567&gt;&gt;&gt; a.shape&gt;&gt;&gt; b.shape&gt;&gt;&gt; c.shape(2, 3) # 二维数组(4,) # 一维数组(2, 2, 3) # 三维数组 3 ndarray的类型123&gt;&gt;&gt; type(score.dtype)&lt;type 'numpy.dtype'&gt; dtype是numpy.dtype类型，先看看对于数组来说都有哪些类型 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True或False） ‘b’ np.int8 一个字节大小，-128 至 127 ‘i’ np.int16 整数，-32768 至 32767 ‘i2’ np.int32 整数，-2^31 至 2^32 -1 ‘i4’ np.int64 整数，-2^63 至 2^63 - 1 ‘i8’ np.uint8 无符号整数，0 至 255 ‘u’ np.uint16 无符号整数，0 至 65535 ‘u2’ np.uint32 无符号整数，0 至 2^32 - 1 ‘u4’ np.uint64 无符号整数，0 至 2^64 - 1 ‘u8’ np.float16 半精度浮点数：16位，正负号1位，指数5位，精度10位 ‘f2’ np.float32 单精度浮点数：32位，正负号1位，指数8位，精度23位 ‘f4’ np.float64 双精度浮点数：64位，正负号1位，指数11位，精度52位 ‘f8’ np.complex64 复数，分别用两个32位浮点数表示实部和虚部 ‘c8’ np.complex128 复数，分别用两个64位浮点数表示实部和虚部 ‘c16’ np.object_ python对象 ‘O’ np.string_ 字符串 ‘S’ np.unicode_ unicode类型 ‘U’ 创建数组的时候指定类型 1234567&gt;&gt;&gt; a = np.array([[1, 2, 3],[4, 5, 6]], dtype=np.float32)&gt;&gt;&gt; a.dtypedtype('float32')&gt;&gt;&gt; arr = np.array(['python', 'tensorflow', 'scikit-learn', 'numpy'], dtype = np.string_)&gt;&gt;&gt; arrarray([b'python', b'tensorflow', b'scikit-learn', b'numpy'], dtype='|S12') 注意：若不指定，整数默认int64，小数默认float64 4.3 基本操作1 生成数组的方法1.1 生成0和1的数组 np.ones(shape, dtype) np.ones_like(a, dtype) （a是一个array，按照a的样式生成） np.zeros(shape, dtype) np.zeros_like(a, dtype) 12ones = np.ones([4,8])ones 返回结果: 12345array([[1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1.]])np.zeros_like(ones) 返回结果: 1234array([[0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0.]]) 1.2 从现有数组生成1.2.1 生成方式 np.array(object, dtype) np.asarray(a, dtype) 12345a = np.array([[1,2,3],[4,5,6]])# 从现有的数组当中创建a1 = np.array(a)# 相当于索引的形式，并没有真正的创建一个新的a2 = np.asarray(a) 1.2.2 关于array和asarray的不同array是深拷贝 – 相当于复制了一份 asarray是浅拷贝 – 相当于创建了一个快捷方式 1.3 生成固定范围的数组1.3.1 np.linspace (start, stop, num, endpoint) 创建等差数组 — 指定数量 参数: start:序列的起始值 stop:序列的终止值 num:要生成的等间隔样例数量，默认为50 endpoint:序列中是否包含stop值，默认为ture 12# 生成等间隔的数组np.linspace(0, 100, 11) 返回结果： 1array([ 0., 10., 20., 30., 40., 50., 60., 70., 80., 90., 100.]) 1.3.2 np.arange(start,stop, step, dtype) 创建等差数组 — 指定步长 参数 step:步长,默认值为1 1np.arange(10, 50, 2) 返回结果： 12array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]) 1.3.3 np.logspace(start,stop, num) 创建等比数列 参数: num:要生成的等比数列数量，默认为50 12# 生成10^xnp.logspace(0, 2, 3) 返回结果: 1array([ 1., 10., 100.]) 1.4 生成随机数组1.4.1 使用模块介绍 np.random模块 1.4.2 正态分布一、基础概念复习：正态分布（理解）a. 什么是正态分布正态分布是一种概率分布。正态分布是具有两个参数μ和σ的连续型随机变量的分布，第一参数μ是服从正态分布的随机变量的均值，第二个参数σ是此随机变量的方差，所以正态分布记作N(μ，σ )。 b. 正态分布的应用生活、生产与科学实验中很多随机变量的概率分布都可以近似地用正态分布来描述。 c. 正态分布特点μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。 标准差如何来？ 方差 是在概率论和统计方差衡量一组数据时离散程度的度量 其中M为平均值，n为数据总个数，σ 为标准差，σ ^2可以理解一个整体为方差 标准差与方差的意义 可以理解成数据的一个离散程度的衡量 二、正态分布创建方式 np.random.randn(d0, d1, …, dn) 功能：从标准正态分布中返回一个或多个样本值 np.random.normal(*loc=0.0*, *scale=1.0*, *size=None*) loc：float 此概率分布的均值（对应着整个分布的中心centre） scale：float 此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高） size：int or tuple of ints 输出的shape，默认为None，只输出一个值 np.random.standard_normal(size=None) 返回指定形状的标准正态分布的数组。 举例1：生成均值为1.75，标准差为1的正态分布数据，100000000个1x1 = np.random.normal(1.75, 1, 100000000) 返回结果： 1234567891011121314array([2.90646763, 1.46737886, 2.21799024, ..., 1.56047411, 1.87969135, 0.9028096 ])# 生成均匀分布的随机数x1 = np.random.normal(1.75, 1, 100000000)# 画图看分布状况# 1）创建画布plt.figure(figsize=(20, 10), dpi=100)# 2）绘制直方图plt.hist(x1, 1000)# 3）显示图像plt.show() 例如：我们可以模拟生成一组股票的涨跌幅的数据 举例2：随机生成4支股票1周的交易日涨幅数据4支股票，一周(5天)的涨跌幅数据，如何获取？ 随机生成涨跌幅在某个正态分布内，比如均值0，方差1 股票涨跌幅数据的创建123# 创建符合正态分布的4只股票5天的涨跌幅数据stock_change = np.random.normal(0, 1, (4, 5))stock_change 返回结果： 1234array([[ 0.0476585 , 0.32421568, 1.50062162, 0.48230497, -0.59998822], [-1.92160851, 2.20430374, -0.56996263, -1.44236548, 0.0165062 ], [-0.55710486, -0.18726488, -0.39972172, 0.08580347, -1.82842225], [-1.22384505, -0.33199305, 0.23308845, -1.20473702, -0.31753223]]) 1.4.2 均匀分布 np.random.rand( d0 , d1 , … , dn ) 返回[0.0，1.0)内的一组均匀分布的数。 np.random.uniform(low=0.0, high=1.0, size=None) 功能：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high. 参数介绍: low: 采样下界，float类型，默认值为0； high: 采样上界，float类型，默认值为1； size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出mnk个样本，缺省时输出1个值。 返回值：ndarray类型，其形状和参数size中描述一致。 np.random.randint( low , high=None , size=None , dtype=’l’ ) 从一个均匀分布中随机采样，生成一个整数或N维整数数组， 取数范围：若high不为None时，取[low,high)之间随机整数，否则取值[0,low)之间随机整数。 12# 生成均匀分布的随机数x2 = np.random.uniform(-1, 1, 100000000) 返回结果： 12array([ 0.22411206, 0.31414671, 0.85655613, ..., -0.92972446,0.95985223, 0.23197723]) 画图看分布状况： 1234567891011121314import matplotlib.pyplot as plt# 生成均匀分布的随机数x2 = np.random.uniform(-1, 1, 100000000)# 画图看分布状况# 1）创建画布plt.figure(figsize=(10, 10), dpi=100)# 2）绘制直方图plt.hist(x=x2, bins=1000) # x代表要使用的数据，bins表示要划分区间数# 3）显示图像plt.show() 2 数组的索引、切片一维、二维、三维的数组如何索引？ 直接进行索引,切片 对象[:, :] – 先行后列 二维数组索引方式： 举例：获取第一个股票的前3个交易日的涨跌幅数据 12# 二维的数组，两个维度 stock_change[0, 0:3] 返回结果： 1array([-0.03862668, -1.46128096, -0.75596237]) 三维数组索引方式： 12345678910# 三维a1 = np.array([ [[1,2,3],[4,5,6]], [[12,3,34],[5,6,7]]])# 返回结果array([[[ 1, 2, 3], [ 4, 5, 6]], [[12, 3, 34], [ 5, 6, 7]]])# 索引、切片&gt;&gt;&gt; a1[0, 0, 1] # 输出: 2 3 形状修改3.1 ndarray.reshape(shape, order) 返回一个具有相同数据域，但shape不一样的视图 行、列不进行互换 123# 在转换形状的时候，一定要注意数组的元素匹配stock_change.reshape([5, 4])stock_change.reshape([-1,10]) # 数组的形状被修改为: (2, 10), -1: 表示通过待计算 3.2 ndarray.resize(new_shape) 修改数组本身的形状（需要保持元素个数前后相同） 行、列不进行互换 12345stock_change.resize([5, 4])# 查看修改后结果stock_change.shape(5, 4) 3.3 ndarray.T 数组的转置 将数组的行、列进行互换 12stock_change.T.shape(4, 5) 4 类型修改4.1 ndarray.astype(type) 返回修改了类型之后的数组 1stock_change.astype(np.int32) 4.2 ndarray.tostring([order])或者ndarray.tobytes([order]) 构造包含数组中原始数据字节的Python字节 12arr = np.array([[[1, 2, 3], [4, 5, 6]], [[12, 3, 34], [5, 6, 7]]])arr.tostring() 4.3 jupyter输出太大可能导致崩溃问题【了解】如果遇到 12345IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`. 这个问题是在jupyer当中对输出的字节数有限制，需要去修改配置文件 创建配置文件 12jupyter notebook --generate-configvi ~/.jupyter/jupyter_notebook_config.py 取消注释,多增加 123## (bytes/sec) Maximum rate at which messages can be sent on iopub before they# are limited.c.NotebookApp.iopub_data_rate_limit = 10000000 但是不建议这样去修改，jupyter输出太大会崩溃 5 数组的去重5.1 np.unique()123temp = np.array([[1, 2, 3, 4],[3, 4, 5, 6]])&gt;&gt;&gt; np.unique(temp)array([1, 2, 3, 4, 5, 6]) 6 小结 创建数组【掌握】 生成0和1的数组 np.ones() np.ones_like() 从现有数组中生成 np.array – 深拷贝 np.asarray – 浅拷贝 生成固定范围数组 np.linspace() nun – 生成等间隔的多少个 np.arange() step – 每间隔多少生成数据 np.logspace() 生成以10的N次幂的数据 生层随机数组 正态分布 里面需要关注的参数:均值:u, 标准差:σ u – 决定了这个图形的左右位置 σ – 决定了这个图形是瘦高还是矮胖 np.random.randn() np.random.normal(0, 1, 100) 均匀 np.random.rand() np.random.uniform(0, 1, 100) np.random.randint(0, 10, 10) 数组索引【知道】 直接进行索引,切片 对象[:, :] – 先行后列 数组形状改变【掌握】 对象.reshape() 没有进行行列互换,新产生一个ndarray 对象.resize() 没有进行行列互换,修改原来的ndarray 对象.T 进行了行列互换 数组去重【知道】 np.unique(对象) K-近邻算法1.1 K-近邻算法(KNN)概念K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法 定义 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 来源：KNN算法最早是由Cover和Hart提出的一种分类算法 距离公式 两个样本的距离可以通过如下公式计算，又叫欧式距离 1.2 k近邻算法api(scikit-learn)初步使用机器学习流程复习： 1.获取数据集 2.数据基本处理 3.特征工程 4.机器学习 5.模型评估 一个简单的例子： 1234567891011121314151617# coding:utf-8from sklearn.neighbors import KNeighborsClassifier# 获取数据（y代表类别）x = [[1], [2], [0], [0]]y = [1, 1, 0, 0]# 机器学习# 1.实例化一个训练模型estimator = KNeighborsClassifier(n_neighbors=2)# 2.调用fit方法进行训练estimator.fit(x, y)# 预测其他值ret = estimator.predict([[-1]])print(ret) 1.3 距离度量1 欧式距离(Euclidean Distance)： 2 曼哈顿距离(Manhattan Distance)：在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。 3 切比雪夫距离 (Chebyshev Distance)：国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离。 XY轴的最大值 4 闵可夫斯基距离(Minkowski Distance)：闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。 两个n维变量a(x11,x12,…,x1n)与b(x21,x22,…,x2n)间的闵可夫斯基距离定义为： 其中p是一个变参数： 当p=1时，就是曼哈顿距离； 当p=2时，就是欧氏距离； 当p→∞时，就是切比雪夫距离。(相当于p次方的时候把小的值都忽略了) 根据p的不同，闵氏距离可以表示某一类/种的距离。 小结： 1 闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点: e.g. 二维样本(身高[单位:cm],体重[单位:kg]),现有三个样本：a(180,50)，b(190,50)，c(180,60)。 a与b的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c的闵氏距离。但实际上身高的10cm并不能和体重的10kg划等号。 2 闵氏距离的缺点： (1)将各个分量的量纲(scale)，也就是“单位”相同的看待了; (2)未考虑各个分量的分布（期望，方差等）可能是不同的。 5 标准化欧氏距离 (Standardized EuclideanDistance)：标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。 思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。假设样本集X的均值(mean)为m，标准差(standard deviation)为s，X的“标准化变量”表示为： 如果将方差的倒数看成一个权重，也可称之为加权欧氏距离(Weighted Euclidean distance)。 举例: 123X=[[1,1],[2,2],[3,3],[4,4]];（假设两个分量的标准差分别为0.5和1）经计算得:d = 2.2361 4.4721 6.7082 2.2361 4.4721 2.2361 6 余弦距离(Cosine Distance)几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。 二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式： 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦为： 即： 夹角余弦取值范围为[-1,1]。余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个向量的方向重合时余弦取最大值1，当两个向量的方向完全相反余弦取最小值-1。 举例: 123X=[[1,1],[1,2],[2,5],[1,-4]]经计算得:d = 0.9487 0.9191 -0.5145 0.9965 -0.7593 -0.8107 7 杰卡德距离(Jaccard Distance)：杰卡德相似系数(Jaccard similarity coefficient)：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示： 杰卡德距离(Jaccard Distance)：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度： 举例: 1234X=[[1,1,0][1,-1,0],[-1,1,0]]注：以下计算中，把杰卡德距离定义为不同的维度的个数占“非全零维度”的**比例**经计算得:d = 0.5000 0.5000 1.0000 1.4 k值的选择K值过小： 容易受到异常点的影响 k值过大： 受到样本均衡的问题 K值选择问题，李航博士的一书「统计学习方法」上所说： 1) 选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 2) 选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 3) K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。对这个简单的分类器进行泛化，用核方法把这个线性模型扩展到非线性的情况，具体方法是把低维数据集映射到高维特征空间。 近似误差：对现有训练集的训练误差，关注训练集，如果近似误差过小可能会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。 估计误差：可以理解为对测试集的测试误差，关注测试集，估计误差小说明对未知数据的预测能力好，模型本身最接近最佳模型。 1.5 kd树问题导入： 实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。 这在特征空间的维数大及训练数据容量大时尤其必要。 k近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找K近邻。当训练集很大时，计算非常耗时。 为了提高kNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数。 1 kd树简介1.1 什么是kd树根据KNN每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。当数据集很大时，这个计算成本非常高，针对N个样本，D个特征的数据集，其算法复杂度为O（DN^2）。 kd树：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，如果A和B距离很远，B和C距离很近，那么A和C的距离也很远。有了这个信息，就可以在合适的时候跳过距离远的点。 这样优化后的算法复杂度可降低到O（DNlog（N））。感兴趣的读者可参阅论文：Bentley，J.L.，Communications of the ACM（1975）。 1989年，另外一种称为Ball Tree的算法，在kd Tree的基础上对性能进一步进行了优化。感兴趣的读者可以搜索Five balltree construction algorithms来了解详细的算法信息。 1.2 原理 黄色的点作为根节点，上面的点归左子树，下面的点归右子树，接下来再不断地划分，分割的那条线叫做分割超平面（splitting hyperplane），在一维中是一个点，二维中是线，三维的是面。 黄色节点就是Root节点，下一层是红色，再下一层是绿色，再下一层是蓝色。 1.树的建立； 2.最近邻域搜索（Nearest-Neighbor Lookup） kd树(K-dimension tree)是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是一种二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。 类比“二分查找”：给出一组数据：[9 1 4 7 2 5 0 3 8]，要查找8。如果挨个查找（线性扫描），那么将会把数据集都遍历一遍。而如果排一下序那数据集就变成了：[0 1 2 3 4 5 6 7 8 9]，按前一种方式我们进行了很多没有必要的查找，现在如果我们以5为分界点，那么数据集就被划分为了左右两个“簇” [0 1 2 3 4]和[6 7 8 9]。 因此，根本就没有必要进入第一个簇，可以直接进入第二个簇进行查找。把二分查找中的数据点换成k维数据点，这样的划分就变成了用超平面对k维空间的划分。空间划分就是对数据点进行分类，“挨得近”的数据点就在一个空间里面。 2 构造方法（1）构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域； （2）通过递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。 （3）上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。 （4）通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。 KD树中每个节点是一个向量，和二叉树按照数的大小划分不同的是，KD树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建KD树时，关键需要解决2个问题： （1）选择向量的哪一维进行划分； （2）如何划分数据； 第一个问题简单的解决方法可以是随机选择某一维或按顺序选择，但是更好的方法应该是在数据比较分散的那一维进行划分（分散的程度可以根据方差来衡量）。好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分，这样问题2也得到了解决。 2.2.1 查找点(2,4.5) 1.先构建出树(按照x–&gt;y–&gt;x–&gt;y…的顺序构造平衡二叉树) 2.对查找点在树上进行判断，并记录下经过的点(search_path)，最后一个点称之为 当前最佳节点 3.以当前最佳节点到查找点之间的距离为半径画圆，把圆内节点的子树加入到search_path中，—–回溯 4.直到search_path内的点为空时，找到最小的dist 在(7,2)处测试到达(5,4)，在(5,4)处测试到达(4,7)【优先选择在本域搜索】，然后search_path中的结点为&lt;(7,2),(5,4), (4,7)&gt;，从search_path中取出(4,7)作为当前最佳结点nearest, dist为3.202； 然后回溯至(5,4)，以(2,4.5)为圆心，以dist=3.202为半径画一个圆与超平面y=4相交，所以需要跳到(5,4)的左子空间去搜索。所以要将(2,3)加入到search_path中，现在search_path中的结点为&lt;(7,2),(2, 3)&gt;；另外，(5,4)与(2,4.5)的距离为3.04 &lt; dist = 3.202，所以将(5,4)赋给nearest，并且dist=3.04。 回溯至(2,3)，(2,3)是叶子节点，直接平判断(2,3)是否离(2,4.5)更近，计算得到距离为1.5，所以nearest更新为(2,3)，dist更新为(1.5) 回溯至(7,2)，同理，以(2,4.5)为圆心，以dist=1.5为半径画一个圆并不和超平面x=7相交, 所以不用跳到结点(7,2)的右子空间去搜索。 至此，search_path为空，结束整个搜索，返回nearest(2,3)作为(2,4.5)的最近邻点，最近距离为1.5。 3 总结首先通过二叉树搜索（比较待查询节点和分裂节点的分裂维的值，小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点），顺着“搜索路径”很快能找到最近邻的近似点，也就是与待查询点处于同一个子空间的叶子结点； 然后再回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索（将其他子结点加入到搜索路径）。 重复这个过程直到搜索路径为空。 1.6 案例：鸢尾花种类预测–数据集介绍本实验介绍了使用Python进行机器学习的一些基本概念。 在本案例中，将使用K-Nearest Neighbor（KNN）算法对鸢尾花的种类进行分类，并测量花的特征。 本案例目的： 遵循并理解完整的机器学习过程 对机器学习原理和相关术语有基本的了解。 了解评估机器学习模型的基本过程。 1 案例：鸢尾花种类预测Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍： 2 scikit-learn中数据集介绍2.1 scikit-learn数据集API介绍 sklearn.datasets 加载获取流行数据集 datasets.load_*() 获取小规模数据集，数据包含在datasets里 datasets.fetch_*(data_home=None) 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/ 2.1.1 sklearn小数据集 sklearn.datasets.load_iris() 加载并返回鸢尾花数据集 2.1.2 sklearn大数据集 sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’) subset：’train’或者’test’，’all’，可选，选择要加载的数据集。 训练集的“训练”，测试集的“测试”，两者的“全部” 2.2 sklearn数据集返回值介绍 load和fetch返回的数据类型datasets.base.Bunch(字典格式) data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组 target：标签数组，是 n_samples 的一维 numpy.ndarray 数组 DESCR：数据描述 feature_names：特征名,新闻数据，手写数字、回归数据集没有 target_names：标签名 12345678910from sklearn.datasets import load_iris# 获取鸢尾花数据集iris = load_iris()print("鸢尾花数据集的返回值：\n", iris)# 返回值是一个继承自字典的Benchprint("鸢尾花的特征值:\n", iris["data"])print("鸢尾花的目标值：\n", iris.target)print("鸢尾花特征的名字：\n", iris.feature_names)print("鸢尾花目标值的名字：\n", iris.target_names)print("鸢尾花的描述：\n", iris.DESCR) 2.3 查看数据分布通过创建一些图，以查看不同类别是如何通过特征来区分的。 在理想情况下，标签类将由一个或多个特征对完美分隔。 在现实世界中，这种理想情况很少会发生。 seaborn介绍 Seaborn 是基于 Matplotlib 核心库进行了更高级的 API 封装，可以让你轻松地画出更漂亮的图形。而 Seaborn 的漂亮主要体现在配色更加舒服、以及图形元素的样式更加细腻。 安装 pip3 install seaborn seaborn.lmplot() 是一个非常有用的方法，它会在绘制二维散点图时，自动完成回归拟合 sns.lmplot() 里的 x, y 分别代表横纵坐标的列名, data= 是关联到数据集, hue=*代表按照 species即花的类别分类显示, fit_reg=是否进行线性拟合。 参考链接: api链接 1234567891011121314151617%matplotlib inline # 内嵌绘图import seaborn as snsimport matplotlib.pyplot as pltimport pandas as pd# 把数据转换成dataframe的格式iris_d = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])iris_d['Species'] = iris.targetdef plot_iris(iris, col1, col2): sns.lmplot(x = col1, y = col2, data = iris, hue = "Species", fit_reg = False) plt.xlabel(col1) plt.ylabel(col2) plt.title('鸢尾花种类分布图') plt.show()plot_iris(iris_d, 'Petal_Width', 'Sepal_Length') 2.4 数据集的划分机器学习一般的数据集会划分为两个部分： 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 划分比例： 训练集：70% 80% 75% 测试集：30% 20% 25% 数据集划分api sklearn.model_selection.train_test_split(arrays, *options) x 数据集的特征值–对应于表示这个花的一些数据参数 y 数据集的标签值–对应于表示这个是什么种类的花 test_size 测试集的大小，一般为float random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。(这是一个可以保证你在random_state设置相同参数，得到一样的结果，至于设置的值不同，只是代表随机选取的数不一样) return 测试集特征训练集特征值值，训练标签，测试标签(默认随机取) 12345678910111213from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_split# 1、获取鸢尾花数据集iris = load_iris()# 对鸢尾花数据集进行分割# 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_testx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)print("x_train:\n", x_train.shape)# 随机数种子x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=6)x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=6)print("如果随机数种子不一致：\n", x_train == x_train1)print("如果随机数种子一致：\n", x_train1 == x_train2) 1.7 特征工程-特征预处理1 什么是特征预处理(类似于标准化/归一化，保证数据的量纲)1.1 特征预处理定义 scikit-learn的解释provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. 翻译过来：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 为什么我们要进行归一化/标准化？ 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征 归一化 标准化 1.3 特征预处理API1sklearn.preprocessing #### 2 归一化–特别容易受到异常点的影响xxxx2.1 定义通过对原始数据进行变换把数据映射到(默认为[0,1])之间 2.2 公式 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 那么怎么理解这个过程呢？我们通过一个例子 2.3 API–一步帮你完成 sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… ) MinMaxScalar.fit_transform(X) X:numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 2.4 数据计算我们对以下数据进行运算，在dating.txt中。保存的就是之前的约会对象数据 123456milage,Liters,Consumtime,target40920,8.326976,0.953952,314488,7.153469,1.673904,226052,1.441871,0.805124,175136,13.147394,0.428964,138344,1.669788,0.134296,1 分析 1、实例化MinMaxScalar 2、通过fit_transform转换 1234567891011121314151617import pandas as pdfrom sklearn.preprocessing import MinMaxScalerdef minmax_demo(): """ 归一化演示 :return: None """ data = pd.read_csv("dating.txt") print(data) # 1、实例化一个转换器类 transfer = MinMaxScaler(feature_range=(2, 3)) # 2、调用fit_transform data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) print("最小值最大值归一化处理的结果：\n", data) return None 返回结果： 123456789101112131415161718 milage Liters Consumtime target0 40920 8.326976 0.953952 31 14488 7.153469 1.673904 22 26052 1.441871 0.805124 13 75136 13.147394 0.428964 1.. ... ... ... ...998 48111 9.134528 0.728045 3999 43757 7.882601 1.332446 3[1000 rows x 4 columns]最小值最大值归一化处理的结果： [[ 2.44832535 2.39805139 2.56233353] [ 2.15873259 2.34195467 2.98724416] [ 2.28542943 2.06892523 2.47449629] ..., [ 2.29115949 2.50910294 2.51079493] [ 2.52711097 2.43665451 2.4290048 ] [ 2.47940793 2.3768091 2.78571804]] 2.5 归一化总结注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。 怎么办？ 3 标准化3.1 定义通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内 3.2 公式 作用于每一列，mean为平均值，σ为标准差 所以回到刚才异常点的地方，我们再来看看标准化 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。 3.3 API sklearn.preprocessing.StandardScaler( ) 处理之后每列来说所有数据都聚集在均值0附近标准差差为1 StandardScaler.fit_transform(X) X:numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 3.4 数据计算同样对上面的数据进行处理 分析 1、实例化StandardScaler 2、通过fit_transform转换 12345678910111213141516171819import pandas as pdfrom sklearn.preprocessing import StandardScalerdef stand_demo(): """ 标准化演示 :return: None """ data = pd.read_csv("dating.txt") print(data) # 1、实例化一个转换器类 transfer = StandardScaler() # 2、调用fit_transform data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) print("标准化的结果:\n", data) print("每一列特征的平均值：\n", transfer.mean_) print("每一列特征的方差：\n", transfer.var_) return None 返回结果： 12345678910111213141516171819202122 milage Liters Consumtime target0 40920 8.326976 0.953952 31 14488 7.153469 1.673904 22 26052 1.441871 0.805124 1.. ... ... ... ...997 26575 10.650102 0.866627 3998 48111 9.134528 0.728045 3999 43757 7.882601 1.332446 3[1000 rows x 4 columns]标准化的结果: [[ 0.33193158 0.41660188 0.24523407] [-0.87247784 0.13992897 1.69385734] [-0.34554872 -1.20667094 -0.05422437] ..., [-0.32171752 0.96431572 0.06952649] [ 0.65959911 0.60699509 -0.20931587] [ 0.46120328 0.31183342 1.00680598]]每一列特征的平均值： [ 3.36354210e+04 6.55996083e+00 8.32072997e-01]每一列特征的方差： [ 4.81628039e+08 1.79902874e+01 2.46999554e-01] 3.5 标准化总结在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 1.8 案例：鸢尾花种类预测—流程实现1 再识K-近邻算法API sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’) n_neighbors： int,可选（默认= 5），k_neighbors查询默认使用的邻居数 algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’} 快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索， brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。 kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。 ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。 2 案例：鸢尾花种类预测2.1 数据集介绍Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍： 2.2 步骤分析 1.获取数据集 2.数据基本处理 3.特征工程 4.机器学习(模型训练) 5.模型评估 2.3 代码过程 导入模块 1234from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier 先从sklearn当中获取数据集，然后进行数据集的分割 123456# 1.获取数据集iris = load_iris()# 2.数据基本处理# x_train,x_test,y_train,y_test为训练集特征值、测试集特征值、训练集目标值、测试集目标值x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22) 进行数据标准化 特征值的标准化 1234# 3、特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test) 模型进行训练预测 1234567891011# 4、机器学习(模型训练)estimator = KNeighborsClassifier(n_neighbors=9)estimator.fit(x_train, y_train)# 5、模型评估# 方法1：比对真实值和预测值y_predict = estimator.predict(x_test)print("预测结果为:\n", y_predict)print("比对真实值和预测值：\n", y_predict == y_test)# 方法2：直接计算准确率score = estimator.score(x_test, y_test)print("准确率为：\n", score)]]></content>
      <tags>
        <tag>python</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos硬盘扩容到根目录]]></title>
    <url>%2F2020%2F03%2F14%2Fcentos%E7%A1%AC%E7%9B%98%E6%89%A9%E5%AE%B9%E5%88%B0%E6%A0%B9%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[问题描述centos6.4根目录扩容 几个查询磁盘/分区的命令以下均在root用户中进行 df -Th fdisk -l 1.先把根目录的分区变成最后一个如果有交换空间就swapoff /dev/sda3(交换空间) 2.删除所有的分区，并重新创建分区fdisk /dev/sda 按照提示进行操作 3.reboot4.resize2fs /dev/sda2完结！]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用PySpark分析空气质量]]></title>
    <url>%2F2020%2F03%2F12%2F%E4%BD%BF%E7%94%A8pyspark%E5%88%86%E6%9E%90%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%2F</url>
    <content type="text"><![CDATA[项目概述1. 数据来源: http://stateair.net/web/historical/1/1.html2. 根据北京的数据进行统计分析 空气质量指数(PM2.5) 健康建议 0-50 健康 51-100 中等 101-150 对敏感人群不健康 151-200 危险 201-300 非常不健康 301-500 危险 501-~ 报表 3. 数据分析–&gt;es–&gt;kibana先进行数据分析: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pyspark.sql import SparkSessionfrom pyspark.sql.types import *from pyspark.sql.functions import udfdef get_grade(value): if value &lt;= 50 and value &gt;= 0: return "健康" elif value &lt;= 100: return "中等" elif value &lt;= 150: return "对敏感人群不健康" elif value &lt;= 200: return "不健康" elif value &lt;= 300: return "非常不健康" elif value &lt;= 500: return "危险" elif value &gt; 500: return "爆表" else: return Noneif __name__ == '__main__': spark = SparkSession.builder.appName("project").getOrCreate() data2017 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2017_HourlyPM25_created20170803.csv").select("Year","Month","Day","Hour","Value","QC Name") data2016 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2016_HourlyPM25_created20170201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2015 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("file:///home/hadoop/data/Beijing_2015_HourlyPM25_created20160201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2017.show() data2016.show() data2015.show() grade_function_udf = udf(get_grade,StringType()) # 进来一个Value，出去一个Grade group2017 = data2017.withColumn("Grade",grade_function_udf(data2017['Value'])).groupBy("Grade").count() group2016 = data2016.withColumn("Grade",grade_function_udf(data2016['Value'])).groupBy("Grade").count() group2015 = data2015.withColumn("Grade",grade_function_udf(data2015['Value'])).groupBy("Grade").count() group2015.select("Grade", "count", group2015['count'] / data2015.count()).show() group2016.select("Grade", "count", group2016['count'] / data2016.count()).show() group2017.select("Grade", "count", group2017['count'] / data2017.count()).show() group2017.show() group2016.show() group2015.show() spark.stop() 得到: 并在yarn上运行 先在hadoop集群中创建新文件夹 hadoop fs -mkdir -p /data/（若已经有请删除hadoop fs -rmr /data） 把2017、16、15 年的数据上传至hadoop hadoop fs -put Beijing* /data/（参看是否上传成功hadoop fs -ls /data） 把pycharm中的程序改成yarn中的程序 再执行 ./spark-submit --master yarn ~/script/sparky.py 得到和上面一样的结果: es使用举例先es中创建目录 curl -XPUT &#39;http://192.168.211.4:9200/imooc_es&#39; 查看数据 curl -XGET &#39;http://192.168.211.4:9200/_search&#39; 创建一个索引 curl -XPOST &#39;http://hadoop000:9200/imooc_es/student/1&#39; -H &#39;Content-Type: application/json&#39; -d &#39;{ &quot;name&quot;:&quot;imooc&quot;, &quot;age&quot;:5, &quot;interests&quot;:[&quot;Spark&quot;,&quot;Hadoop&quot;] }&#39; es–&gt;kibana(注意这两个启动都在bin目录下)12345678910111213141516171819202122232425262728293031323334353637383940414243444546from pyspark.sql import *from pyspark.sql.functions import *from pyspark.sql.types import *def get_grade(value): if value &lt;= 50: return "健康" elif value &lt;= 100: return "中等" elif value &lt;= 150: return "对敏感人群不健康" elif value &lt;= 200: return "不健康" elif value &lt;= 300: return "非常不健康" elif value &lt;= 500: return "危险" elif value &gt; 500: return "爆表" else: return Noneif __name__ == '__main__': spark = SparkSession.builder.appName("project").getOrCreate() data2017 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2017_HourlyPM25_created20170803.csv").select("Year","Month","Day","Hour","Value","QC Name") data2016 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2016_HourlyPM25_created20170201.csv").select("Year","Month","Day","Hour","Value","QC Name") data2015 = spark.read.format("csv").option("header","true").option("inferSchema","true").load("/data/Beijing_2015_HourlyPM25_created20160201.csv").select("Year","Month","Day","Hour","Value","QC Name") grade_function_udf = udf(get_grade, StringType()) group2017 = data2017.withColumn("Grade", grade_function_udf(data2017['Value'])).groupBy("Grade").count() group2016 = data2016.withColumn("Grade", grade_function_udf(data2016['Value'])).groupBy("Grade").count() group2015 = data2015.withColumn("Grade", grade_function_udf(data2015['Value'])).groupBy("Grade").count() result2017 = group2017.select("Grade", "count").withColumn("precent",group2017['count'] / data2017.count()*100) result2016 = group2016.select("Grade", "count").withColumn("precent",group2016['count'] / data2016.count()*100) result2015 = group2015.select("Grade", "count").withColumn("precent",group2015['count'] / data2015.count()*100)# 将数据写入到es中 result2017.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2017/pm") result2016.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2016/pm") result2015.selectExpr("Grade", "count", "precent").write.format("org.elasticsearch.spark.sql").option("es.nodes","192.168.211.4:9200").mode("overwrite").save("weather2015/pm") spark.stop() 然后再到192.168.211.4:5601里面查看es的数据是否导进来，再利用kibanna进行可视化]]></content>
      <tags>
        <tag>python</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo新电脑的设置]]></title>
    <url>%2F2020%2F03%2F08%2Fhexo%E6%96%B0%E7%94%B5%E8%84%91%E7%9A%84%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装hexo博客必要的软件下载安装Git客户端安装node.js Github添加新电脑生成的密钥打开git bash输入如下命令： 1ssh-keygen -t rsa -C &quot;xxxxx@163.com&quot; 邮箱为GitHub注册邮箱，输入命令后直接回车，生成密钥对。根据提示找到密钥对所在位置，将id_rsa.pub公钥内容复制粘贴到Github-settings-‘SSH and GPG keys’-‘SSH keys’中。使用ssh -T git@github.com测试公钥是否添加成功 安装Hexo在cmd或者创建博客主文件夹，右键git bash内输入下面的命令： 1npm install hexo-cli -g 文件移动进入博客目录，如D:\blog\，Hexo init，这一步有可能在windows上特别慢，看了网上说先退出，再hexo init，结果秒完成，玄学。然后把之前复制的文件全部粘贴过来，选择全部覆盖。这个时候，这个本地环境的文件已经是要发布的文件了，接着安装一些模块插件，否则看不到你的一些功能（比如字数统计，访问量统计等）。 That`s all12$ hexo clean # 如果配置文件没有更改，忽略该命令$ hexo g -d 参考[1] https://fl4g.cn/2018/08/03/Hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%85%B6%E4%BB%96%E7%94%B5%E8%84%91/ [2] https://blog.csdn.net/m0_37286282/article/details/89496837?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task]]></content>
  </entry>
  <entry>
    <title><![CDATA[HCIA-BigData题目]]></title>
    <url>%2F2020%2F01%2F25%2FHCIA-BigData%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[HDFSHDFS： QUESTION 10-m Hadoop 的 HDFS 是一种分布式文件系统，适合以下哪种应用场景的数据存储和管理？ A. 大量小文件存储 B. 高容错、高吞吐量 C. 低延迟读取 D. 流式数据访问 QUESTION 15 华为 FusionInsight HD 系统中关于 HDFS 的 DataNode 的说法正确的是？ A. 不会检查数据的有效性 B. 周期性地将本节点的 Block 相关信息发送给 NameNode C. 不同的 DataNode 存储的 Block 一定是不同的 D. 一个 DataNode 上的 Block 可以是相同的 QUESTION 39HDFS 机制中 NameNode 负责管理元数据， Client 端每次读请求都需要从 NameNode 的元数据磁盘中读取元数据信息，以此获取所读文件在 DataNode 中的位置。A. 对B. 错 QUESTION 47HDFS 的 Client 写入文件时，数据的第一副本写入位置由 NameNode 确定，其他副本的写入位置由 DataNode 确定。A. 对B. 错 QUESTION 53下列哪条 HDFS 命令可用于检测数据块的完整性？A. hdfs fsck /B. hdfs fsck – deleteC. hdfs dfsadmin – reportD. hdfs balancer – threshold 1 QUESTION 65-mHDFS 系统中对备用 NameNode 的作用的描述正确的有？A. 主 NameNode 的热备B. 备 NameNode 对内存没有要求C. 帮助主 NameNode 合并 编辑日志，减少主 NameNode 的启动时间D. 备 NameNode 应与主 NameNode 部署到一个节点 QUESTION 76FusionInsight HD 系统中 HDFS 默认 Block Size 是多少？A. 32MB. 64MC. 128M QUESTION 84-mFusionInsight HD 集群中包含多种服务，每种服务又由若干角色组成，下面哪些是服务的角色？A. HDFSB. NameNodeC. DataNodeD. HBase QUESTION 96HDFS 的 NameNode 节点主备状态管理及元数据文件合并分别由哪两个角色负责?A. ZKFC 和备 NameNodeB. 主 NameNode 和备 NameNodeC. ZKFC 和主 NameNodeD. 主 NameNode 和 JournalNode QUESTION 100Hadoop 系统中关于客户端向 HDFS 文件系统上传文件说法正确的是？A. 客户端的文件数据经过 NameNode 传递给 DataNodeB. 客户端将文件划分为多个 Block,根据 DataNode 的地址信息，按顺序写入每一个DataNode 中C. 客户端根据 DataNode 的地址信息，按顺序将整个文件写入每一个 DataNode 中，然后由 DataNode 将文件划分为多个 BlockD. 客户端只上传数据到一个 DataNode ，然后由 NameNode 负责 Block 复制 QUESTION 106HDFS 支持大文件存储，同时支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。A. 正确B. 错误 QUESTION 117Fusionlnsight HD 系统中 HDFS 的 Block 默认保存几份？A. 3 份B. 2 份C. 1 份D. 不确定 QUESTION 128华为 Fusionlnsight HD 系统中关于 HDFS 的 DataNode 说法正确的是？A. 不会检查数据的有效性B. 周期性的将本节点的 Block 发送给 NameNodeC. 不同的 DataNode 存储的 Block 一定是不同的D. 一个 DataNode 上的的 Block 可以是相同的 QUESTION 151HDFS 的 Client 写入文件时，数据的第一副本写入位置是由 NameNode 确定，其他副本的写入位置由 DataNode 确定A. 正确B. 错误 QUESTION 156Hadoop 中哪个模块负责 HDFS 的数据存储？A. NameNodeB. DataNodeC. ZooKeeperD. JobTraoker QUESTION 186-m安全模式下安装 Fusionlnsight HD 集群时，哪些组件是必须安装的？A. ZookeeperB. LDAPServerC. KrbServerD. HDFS QUESTION 223HDFS 机制中， NameNode 负责管理元数据， Client 端每次请求都需要从 NameNode 的元数据盘中读取元数据信息以此获取所读文件在 DataNode 的位置A. 正确B. 错误 QUESTION 224-m以下关于 Hadoop 分布式文件系统 HDFS 联邦描述正确的有？A. 一个 Namespace 使用一个 block pool 管理数据块B. 一个 Namespace 可使用多个 block pool 管理数据库C. 每个 block pool 的磁盘空间是物理共享的，逻辑空间是隔离的D. 支持 NameNode/NameSpace 水平扩展 QUESTION 259-mFusionlnsight HD 系统中，关于 Solt 索引的存储部署策略，以下说正确的有？（多选）A. 利用 HDFS 数据存储可靠性和易于扩容的特点优先选择索引存储于 HDFSB. 不论 Solt 索引存储在 HDFS 上还是存储在本地磁盘，在同一节点上部署 5 个 Solt 实例，根据 IP 和不同的端口号来区分不同的 Solt 实例C. 当对实时索引录入速度要求较高时，可选择索引存放于本地磁盘D. 当索引数据存放在 HDFS 上时， SolrServer 实例与 DataNode 实例部署在同一节点上。 QUESTION 279Fusionlnsight HD 系统部署时，如果 Solr 索引默认存放在 HDFS 上时，以下理解正确的有？A. 不需要考虑各 SolrServer 实例上创建了多少 SharedB. 为保证数据可靠性，创建索引时必须创建 ReplicaC. 通过 HDFS 读取索引时占用磁盘 IO，因此不建议 Solr 实例与 DataNode 部署在同一节点上D. 当 Solr 服务参考 INDEX_ON_HDFS 值为 HDFS 时，创建 Collection 的索引默认存储在HDFS 上。 QUESTION 284-m以下关于 Hadoop 的 HDFS 描述正确的有？（多选）A. HDFS 由 NameNode， DataNode， Client 组成B. HDFS 备 NameNode 上的元数据是由主 NameNode 同步过去的C. HDFS 采用就近的机架节点进行数据的第一副本存储D. HDFS 适合一次写入，多次读取的读写任务 QUESTION 286客户 IT 系统中 Fusionlnsight HD 集群有 150 个节点，每个节点 12 块磁盘（不包括 OS盘），每块磁盘大小 IT,只安装 HDFS，按照默认配置最大可存储多少数据？A. 1764TBB. 1800TBC. 600TBD. 588TB MapReduceQUESTION 34Hadoop 中 MapReduce 组件擅长处理哪种场景的计算任务？A. 迭代计算B. 离线计算C. 实时交互计算D. 流式计算 QUESTION 35Hadoop 系统中，如果 HDFS 文件系统的备份因子是 3，那么 MapReduce 每次运行 Task 都要从 3 个有副本的机器上传输需要处理的文件段。A. 对B. 错 QUESTION 62Hadoop 系 统 中 YARN 分 配 给 Container 的 内 存 大 小 ， 可 以 通 过 参 数yarn.app.mapreduce.am.resource.mb 来设置A. 对B. 错 QUESTION 80以下哪个不属于 Hadoop 中 MapReduce 组件的特点？A. 易于编程B. 良好的拓展性C. 实时计算D. 高容错性 QUESTION 246Hadoop 中 MapReduce 组件擅长处理哪种场景的计算任务？A. 迭代计算B. 离线计算C. 实时交互计算D. 流式计算 Yarn关于loader的都不是很懂 QUESTION 20-mYARN 服务中，如果要给队列 QuqueA 设置容量为 30%，应该配置哪个参数？A. yarn.scheduler.capacity.root.QueueA.user-limit-factorB. yarn.scheduler.capacity.root.QueueA.minimum-user-limit-percentC. yarn.scheduler.capacity.root.QueueA.capacityD. yarn.scheduler.capacity.root.QueueA.state QUESTION 36YARN 调度器分配资源的顺序，下面哪一个描述是正确的？A. 任意机器 -&gt; 同机架 -&gt; 本地资源B. 任意机器 -&gt; 本地资源 -&gt; 同机架C. 本地资源 -&gt; 同机架 -&gt; 任意机器D. 同机架 -&gt; 任意机器 -&gt; 本地资源 QUESTION 45如果 YARN 集群中只有 Default、 QueueA 和 QueueB 子队列，那么允许将他们的容量分别设置为 60%、 25%、 22%。A. 对B. 错 QUESTION 63-mYARN 通过 ResourceManager 对集群资源进行管理，它的主要功能有？A. 集群资源调度B. 应用程序管理C. 日志管理D. 以上说法都不对 QUESTION 88YARN 上有两个同级队列 Q1 与 Q2，容量都是 50%， Q1 上已经有 10 个任务共占用了 40 的容量， Q2 上有两个任务共占用了 30 的容量，那么由于 Q1 的任务数多，调度器会优先将资源分配给 Q1。A. 对B. 错 QUESTION 91FusionInsight HD 中 Loader 作业提交到 YARN 后，作业不能手动停止。A. 对B. 错 QUESTION 97Hadoop 平台中启用 YARN 组件的日志聚集功能，需要配置哪个参数？A. yarn.nodemanager.local-dirB. yarn.nodemanager.log-dirsC. yarn.acl.enableD. yarn.log-aggregation-enable QUESTION 104YARN 中设置队列 QueueA 最大使用资源量，需要配置哪个参数？A. yarn,scheduler.capacity.root,QueueA.user-limit-factorB. yarn,scheduler.capacity.root,QueueA.minimum-user-limit-fpercentC. yarn,scheduler.capacity.root,QueueA.stateD. yarn,scheduler.capacity.root,QueueA.maximum-capacity QUESTION 124Fusionlnsight HD Loader 作业前后，需要哪些节点与外部数据源通讯？A. loader 服务主节点B. 运行 Yarn 服务作业的节点C. 前面两个都需要D. 前面两个都不需要 QUESTION 143-mFusionlnsight HD LLD 配置规划工具可以生成哪些配置文件？A. 监控告警阈值配置文件B. 集群的安装模板文件C. HDFS 和 YARN 的配置文件D. 执行 Precheck 所需要的配置文件 CheckNodes.Config QUESTION 153Fusionlnsight HD 系统中 ，下列哪个方法不能查看到 Loader 作业执行的结果？A. 通过 Yarn 任务管理查看B. 通过 Loader UI 界面查看C. 通过 Manager 的告警查看D. 通过 NodeManager 查看 QUESTION 169关于 Fusionlnsight HD 中 Loader 作业描述正确的是？A. Loader 将作业提交到 Yarn 执行后，如果此时 Loader 服务异常，则此作业执行失败。B. Loader 将作业提交到 Yarn 执行后，如果某个 Mapper 任务执行失败，能够自动进行重试C. Loader 将作业执行失败后将会产生垃圾数据，需要用户手动清除D. Loader 将一个作业提交至 Yarn 执行后，该作业执行完成前，不能再提交其他作业 QUESTION 254Hadoop 平台中，要查看 YARN 服务中的一个 application 信息，通常需要使用什么命令？A. containerB. applicationatternptC. iarD. application QUESTION 261-mHadoop 系统中， YARN 支持哪些资源类型的管理？（多选）A. 内存B. CPUC. 网络D. 磁盘空间 #Spark QUESTION 9 Spark On Yarn 模式下的 driver 只能运行在客户端。A. 对B. 错 Spark QUESTION 21Spark 和 Hadoop 都不适用于迭代计算的场景。A. 对B. 错 QUESTION 37Spark SQL 表中，经常会存在很多小文件（大小远小于 HDFS 块大小），在这种情况下，Spark 会启动更多的 Task 来处理这些小文件，当 SQL 逻辑中存在 Shuffle 操作时，会大大增加 hash 分桶数，从而严重影响性能。A. 对B. 错 QUESTION 40Spark 应用运行时，如果某个 Task 运行失败则导致整个 app 运行失败。A. 对B. 错 QUESTION 54Spark On YARN 模式下，没有部署 NodeManager 的节点不能启动 executor 执行 TaskA. 对B. 错 QUESTION 79Spark 是基于内存的计算引擎，所有 Spark 程序运行过程中，的数据只能存储在内存中A. 对B. 错 QUESTION 95Spark 应用在运行时， Stage 划分的依据是什么？A. taskB. taskSetC. actionD. shuffle QUESTION 98Spark 任务的一个 Executor 同时可以运行多个 taskA. 对B. 错 QUESTION 101Fusionlnsight Manager 不能够管理哪个对象？A. SparkB. 主机 OSC. YARND. HDFS QUESTION 121Spark 任务的每个 stage 可划分为 job, 划分的标记是 shuffleA. 正确B. 错误 QUESTION 147下列哪些是 Spark 可以提供的功能？A. 分布式内存计算引擎B. 分布式文件系统C. 集群资源的统一调度D. 流处理功能 QUESTION 238Spark 组建中哪个选项不属于 transformation 操作？A. joinB. distinctC. reduceBykeyD. reduce QUESTION 278以下哪些是 Spark 服务的常驻进程？（多选）A. JobHistoryB. JDBCServerC. SparkResourceD. Nodemanager QUESTION 299华为 Fusionlnsight HD 集群中， Spark 服务可以从以下哪些服务读取数据？（多选）A. YARNB. HDFSC. HiveD. HBase HBaseQUESTION 3-mHBase 集群定时执行 Compaction 的目的是什么？A. 减少同一 Region，同一 ColumnFamily 下的文件数目B. 提升数据读取性能C. 减少同一 ColumnFamily 的文件数据D. 减少同一 Region 的文件数目 QUESTION 8FusionInsight HD HBase 默认使用什么作为其底层文件存储系统？A. HDFSB. HadoopC. MemoryD. MapReduce QUESTION 14-mHBase 的主要特点有哪些？A. 高可靠性B. 高性能C. 面向列D. 可伸缩 QUESTION 18下列哪些组件必须依赖于 Zookeeper 才能运行？A. HDFSB. HBaseC. SparkD. YARN QUESTION 24HBase 中 Region 的物理存储单元是什么A. RegionB. ColumnFamilyC. ColumnD. Row QUESTION 32FusionInsight HD 中使用 HBase 进行数据读取服务时需要连接 HMasterA. 对B. 错 QUESTION 33HBase 中一个 Region 进行 Split 操作时，将一个 HFile 文件真正分开到两个 Region 的过程发生在以下什么阶段？A. Split 过程中B. Flush 过程中C. Compaction 过程中D. HFile 分开过程中 QUESTION 43-mHBase 的数据文件 HFile 中一个 KeyValue 格式包含哪些信息？A. KeyB. ValueC. TimestampD. KeyType QUESTION 48HBase 的某张表的 RowKey 划分 SplitKey 为 9， E， a， z，请问该表有几个 Region？A. 3B. 4C. 5D. 6 QUESTION 52FusionInsight HD 平台中， HBase 暂不支持二级索引A. 对B. 错 QUESTION 56下列关于 HBase 的 BloomFilter 特性理解正确的是？A. 用来过滤数据B. 用来优化随机读取的性能C. 会增加存储的消耗D. 可以准确判断某条数据不存在 QUESTION 60Loader 仅支持关系型数据库与 HBase 之间的数据导入导出。A. 对B. 错 QUESTION 71HBase 的最小处理单元是 Region， User Region 和 Region Server 之间的路由信息是保存在哪里的？A. ZookeeperB. HDFSC. MasterD. meta 表 QUESTION 83HBase 元数据 Meta Region 路由信息保存在哪里？A. Root 表B. ZookeeperC. HMasterD. Meta 表 QUESTION 87执行 HBase 数据读取业务时，需要读取哪几部分数据？A. HFileB. HLogC. MemStoreD. HMaster QUESTION 89FusionInsight HD 的 HBase 服务包含哪些进程？A. HMasterB. SlaveC. HRegionServerD. DataNode QUESTION 92HBase 的主 Master 是如何选举的？A. 随机选取B. 由 RegionServer 进行裁决C. 通过 Zookeeper 进行裁决D. HMaster 为双主模式，不需要进行裁 QUESTION 109Fusionlnsight HD HBase 的管理进程是如何选择主节点的？A. 随机选取B. 由 RegionServer 进行裁决C. 通过 ZooKeeper 进行裁决D. HMaster 为双主模式，不需要进行裁决 QUESTION 142Fusionlnsight HD 系统中 HBase 元数据 Meta region 路由信息保存在哪？A. Root 表B. ZooKeeperC. HMasterD. Mata 表 QUESTION 155Fusionlnsight HD 中,如果需要查看当前登录 HBase 的用户和权限组，可以在 HBase shell中执行什么命令？A. use-permissionB. whoamiC. whoD. get-user QUESTION 162Hadoop 的 HBase 不适合哪些数据类型的应用场景？A. 大文件应用场景B. 海量数据应用场景C. 高吞吐率应用场景D. 半结构化数据应用场景 QUESTION 165Hadoop 平台中 HBase 的 Region 是由哪个服务进程来管理？A. HMasterB. DatanodeC. RegionServerD. Zookeeper QUESTION 173-m基于 Hadoop 开源大数据平台主要提供了针对数据分布式计算和存储能力，如下属于分布式存储组件的有？（多选）A. MRB. SparkC. HDFSD. HBase QUESTION 190Fusionlnsight HD 使用 HBase 客户端批量写入 10 条数据，某个 RegionServer 节点上包含该表的 2 个 Region，分别 A 和 B， 10 条数据中有两条属于 A， 4 条属于 B，请问写入这 10 条数据需要向该 RegionServer 发送几次 RPC 请求？A. 1B. 2C. 3D. 4 QUESTION 210Fusionlnsight HD 中使用 HBase 进行数据读写服务时，需要连接 HManager。A. 正确B. 错误 QUESTION 214Fusionlnsight HD 系统中，以下选项哪一个不是 HBase 写数据流程涉及的角色或服务？A. ZookeeperB. HDFSC. HMasterD. RegionServer QUESTION 225关于 HBase 中 HFile 的描述不正确的是？A. 一个 HFile 属于一个 RegionB. 一个 HFile 包含多个列族的数据C. 一个 HFile 包含多列数据D. 一个 HFile 包含多行数据 QUESTION 252Fusionlnsight HD 的 HBase 中会保存一张用户信息表 meg-table,Rowkey 为用户 ID ,其 中 一 列 为 用 户 昵 称 ， 现 在 按 先 后 顺 序 往 这 列 写 入 三 个KeyValue:001:Li,001:Mary,001:Lily，请问 scan’meg-table’,{ERSIONS=&gt;2}会返回哪几条数据？A. 001:LiB. 001:LilyC. 001:Li,001:Mary,001:LilyD. 001:Mary,001:Lily QUESTION 253关于 HBase 的 Region 分裂流程 Split 的描述不正确的是？A. Split 过程中并没有真正的将文件分开，仅仅是创建了引用文件B. Split 为了减少 Region 中数据大小，从而将一个 Region 分裂成两个 RegionC. Split 过程中该表会暂停服务D. Split 过程中分裂的 Region 会暂停服务 QUESTION 255Fusionlnsight HD 系统中执行 HBase 写数据时候，数据被写入内存 Memstore、日志 HLog和 HDP 中，请问哪一步写入成功后才会最终返回客户端写数据成功？A. MemstoreB. HLogC. HDFSD. Menmory QUESTION 265HBase 的物理存储单元是什么？A. RegionB. ColumnFamilyC. ColumnD. ROW QUESTION 270HBase 中数据存储的文件格式是什么？A. FlieB. SequenceFileC. LogD. TXTflie QUESTION 292Fusionlnsight HD 系统中 HBase 支持动态扩展列。A. 正确B. 错误 HiveQUESTION 4-mFusionInsight HD 系统中 Hive 支持的存储格式包括？A. HFileB. TextFileC. SequenceFileD. RCFile QUESTION 6-mFusionInsight Manager 界面显示 Hive 服务状态为 Bad 时，可能原因有哪些？A. DBService 不可用B. HDFS 服务不可用C. MetaStore 实例不可用D. HBase 服务不可用 QUESTION 31在 FusionInsight HD 中，以下哪一项不属于 Hive 的流控特性A. 支持对已建立的总连接数做阈值控制B. 支持对每个用户已经建立的连接数做阈值控制C. 支持对某个特定用户已建立的连接数做阈值控制D. 支持对单位时间内所建立的连接数做阈值控制 QUESTION 50关于 Hive 中普通表和外部表的描述不正确的是？A. 默认创建普通表B. 外部表实质是将已经存在 HDFS 上的文件路径跟表关联起来C. 删除普通表时，元数据和数据同时被删除D. 删除外部表时，只删除外部表数据而不删除元数据 QUESTION 68关于 Hive 建表的基本操作，描述正确的是？A. 创建外部表的时需要指定 external 关键字B. 一旦表创建好，不可再修改表名C. 一旦表创建好，不可再修改列名D. 一旦表创建好，不可再增加新列 QUESTION 137加载数据到 Hive 表，哪种方式不正确？A. 直接将本地路径的文件 load 到 Hive 表中B. 将 HDFS 上的额文件 load 到 Hive 表中C. Hive 支持 insert into 单条记录的方法，所以可以直接在命令行插入单条记录D. 将其他表的结果集 insert into 到 Hive 表中 QUESTION 185Fusionlnsight HD Manager 界面 Hive 日志收集，哪个选项不正确？A. 可指定实例进行日志收集，比如指定单独收集 MetaStore 的日志B. 可指定时间段进行日志收集，比如只收集 2016-1-10 的日志C. 可指定节点 IP 进行日志收集，例如仅下载某个 IP 的日志D. 可指定特定用户进行日志收集，例如仅下载 userA 用户产生的日志 QUESTION 202关于 Hive 与 Hadoop 其他组件的关系，以下描述错误的是？A. Hive 最终将数据存储在 HDFS 中B. Hive SQL 其本质是执行 MapReduce 任务C. Hive 是 Hadoop 平台的数据仓库工具D. Hive 对 HBase 有强依赖 QUESTION 297关于 Hive 在 Fusionlnsight HD 中的架构描述错误的是？A. 只要有一个 HiveServer 不可用，整个 Hive 集群便不可用B. Hiveserver 负责接受客户端请求、解析、执行 HQL 命令并返回查询结果C. MetaStore 用于提供数据服务，依赖于 DBServerD. 在同一个时间点 Hiveserve 只有一个处于 Active 状态，另一个则处于 Standby 状态 StreamingQUESTION 2FusionInsight HD 的 Streaming 对于 Zookeeper 弱依赖，即使 Zookeeper 故障 Streaming可以正常提供服务。A. 对B. 错 QUESTION 28关于 FusionInsight HD Streaming 的 Supervisor 描述正确的是？A. Supervisor 负责资源的分配和任务的调度B. Supervisor 负责接受 Nimbus 分配的任务，启动停止属于自己管理的 Worker 进程C. Supervisor 是运行具体处理逻辑的进程D. Supervisor 是在 Topology 中接收数据然后执行处理的组件 QUESTION 74-mFusionInsight HD 系统中使用 Streaming 客户端 Shell 命令查看拓扑或提交拓扑失败，以下哪些定位手段是正确的？A. 查看客户端异常堆栈，判断是否客户端使用问题B. 查看主 Nimbus 的运行日志，判断是否 Nimbus 服务端异常C. 查看 Supervisor 运行日志，判断是否 Supervisor 异常D. 查看 Worker 运行日志 QUESTION 207-mFusionlnsight HD 系统中 使用 Streaming 客户端 Shell 命令提交了拓扑之后，使用Storm UI 查看发现该拓扑长时间没有处理数据，可能原因有？A. 拓扑结构过于复杂或者并发太大，导致 worker 启动时间过长，超过 supervisor 的等待时间B. supervisor 的 slots 资源被耗尽，拓扑提交上去后分不到 slot 去启动 worker 进程C. 拓扑业务存在扩及错误，提交之后无法正常运行D. 当数据量较大时，拓扑处理速度较慢 QUESTION 263安装 Fusionlnsight HD 的 Streaming 组件时， Nimbus 角色要求安装几个节点？A. 1B. 2C. 3D. 4]]></content>
      <tags>
        <tag>HCIA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop各组件URL]]></title>
    <url>%2F2020%2F01%2F06%2Fhadoop%E5%90%84%E7%BB%84%E4%BB%B6URL%2F</url>
    <content type="text"><![CDATA[1、HDFS页面：50070 2、YARN的管理界面：8088 3、HistoryServer的管理界面：19888 4、Zookeeper的服务端口号：2181 5、Mysql的服务端口号：3306 6、Hive.server1=10000 7、Kafka的服务端口号：9092 8、azkaban界面：8443 9、Hbase界面：16010,60010 10、Spark的界面：8080 11、Spark的URL：7077]]></content>
  </entry>
  <entry>
    <title><![CDATA[leetcode刷题]]></title>
    <url>%2F2020%2F01%2F01%2Fleetcode%E5%88%B7%E9%A2%98%2F</url>
    <content type="text"><![CDATA[leetcode理论知识学习笔记collections模块https://blog.csdn.net/weixin_41644993/article/details/96498297 栈压栈 push()出栈 pop()返回栈顶元素 peek() 123456789101112131415161718192021class Stack(object): def __init__(self): self.__li = [] def push(self, item): self.__li.append(item) def pop(self): return self.__li.pop(-1) def peek(self): if self.__li: return self.__li[-1] else: return None def is_e(self): return self.__li == [] def size(self): return len(self.__li) 最大（小）堆和堆排序简介 调用heapq库 (排好序的) from heapq import xxxx heappush(heap, x) 将x压入堆中 heappop(heap) 从堆中弹出最小的元素 heapify(heap) 让列表具备堆特征 heapreplace(heap, x) 弹出最小的元素，并将x压入堆中，返回弹出的元素 nlargest(n, iter) 返回iter中n个最大的元素 nsmallest(n, iter) 返回iter中n个最小的元素 迷宫问题栈空的时候是迷宫走完了(此时没走出去) 可以通过在列表或者元祖前加*,来展开列表中的元素 队列排序和刹闸坡(对比)[https://images2015.cnblogs.com/blog/975503/201702/975503-20170214211234550-1109833343.png] 冒泡排序 O(n^2) 12345678910111213def bubble_sort(li): # 冒泡排序，两两进行比较 for j in range(len(li)): # 0 ~ n-1 for i in range(j): # 0 ~ j-1 if li[i] &gt; li[i+1]: li[i], li[i+1] = li[i+1], li[i] return lia = [2, 5, 1, 6]print(bubble_sort(a)) 选择排序 O(n^2) (主要是分成两部分，前面那部分是排好序的，后面那部分是未排序的) 1234567891011121314151617def select_sort(li): # 选择排序是找到后j个里面最小的一个，并把它放在li[]上 for j in range(len(li)): # 0 ~ n-1 min = li[j] min_index = j for i in range(j, len(li)): # 0 ~ j-1 if li[i] &lt; min: min = li[i] min_index = i li[j], li[min_index] = li[min_index], li[j] return lia = [2, 5, 1, 6]print(select_sort(a)) 插入排序 O(n^2) 1234567891011121314def insert_sort(li): # 思路: 每次对第i个元素之前的进行排序 for i in range(1, len(li)): while i &gt; 0: if li[i] &lt; li[i-1]: li[i], li[i-1] = li[i-1], li[i] i -= 1 else: break return lia = [2, 5, 1, 6, 2]print(insert_sort(a)) 希尔排序 123456789101112131415161718def shell_sort(li): # 思路: 本质上是在插入排序的基础上进行的改进 # 主要是把原来的数组按gap划分 n = len(li) gap = n // 2 while gap &gt; 0: for i in range(gap, n): while i &gt; 0: if li[i] &lt; li[i-gap]: li[i], li[i-gap] = li[i-gap], li[i] i -= 1 else: break gap = gap//2 return lia = [2, 5, 1, 6, 2]print(shell_sort(a)) 快速排序 123456789101112131415161718192021222324def quick_sort(li, start, final): if start &gt;= final: return low = start high = final minvalue = li[low] while low &lt; high: # 如果high处的值大于 minvalue high的值减一，然后继续比较 while low &lt; high and minvalue &lt;= li[high]: high -= 1 # 直到比较到high处的值小，这是把high处的值交换到low处 li[low] = li[high] while low &lt; high and li[low] &lt; minvalue: low += 1 li[high] = li[low] li[low] = minvalue quick_sort(li, start, low-1) quick_sort(li, low+1, final)a = [2, 5, 1, 6,7,7,7,7,9]quick_sort(a, 0, 3)print(a) 归并排序 123456789101112131415161718192021222324252627def Merge_Sort(li): n = len(li) if n &lt;= 1: return li # 这是其内部只有一个元素 m = n//2 left_li = Merge_Sort(li[:m]) right_li = Merge_Sort(li[m:]) # 先写当分成每个数组都是一个元素的时候： left_pointer, right_poiter = 0, 0 result = [] while left_pointer &lt; len(left_li) and right_poiter &lt; len(right_li): if left_li[left_pointer] &lt;= right_li[right_poiter]: result.append(left_li[left_pointer]) left_pointer += 1 else: result.append(right_li[right_poiter]) right_poiter += 1 result += left_li[left_pointer:] result += right_li[right_poiter:] return resulta = [2, 5, 1, 6, 7, 7, 7, 7, 9]b = Merge_Sort(a)print(a)print(b) 查找二分查找两个条件：1. 有序 2.列表(顺序表) 计算：头跟尾坐标相加除二 树注意一点 bool([])是False; bool[None]是True。 因为这本质上是对列表进行判断。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687class Node(object): def __init__(self, item): self.elem = item self.lchild = None self.rchild = Noneclass Tree(object): def __init__(self): self.root = None def add(self, item): node = Node(item) if self.root is None: self.root = node return queue = [] queue.append(self.root) while queue: cur_node = queue.pop(0) # 弹出来的是一个node？ if cur_node.lchild is None: cur_node.lchild = node return else: queue.append(cur_node.lchild) if cur_node.rchild is None: cur_node.rchild = node return else: queue.append(cur_node.rchild) def B_travel(self): if self.root is None: return queue = [] queue.append(self.root) while queue: cur_node = queue.pop(0) print(cur_node.elem, end=' ') # 其实每一个元素都是一个节点，直接在队列里把他们都显示出来 if cur_node.lchild is not None: queue.append(cur_node.lchild) if cur_node.rchild is not None: queue.append(cur_node.rchild) def before_order(self, root): if root is None: return node = root print(node.elem, end=' ') self.before_order(node.lchild) self.before_order(node.rchild) def in_order(self, root): if root is None: return node = root self.in_order(node.lchild) print(node.elem, end=' ') self.in_order(node.rchild) def pos_order(self, root): if root is None: return node = root self.pos_order(node.lchild) self.pos_order(node.rchild) print(node.elem, end=' ')tree = Tree()tree.add(0)tree.add(1)tree.add(2)tree.add(3)tree.add(4)tree.add(5)tree.add(6)tree.add(7)tree.add(8)tree.add(9)tree.B_travel()print('\n')tree.before_order(tree.root)print('')tree.in_order(tree.root)print('')tree.pos_order(tree.root) 图DAG：有向无圈图 图论中BFS、DFS[Python] BFS和DFS算法（第1讲）、DijkstraBFS（一层一层的来） BFS的算法思想： 1.选择一个根 ，并放入队列 2.将队列位于head的 节点拿出来，并把与目前head节点相连的节点放到队列中 3.重复2 python实现： 1234567891011121314151617181920212223graph1 = &#123; 'A': ['B', 'C'], 'B': ['A', 'C', 'D'], 'C': ['A', 'B', 'D'], 'D': ['B', 'C', 'E'], 'E': ['C', 'D'], 'F': ['D']&#125;def BFS(graph, s): queue = [s] # 1.把根进入队列 seen = set() seen.add(s) while len(queue) &gt; 0: head = queue.pop(0) # 2. head pop出来 nodes = graph[head] # 并把与head相连的节点遍历出来 for w in nodes: if w not in seen: queue.append(w) # 放入队列中 seen.add(w) print(head)BFS(graph1, 'A') 对BFS进行扩展，如何找到路径 12345678910111213141516171819202122232425262728293031graph1 = &#123; 'A': ['B', 'C'], 'B': ['A', 'C', 'D'], 'C': ['A', 'B', 'D'], 'D': ['B', 'C', 'E'], 'E': ['C', 'D'], 'F': ['D']&#125;def BFS(graph, s): queue = [s] # 1.把根进入队列 seen = set() seen.add(s) parent = &#123;s: None&#125; # 创建一个父亲节点 while len(queue) &gt; 0: head = queue.pop(0) # 2. head pop出来 nodes = graph[head] # 并把与head相连的节点遍历出来 for w in nodes: if w not in seen: queue.append(w) # 放入队列中 seen.add(w) parent[w] = head # 记录节点的父亲节点(父亲节点是head，w是与父亲节点相连接的点) # print(head) return parentparent1 = BFS(graph1, 'E')v = 'B' # 从B开始print(parent1)while v is not None: # 只要v不是空，就一直去打印它的父亲节点 print(v) v = parent1[v] DFS（一条路走到黑）（回溯法）DFS的算法思想： 1.选择一个根 ，并放入堆栈中 2.将堆栈位于top的 节点拿出来，并把与目前top节点相连的节点压栈 3.重复2 python实现： 123456789101112def DFS(graph, s): stack = [s] seen = set() seen.add(s) while len(stack) &gt; 0: top = stack.pop() # 2. top(栈顶) pop出来 nodes = graph[top] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(top) Dijkstra 讲解利用priority queue (优先队列) 思路：1.先确定一个起点，并初始化一个parent字典用以记录父节点 ​ 2.把与起点(或pop出的点)相连接的节点放入priority queue中，放入后会自动根据距离排序(此时节点中要包含与起点的距离) ​ 3.在priority queue中pop出距离最短的节点(其余节点均保留在队列之中)（若pop出的节点已经输出则舍去不用），并记录父节点 ​ 4.重复2 3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import heapqimport mathgraph1 = &#123; 'A': &#123;'B': 5, 'C': 1&#125;, 'B': &#123;'A': 5, 'C': 2, 'D': 1&#125;, 'C': &#123;'A': 1, 'B': 2, 'D': 4, 'E': 8&#125;, 'D': &#123;'B': 1, 'C': 4, 'E': 3, 'F': 6&#125;, 'E': &#123;'C': 8, 'D': 3&#125;, 'F': &#123;'D': 6&#125;&#125;# 初始化distance 把除了根节点以外的节点到根节点的距离设置为无穷def init_distnace(graph, s): distance = &#123;s: 0&#125; for node in graph: if node != s: distance[node] = math.inf return distancedef dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue, (0, s)) # 1.把根进入优先队列 seen = set() parent = &#123;s: None&#125; distance = init_distnace(graph, s) while len(pqueue) &gt; 0: # 2. head pop出来(注意pop出来的东西是一对的) head_pair = heapq.heappop(pqueue) dist = head_pair[0] head = head_pair[1] seen.add(head) # 这里的seen与bfs中的位置不同，这里只有当点被弹出的时候才能认为这个点被看到 # 并把与head相连的节点遍历出来(要注意此时是.keys()因为graph里面的值此时也是一个字典) nodes = graph[head].keys() for w in nodes: if w not in seen: if dist + graph[head][w] &lt; distance[w]: # distance存的是根到w的最短距离 graph[head][w]为两点之间的距离 heapq.heappush(pqueue, (dist + graph[head][w], w)) # 下面两组数据即为要输出的结果 parent[w] = head distance[w] = dist + graph[head][w] return parent, distanceparent1, distance1 = dijkstra(graph1, 'A')print(parent1)print(distance1) 有关return的一个小陷阱https://blog.csdn.net/csdniter/article/details/90694394 由于return返回值会传递给上一层函数，而上一层函数没有return命令，故会返回None值给最外层，所以结果是None找到原因之后，再加一个return命令即可 测试函数编写链表创建和遍历 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = Nonedef BuildListNode(numbers): # convert that list into linked list dummyRoot = ListNode(0) ptr = dummyRoot for number in numbers: ptr.next = ListNode(number) ptr = ptr.next ptr = dummyRoot.next return ptrdef TrvalListNode(node): if not node: return "[]" result = "" while node: result += str(node.val) + ", " node = node.next return "[" + result[:-2] + "]"if __name__ == '__main__': numbers = [1,2,3,4] s = BuildListNode(numbers) print(TrvalListNode(s)) 使用举例 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Nonefrom ListNodeBuild import *class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: res = [] while l1: res.append(l1.val) l1 = l1.next while l2: res.append(l2.val) l2 = l2.next res.sort() root = ListNode(0) ptr = root for number in res: ptr.next = ListNode(number) ptr = ptr.next ptr = root.next return ptrif __name__ == '__main__': l1 = BuildListNode([1,2,4]) l2 = BuildListNode([1,3,4]) s = Solution().mergeTwoLists(l1,l2) print(TrvalListNode(s)) 二叉树树的创建和遍历 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Definition for a binary tree node.class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None# 创建树def build(data): if not data: return None root = TreeNode(data) nodeQueue = [root] front = 0 index = 1 while index &lt; len(data): node = nodeQueue[front] front = front + 1 item = data[index] index = index + 1 if item: leftNumber = item node.left = TreeNode(leftNumber) nodeQueue.append(node.left) if index &gt;= len(data): break item = data[index] index = index + 1 if item: rightNumber = item node.right = TreeNode(rightNumber) nodeQueue.append(node.right) return rootdef travelTree(root): if not root: return "[]" output = "" queue = [root] current = 0 while current != len(queue): node = queue[current] current = current + 1 if not node: output += "null, " continue output += str(node.val) + ", " queue.append(node.left) queue.append(node.right) return "[" + output[:-2] + "]" 使用举例 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Nonefrom typing import Listfrom BuildTree import *class Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if len(preorder) == 0: return None # 先把根找到 (注意根root是个节点不是一个数) root = TreeNode(preorder[0]) mid = inorder.index(preorder[0]) root.left = self.buildTree(preorder[1:mid+1], inorder[:mid]) root.right = self.buildTree(preorder[mid+1:], inorder[mid+1:]) return rootif __name__ == "__main__": preo = [3,9,20,15,7] ino = [9,3,15,20,7] root = Solution().buildTree(preo, ino) s = travelTree(root) print(s) 数学50. Pow(x, n)1234567891011121314151617181920class Solution: def myPow(self, x: float, n: int) -&gt; float: if n == 0: return 1 if n == 1: return x if n &gt; 0: half = self.myPow(x, n//2) if n % 2 == 0: # 说明n能被2整除 return half*half else: return half*half*x if n &lt; 0: n = -n x = 1/x half = self.myPow(x, n//2) if n % 2 == 0: # 说明n能被2整除 return half*half else: return half*half*x 总结： 分情况讨论，注意是用到了分治的思想。 412.FizzBuzz1234567891011121314151617181920class Solution: def fizzBuzz(self, n: int) -&gt; List[str]: res = [] # if n == 1: # res.append("1") for i in range(1, n+1): if i%3 == 0 and i%5 == 0: res.append("FizzBuzz") continue elif i%3 == 0 and i%5 != 0: res.append("Fizz") continue elif i%3 != 0 and i%5 == 0: res.append("Buzz") continue else: res.append(str(i)) continue return res 326 3的幂1234567891011121314class Solution: def isPowerOfThree(self, n: int) -&gt; bool: if n == 1: return True if n &lt;= 2: return False while n &gt; 2: if n%3 != 0: return False n = n/3 if n == 1: return True else: return False 总结: 注意以下几个问题，1.小于等于0和2都不是3的幂，1是3的0次幂；2.最后要再加一次判断，n==1是True 列表 sort() 无返回值 l=list(s) 把s转换成列表9.回文数 20-1-4 12345678910111213141516class Solution: def isPalindrome(self, x: int) -&gt; bool: if x &gt;=0 : num = 0 a = abs(x) while(a != 0): temp = a % 10 #保存余数 a = a//10 num = num * 10 + temp if num == x: return True else: return False else: return False 总结：与上一题类似，需要注意在a除10的时候是整除，即a = int(a/10)ora = a // 10 11.盛最多水的容器123456789101112131415class Solution: def maxArea(self, height: List[int]) -&gt; int: left = 0 right = len(height) - 1 result = 0 while left &lt; right: area = (right - left) * min(height[left],height[right]) if result &lt; area: result = area if height[left] &gt; height[right]: right -= 1 else: left += 1 return result 总结：1.min()可以找到最小值； ​ 2.一开始就已经把指针定义在两端，如果短指针不动，而把长指针向着另一端移动，两者的距离已经变小了，无论会不会遇到更高的指针，结果都只是以短的指针来进行计算。 故移动长指针是无意义的。 22. 括号生成123456789101112131415161718192021class Solution: def generateParenthesis(self, n: int) -&gt; List[str]: if n == 0: return [] result = [] self.helpler(n, n, '', result) return result def helpler(self, l, r, item, result1): # l: 左边剩余括号数量 # r: 右边剩余括号数量 # item: 现在输出的结果(不一定是对的) # result1: 最后要return的结果 if l &gt; r: return if l == 0 and r == 0: result1.append(item) if l &gt; 0: self.helpler(l-1, r, item + '(', result1) if r &gt; 0: self.helpler(l, r-1, item + ')', result1) 总结: 1.搞懂实例中的变量和函数 注意self.的用法 26. 删除排序数组中的重复项123456789class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: j=len(nums) k=list(set(nums)) k.sort() for i in k: nums.append(i) del nums[0:j] return len(nums) 总结: 1.为了避开原地 用了把新的加进去，再删除前面的j个； 2.注意set(nums)是把nums这个数组变成一个无序的集合，list()又将其转化为数组； 3.k.sort()可以对数组进行排序。 122. 买卖股票的最佳时机 II12345678class Solution: def maxProfit(self, prices: List[int]) -&gt; int: profit = 0 for i in range(len(prices)-1): if prices[i] &lt; prices[i+1]: profit += (prices[i+1] - prices[i]) return profit 总结: 一句话，涨就买，跌就不管，把所有涨价的日子都加进来！ 189. 旋转数组1234567891011121314class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: """ Do not return anything, modify nums in-place instead. """ j = len(nums) if k &gt; j: for i in range(j-k%j): nums.append(nums[i]) del nums[0:j-k%j] else: for i in range(j-k): nums.append(nums[i]) del nums[0:j-k] 217. 存在重复元素1234567class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: nums2 = set(nums) if len(nums) == len(nums2): return False else: return True 724.寻找数组的中心索引123456789class Solution: def pivotIndex(self, nums: List[int]) -&gt; int: total = sum(nums) part_sum = 0 for i, j in enumerate(nums): if part_sum == (total - j) / 2: return i part_sum += j return -1 总结: 498.对角线遍历123456789101112131415161718192021222324252627class Solution: def findDiagonalOrder(self, matrix: List[List[int]]) -&gt; List[int]: res = [] M = len(matrix) N = len(matrix[0]) if M == 0: return [] for c in range(M+N-1): # c代表第几个对角线 # 一开始先不考虑对角线顺序的问题 if c+1 &lt;= N: # 说明还在第一行 row_begin = 0 else: row_begin = c-N+1 # 第二/三/..行最后一列 if c+1 &lt;= M: row_end = c else: row_end = M # 最后半部分结尾的行一定是M if c%2 == 1: for i in range(row_begin, row_end+1): res.append(matrix[i][c-i]) else: for i in range(row_end, row_begin+1,-1): res.append(matrix[i][c-i]) return res 54.螺旋矩阵1234567class Solution: def spiralOrder(self, matrix: List[List[int]]) -&gt; List[int]: res = [] while matrix: res += matrix.pop(0) matrix = list(map(list, zip(*matrix)))[::-1] return res 总结:1.Python变量前’‘和’*‘的作用 2.Python zip() 函数 3.python 中的[:-1]和[::-1] 169. 多数元素方法一： 1234class Solution: def majorityElement(self, nums: List[int]) -&gt; int: nums.sort() return nums[len(nums)//2] 方法二： 123456789101112class Solution: def majorityElement(self, nums: List[int]) -&gt; int: candidate, cnt = nums[0], 1 for i in range(1, len(nums)): if nums[i] == candidate: cnt += 1 else: cnt -= 1 if cnt == 0: candidate = nums[i] cnt = 1 return candidate 字符串 字符串替换 s = s.replace(旧, 新, 个数) 只保留字符串中的字母和数字，并且变成小写 s_n = [*filter(str.isalnum, s.lower())] filter(function, iterable) 3.无重复字符的最长子串123456789101112131415161718class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: dist = &#123;&#125; start = -1 max = 0 #先做一个遍历 for i in range(len(s)): # range（0， 5） 是[0, 1, 2, 3, 4]没有5 #分两种情况 #1.要判断的字符已经在字典中，and是为了保证是在新的子串中，和之前计算过的子串区分开 if s[i] in dist and dist[s[i]] &gt; start: start = dist[s[i]] # 主要是后面i-start 其中的start是代表的重复的那个字母之前的字母 dist[s[i]] = i #2.要判断的字符未在字典中,如果未在判断是否需要更新max else: if i-start &gt; max: max = i - start dist[s[i]] = i return max 总结：1.start要设置成-1，作为只有一个值的时候用i-start（0-1） 2.start = dist[s[i]] 保证star是从前一个值开始，就像-1一样 7.整数反转方法一： 12345678910111213141516class Solution: def reverse(self, x: int) -&gt; int: num = 0 x_abs = abs(x) while(x_abs != 0): tmp = x_abs % 10 num = tmp + num*10 x_abs = int(x_abs/10) if x &gt; 0 and num &lt;= 2**31-1: return num elif x &lt; 0 and num &lt;= 2**31: return -num else: return 0 方法二： 12345678910class Solution: def reverse(self, x: int) -&gt; int: if x &lt; 0: x = -1*x x = int("-" + str(x)[::-1]) else: x = int(str(x)[::-1]) if x &gt; 2**31-1 or x &lt; -2**31: return 0 return x 总结：1.方法一中注意是 x_abs = int(x_abs/10)否则x_abs的数值将一直保留小数，或者改成x_abs//10 ​ 2.方法二中是把输入结果转换成字符串，再通过str(x)[::-1]就可以直接把字符串反向输入 ​ 3. ** 代表平方 ## 123456789101112class Solution: def isPalindrome(self, s: str) -&gt; bool: s_n = [*filter(str.isalnum, s.lower())] n = len(s_n) i=0 while i &lt; n-i-1: if s_n[i] == s_n[n-i-1]: i += 1 continue else: return False return True 242. 有效的字母异位词方法一: 123456789class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: sl, tl = list(s), list(t) sl.sort() tl.sort() if sl == tl: return True else: return False 方法二: 1234567891011class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: k = len(t) if k != len(s): return False for i in range(k): if s[i] not in t: return False else: t = t.replace(s[i], '', 1) return True 344. 反转字符串12345678910111213141516class Solution: def reverseString(self, s: List[str]) -&gt; None: """ Do not return anything, modify s in-place instead. """ if len(s)==0 or len(s)==1: return s j = len(s)-1 i = 0 while i&lt;j: tmp = s[i] s[i] = s[j] s[j] = tmp j -= 1 i += 1 return s 总结: 利用双字符串，设置中间量。 387. 字符串中的第一个唯一字符123456class Solution: def firstUniqChar(self, s: str) -&gt; int: for index, c in enumerate(s): if c not in s[:index] and c not in s[index+1:]: return index return -1 38. 外观数列1234567891011121314151617181920212223242526272829class Solution: def countAndSay(self, n: int) -&gt; str: # 设置一个函数来每次计算新的seq if n == 1: return '1' seq = '1' for i in range(n-1): seq = self.helper(seq) return seq def helper(self, seq): count = 1 i = 0 res = '' # 注意下面两个循环的设置，第一个while保证了可以循环的最后一位 # 第二个while保证了循环不会出界(两个判断条件不可调换) while i &lt; len(seq): count = 1 while i &lt; len(seq)-1 and seq[i] == seq[i+1]: count += 1 i += 1 res += str(count) + seq[i] i += 1 return resif __name__ == '__main__': s = 6 print(Solution().countAndSay(s)) 15.三数之和1234567891011121314151617181920212223242526272829303132333435class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: n = len(nums) if n &lt; 3: return [] res = [] nums.sort() for i in range(n - 2): if nums[i] + nums[i + 1] + nums[i + 2] &gt; 0: break if i &gt; 0 and nums[i] == nums[i - 1]: continue l, r = i + 1, n - 1 while l &lt; r: if nums[i] + nums[l] + nums[r] == 0: res.append([nums[i], nums[l], nums[r]]) while l+1 &lt; r and nums[l] == nums[l+1]: l += 1 l += 1 while l &lt; r-1 and nums[r] == nums[r-1]: r -= 1 r -= 1 elif nums[i] + nums[l] + nums[r] &gt; 0: if l &lt; r - 1: r -= 1 else: break elif nums[i] + nums[l] + nums[r] &lt; 0: if l + 1 &lt; r: l += 1 else: break return res 总结：本题的难点在于如何去除重复解。这里有3个帮助去重的地方。 字典1.两个数的和12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dist=&#123;&#125; for i in range(len(nums)): if (target - nums[i]) not in dist: #对健判断 dist[nums[i]] = i # num[i]是键 i是值 else: return [dist[target - nums[i]],i] #返回的是键的=值 总结：Python 字典 in 操作符用于判断键是否存在于字典中 20. 有效的括号12345678910111213class Solution: def isValid(self, s: str) -&gt; bool: stack = [] lookup = &#123;'(': ')', '[': ']', '&#123;': '&#125;'&#125; for ss in s: if ss in lookup: stack.append(ss) elif len(stack) == 0 or ss != lookup[stack.pop()]: return False return len(stack) == 0 总结： 1. 最后的return 要考虑到{ （此时最后一个return为Falsee） 17.电话号码的字母组合方法一： 123456789101112131415161718class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: if digits == '': return [] d = &#123;'2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz'&#125; result = [''] for digit in digits: # 先遍历出来输入的所有字符串 tmp_list = [] for ch in d[digit]: # 再把第一个字符串内的字符遍历出来 for ch2 in result: # 将上面遍历出来的字符串和result内的字符串进行串联 tmp_list.append(ch2 + ch) result = tmp_list return result 总结: 1.此方法需要三个循环遍历，需要注意的是 把新遍历出的字符串与之前存在result里的字符串进行串联 方法二： 12345678910111213141516171819class Solution: def letterCombinations(self, digits: str) -&gt; List[str]: phone = &#123;'2': ['a', 'b', 'c'], '3': ['d', 'e', 'f'], '4': ['g', 'h', 'i'], '5': ['j', 'k', 'l'], '6': ['m', 'n', 'o'], '7': ['p', 'q', 'r', 's'], '8': ['t', 'u', 'v'], '9': ['w', 'x', 'y', 'z']&#125; res = [] def dfs(combination, index = 0): if len(digits) == index: res.append(combination) return for letter in phone[digits[index]]: dfs(combination + letter, index + 1) if digits: dfs('', 0) return res 总结：1.回溯是一种通过穷举所有可能情况来找到所有解的算法。如果一个候选解最后被发现并不是可行解，回溯算法会舍弃它，并在前面的一些步骤做出一些修改，并重新尝试找到可行解; ​ 2.digits[index]的意思的digits这个字符串的第几个字符; ​ 3.特别注意这里的return的位置(详细解释见前面，不过这里的return是没有返回值的，不会出现return None的结果)：return的作用是将函数结果返回，即退出def函数模块。 12.整数转罗马数字123456789101112131415161718class Solution: def intToRoman(self, num: int) -&gt; str: # values = [1000, 900, 500, 400, 100...] # numberals = ['M', 'CM', 'D', 'CD', 'C'...] # 1994 &gt;? 1000 =&gt; M # 994 &gt;? 1000 X # 994 &gt;? 900 =&gt; MCM # 94 &gt;? 900 X # 94 &gt;? 500 X .... values = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1] numberals = ['M', 'CM', 'D', 'CD','C', 'XC', 'L', 'XL', 'X', 'IX', 'V', 'IV', 'I'] result = '' for i in range(0, len(values)): while num &gt;= values[i]: num -= values[i] result += numberals[i] return result 1234567891011121314151617181920212223242526272829class Solution: def intToRoman(self, num: int) -&gt; str: # values = [1000, 900, 500, 400, 100...] # numberals = ['M', 'CM', 'D', 'CD', 'C'...] # 1994 &gt;? 1000 =&gt; M # 994 &gt;? 1000 X # 994 &gt;? 900 =&gt; MCM # 94 &gt;? 900 X # 94 &gt;? 500 X .... num_dict=&#123;1:'I', 4:'IV', 5:'V', 9:'IX', 10:'X', 40:'XL', 50:'L', 90:'XC', 100:'C', 400:'CD', 500:'D', 900:'CM', 1000:'M' &#125; result = '' for i in sorted(num_dict.keys())[::-1]: while num &gt;= i: num -= i result += num_dict[i] return result 总结：1.尽可能多列出所有可能性，然后从大向减 ​ 2.若要使用字典应该注意，字典是没有索引的，先使用sorted(num_dict.keys())[::-1]排好序，再遍历其key， 这时候i就是key num_dict[i就是其值 ｛i : ‘num_dict[i]’｝ 13.罗马数字转整数123456789101112131415161718class Solution: def romanToInt(self, s: str) -&gt; int: numberal_map = &#123;'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000&#125; result = 0 for i in range(len(s)): if i == 0: result += numberal_map[s[i]] elif i &gt; 0 and numberal_map[s[i]] &lt;= numberal_map[s[i-1]]: result += numberal_map[s[i]] else: result += numberal_map[s[i]] - 2* numberal_map[s[i-1]] return result 总结：1.当numberal_map[s[i]] &gt; numberal_map[s[i-1]]时，此时result内已经加上了i-1时的值，所以这时候要减去2倍的i-1的值;(eg:IV 在第一个字符传进来的时候result内值为I，再第二个值传进来的时候本来是I+V，但是我们需要的是V-I所以要减去二倍的之前传过来的值。) 要注意i == 0的情况。(是＝=哦) 哈希表705. 设计哈希集合123456789101112131415161718192021222324252627282930313233343536class MyHashSet: def __init__(self): self.data = [None for _ in range(1000)] # 相当于创建1000个None def add(self, key: int) -&gt; None: k = key % 1000 # data[k] 里存的可能也是一个列表 if self.data[k] == None: self.data[k] = [key] # 这边是赋的一个list # # 可能这个data[k]里已经存了多个key值了 for val in self.data[k]: if val == key: return # 如果data[k] != None self.data[k].append(key) # 注意这里，一般情况下是使用data.append()加在列表的最后。而这里面 data[k]内存的就是一个列表，相当于这里的data[k]是个list。 def remove(self, key: int) -&gt; None: k = key % 1000 if self.data[k]: for i, val in enumerate(self.data[k]): # 这里的i是起到一个计数的作用 if val == key: self.data[k].remove(key) # list[].remove() break def contains(self, key: int) -&gt; bool: k = key % 1000 if self.data[k]: for val in self.data[k]: if val == key: return True return False 706. 设计哈希映射12345678910111213141516171819202122232425262728293031323334353637383940414243444546class MyHashMap: def __init__(self): """ Initialize your data structure here. """ self.keys = [] self.values = [] def put(self, key: int, value: int) -&gt; None: """ value will always be non-negative. """ if key in self.keys: # self.keys.index(key) --&gt;找到key所在的list keys的索引值，因为一对键值对对应的索引值是相同的 self.values[self.keys.index(key)] = value # Upgrade else: self.keys.append(key) self.values.append(value) def get(self, key: int) -&gt; int: """ Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key """ if key in self.keys: return self.values[self.keys.index(key)] else: return -1 def remove(self, key: int) -&gt; None: """ Removes the mapping of the specified value key if this map contains a mapping for the key """ if key in self.keys: idx = self.keys.index(key) self.keys.pop(idx) self.values.pop(idx)# # Your MyHashMap object will be instantiated and called as such:# # obj = MyHashMap()# # obj.put(key,value)# # param_2 = obj.get(key)# # obj.remove(key) 771. 宝石与石头12345678class Solution: def numJewelsInStones(self, J: str, S: str) -&gt; int: count = 0 Jset = set(J) for s in S: if s in Jset: count += 1 return count 总结: 这道题用哈希集合可以简化思维过程，即记下在哈希集合中的宝石数目。 链表2.两数相加1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: carry = 0 d=ListNode(0) #初始化一个预先指针，目的为了return时是从开头 开始 p = d while l1 and l2 : #l1,2 不为None p.next = ListNode((l1.val + l2.val + carry) % 10) # p.next是链表处的值 carry = (l1.val + l2.val + carry) // 10 l1 = l1.next l2 = l2.next p = p.next if l1: # 防止出现位数不匹配 while l1: p.next = ListNode((l1.val + carry) % 10) carry = (l1.val + carry) // 10 l1 = l1.next p = p.next if l2: while l2: p.next = ListNode(( l2.val + carry) % 10) carry = (l2.val + carry) // 10 l2 = l2.next p = p.next if carry: #防止出现最后还有一个进位 p.next = ListNode(1) return d.next 总结：1.对于链表问题，返回结果为头结点时，通常需要先初始化一个预先指针 dummy，该指针的下一个节点指向真正的头结点head (即第一次用p.next=…的地方)。使用预先指针的目的在于链表初始化时无可用节点值，而且链表构造过程需要指针移动，进而会导致头指针丢失，无法返回结果。 206. 反转链表12345678910111213141516# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: cur = head prv = None while cur: tmp = cur.next cur.next = prv prv = cur cur = tmp return prv 总结： 1.我们可以申请两个指针，第一个指针叫 prv，最初是指向 None 的。 第二个指针 cur 指向 head，然后不断遍历 cur。 每次迭代到 cur，都将 cur 的 next 指向 pre，然后 prv 和 cur 前进一位。 都迭代完了(cur 变成 null 了)，prv 就是最后一个节点了。 234. 回文链表1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def isPalindrome(self, head: ListNode) -&gt; bool: res = [] while head: res.append(head.val) head = head.next i = 0 k = len(res)-1 while i &lt; k: if res[i] != res[k]: return False i += 1 k -= 1 return True 24.两两交换链表中的节点1234567891011class Solution: def swapPairs(self, head: ListNode) -&gt; ListNode: thead = ListNode(-1) thead.next = head c = thead while c.next and c.next.next: a, b=c.next, c.next.next c.next, a.next = b, b.next b.next = a c = c.next.next return thead.next https://pic.leetcode-cn.com/43254846f029b4814a6c9a139e4f9f89833ac54803ea50b24feb35210631f88b-a.jpg 总结： 1.参考的此方法，通过建立一个额外的空头节点c 21. 合并两个有序链表123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: res = [] while l1: res.append(l1.val) l1 = l1.next while l2: res.append(l2.val) l2 = l2.next res.sort() root = ListNode(0) ptr = root for number in res: ptr.next = ListNode(number) ptr = ptr.next ptr = root.next return ptr 237. 删除链表中的节点1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteNode(self, node): """ :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. """ node.val = node.next.val node.next = node.next.next 总结：阅读理解 19. 删除链表的倒数第N个节点12345678910111213141516171819202122232425# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def removeNthFromEnd(self, head: ListNode, n: int) -&gt; ListNode: if head.next == None: return None p1= head i = 0 while p1: i += 1 p1 = p1.next p2= head while i-n-1 &gt; 0: i -= 1 p2 = p2.next if i-n-1 == -1: head = head.next else: p2.next = p2.next.next return head 206. 反转链表12345678910111213141516# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: pre = None cur = head while cur: tmp = cur.next cur.next = pre pre = cur cur = tmp return pre 141. 环形链表12345678910111213class Solution(object): def hasCycle(self, head): if(head == None or head.next == None): return False node1 = head node2 = head.next while(node1 != node2): if(node2 == None or node2.next == None): return False node1 = node1.next node2 = node2.next.next return True 总结：通过两个指针 一快一慢。注意什么时候判断为False 方法二：哈希值法, 空间复杂度O(n) 这个很好考虑, 把遍历过的节点记录,当发现遍历的节点下一个节点遍历过, 说明有环 12345678910111213def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ lookup = set() p = head while p: lookup.add(p) if p.next in lookup: return True p = p.next return False 队列和栈622. 设计循环队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class MyCircularQueue: def __init__(self, k: int): """ Initialize your data structure here. Set the size of the queue to be k. """ self.f,self.r = 0,0 self.k = k + 1 self.__li = [None for _ in range(self.k)] def enQueue(self, value: int) -&gt; bool: """ Insert an element into the circular queue. Return true if the operation is successful. """ if not self.isFull(): self.__li[self.r] = value self.r = (self.r + 1) % self.k return True else: return False def deQueue(self) -&gt; bool: """ Delete an element from the circular queue. Return true if the operation is successful. """ if not self.isEmpty(): self.f = (self.f + 1) % self.k return True else: return False def Front(self) -&gt; int: """ Get the front item from the queue. """ if not self.isEmpty(): return self.__li[self.f] else: return -1 def Rear(self) -&gt; int: """ Get the last item from the queue. """ if not self.isEmpty(): return self.__li[(self.r - 1 + self.k) % self.k] # 防止减完一变成负的 else: return -1 def isEmpty(self) -&gt; bool: """ Checks whether the circular queue is empty or not. """ return self.r==self.f def isFull(self) -&gt; bool: """ Checks whether the circular queue is full or not. """ return (self.r + 1) % self.k == self.f# Your MyCircularQueue object will be instantiated and called as such:# obj = MyCircularQueue(k)# param_1 = obj.enQueue(value)# param_2 = obj.deQueue()# param_3 = obj.Front()# param_4 = obj.Rear()# param_5 = obj.isEmpty()# param_6 = obj.isFull() 215. 数组中的第K个最大元素12345class Solution: def findKthLargest(self, nums: List[int], k: int) -&gt; int: import heapq # 使用堆的nlargest(n,iter)返回前n个最大的数,倒序排练 return heapq.nlargest(k,nums)[-1] 总结: 使用heapq库，详细的函数见前面知识点。 295. 数据流的中位数1234567891011121314151617181920212223# import heapq# class MedianFinder:# def __init__(self):# self.h = []# def addNum(self, num: int) -&gt; None:# heapq.heappush(self.h, num)# def findMedian(self) -&gt; float:# if len(self.h)%2!=0:# return heapq.nlargest(len(self.h)//2+1,self.h)[-1]# else:# return ((heapq.nlargest(len(self.h)//2, self.h)[-1]+heapq.nlargest(len(self.h)//2+1, self.h)[-1])/2)class MedianFinder: def __init__(self): self.store = [] def addNum(self, num: int) -&gt; None: self.store.append(num) def findMedian(self) -&gt; float: self.store.sort() n = len(self.store) if n &amp; 1 == 1: # n 是奇数 return self.store[n // 2] else: return (self.store[n // 2 - 1] + self.store[n // 2]) / 2 排序和搜索88. 合并两个有序数组123456789class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: """ Do not return anything, modify nums1 in-place instead. """ del nums1[m:] nums1 += nums2 nums1.sort() return nums1 总结：sort 与 sorted 区别： sort 是应用在 list 上的方法，属于列表的成员方法，sorted 可以对所有可迭代的对象进行排序操作。 list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。 sort使用方法为ls.sort()，而sorted使用方法为sorted(ls) 278. 第一个错误的版本 12345678910111213141516171819202122232425262728293031 # The isBadVersion API is already defined for you.# @param version, an integer# @return a bool# def isBadVersion(version):class Solution: def firstBadVersion(self, n): """ :type n: int :rtype: int """ # 找到最小的那个True # 下面两句话就相当于，只要是True就继续往左走，只要是False就继续往右走 # 判断 isBadVersion(mid): # True: 更新right = mid (因为此时的right可能就是想要的结果) # False: 更新left = mid + 1(因为第一个True一定在mid右边)) left, right = 1, n while right - left &gt; 1: # 将left和right分别锁定成两个临近值，最后对着两个值进行单独判断 if isBadVersion((left + right)//2) : right = (left + right)//2 else: left = (left + right)//2 + 1 if isBadVersion(left): return left else: return right 二叉树144. 二叉树的前序遍历方法一：递归 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.result.append(root.val) self.helper(root.left) self.helper(root.right) 总结: 1.我们针对之前所学习的print的那个版本进行修改，通过建立一个数组(result)和一个helper函数。注意这里的result数组的值不能被清零。 方法二：(迭代) 123456789101112131415161718class Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: if not root: return [] stack = [root] res = [] while stack: node = stack.pop() res.append(node.val) if node.right: stack.append(node.right) if node.left: stack.append(node.left) return res 94. 二叉树的中序遍历12345678910111213141516171819# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.helper(root.left) self.result.append(root.val) self.helper(root.right) 145. 二叉树的后序遍历123456789101112131415161718# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def postorderTraversal(self, root: TreeNode) -&gt; List[int]: self.result=[] self.helper(root) return self.result def helper(self, root): if not root: return [] self.helper(root.left) self.helper(root.right) self.result.append(root.val) 102. 二叉树的层次遍历123456789101112131415161718192021222324252627282930# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if root is None: return [] que = [root] result = [] tmp = [] while que: n= len(que) for i in range(n): node = que.pop(0) tmp.append(node.val) if node.left is not None and node.right is not None: que.append(node.left) que.append(node.right) elif node.left is None and node.right is not None: que.append(node.right) elif node.left is not None and node.right is None: que.append(node.left) result.append(tmp) tmp=[] return result 总结: 1.首先看答案的结果，是一层当作一个列表进行输出，而不是一个一个输出，所以这里用了一个tmp列表，并且每层都清零一次 2. 我这个迭代的方法是每层把队列里的值一次性都清空，这样就变成一层一层的了。 104. 二叉树的最大深度123456789101112131415# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def maxDepth(self, root: TreeNode) -&gt; int: if root is None: return 0 # 运用递归的方法，先看假如是一个子树如何处理 left_d = self.maxDepth(root.left) right_d = self.maxDepth(root.right) return max(left_d, right_d)+1 101. 对称二叉树1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: if root is None: return True que = [root, root] while que: node1 = que.pop(0) node2 = que.pop(0) # 如果两个节点都是空就继续循环，有一个是空就False if not (node1 or node2): continue if not (node1 and node2): return False if node1.val != node2.val: return False # 将左节点的左孩子和右节点的右孩子入队 que.append(node1.left) que.append(node2.right) # 将左节点的右孩子和右节点的左孩子入队 que.append(node1.right) que.append(node2.left) return True 总结: 与普通的迭代遍历不同，这里要注意压入的左右孩子。 105. 从前序与中序遍历序列构造二叉树1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Nonefrom typing import Listfrom BuildTree import *class Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if len(preorder) == 0: return None # 先把根找到 (注意根root是个节点不是一个数) root = TreeNode(preorder[0]) mid = inorder.index(preorder[0]) root.left = self.buildTree(preorder[1:mid+1], inorder[:mid]) root.right = self.buildTree(preorder[mid+1:], inorder[mid+1:]) return rootif __name__ == "__main__": preo = [3,9,20,15,7] ino = [9,3,15,20,7] root = Solution().buildTree(preo, ino) s = travelTree(root) print(s) 136. 只出现一次的数字1234567891011121314from typing import Listclass Solution: def singleNumber(self, nums: List[int]) -&gt; int: hash_table = &#123;&#125; for i in nums: try: hash_table.pop(i) except: hash_table[i] = 1 return hash_table.popitem()[0]if __name__ == "__main__": nums = [4, 1, 2, 1, 2] print(Solution().singleNumber(nums)) 总结: 1. try except语句是当try中的语句发生错误时执行except内的语句。 2. 也可以使用异或，因为 相同数字 的异或 就是0 350. 两个数组的交集 II12345678class Solution: def intersect(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: result = [] for i in nums1: if i in nums2: result.append(i) nums2.remove(i) return result 方法二： 123class Solution: def intersect(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: return [*(collections.Counter(nums1) &amp; collections.Counter(nums2)).elements()] ## 123456789101112131415161718class Solution: def plusOne(self, digits): d=0 for x in digits: d = d*10 + int(x) # d = "" # for i in digits: # d =d+str(i) # d = int(d) d = d+1 res = [] for i in str(d): res.append(int(i)) return resif __name__ == '__main__': s = [9,9,9] print(Solution().plusOne(s)) 总结:此方法是把 str()–&gt;int()–&gt;str() 283. 移动零1234567891011121314class Solution: def moveZeroes(self, nums: List[int]) -&gt; None: """ Do not return anything, modify nums in-place instead. """ n = len(nums) i, j = 0, 0 while (i + j &lt; n): if nums[i] == 0: nums.pop(i) nums.append(0) j += 1 continue i += 1 动态规划70. 爬楼梯123456789101112class Solution: def climbStairs(self, n: int) -&gt; int: pre, cur = 0, 1 # n=1 - 1 # n=2 - 2 # n=3 - 3 # n=4 - 5 :1111 112 121 211 22 # n=5 - 8 :11111 1112 1121 1211 2111 122 212 221 # fib for i in range(n): pre, cur = cur, pre + cur return cur 总结：观察规律后发现是fib，事半功倍！ 121. 买卖股票的最佳时机12345678class Solution: def maxProfit(self, prices: List[int]) -&gt; int: max_profit, min_price = 0, float("inf") for price in prices: min_price = min(min_price, price) max_profit = max(max_profit, price - min_price) return max_profit 总结：我们都希望在当前时刻去看，要是能在xx号(之前极小值的时候)买入该多好啊！所以 两个变量，一个为最大利润，一个为最小值。 53. 最大子序和123456789101112class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: if max(nums) &lt; 0: return max(nums) local_max, global_max = 0, 0 for num in nums: local_max = max(0, local_max + num) # 大于0说明之前加的数还有意义，小于0之前的数就别要了 global_max = max(global_max, local_max) return global_max 总结：local_max代表着只要大于0，之前加的数就是有意义的 global_max一直记录着求和的最大值]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3实战Spark大数据分析及调度]]></title>
    <url>%2F2019%2F12%2F17%2FPython3%E5%AE%9E%E6%88%98Spark%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%8F%8A%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[spark的运行1.启动hadoop (要在root用户下运行) 123cd app/hadoopxxxxxxx/sbinstart-dfs.sh**可在 http://localhost:50070/ 中查看自己上传的文件** 2.启动spark 12cd app/sparkxxxxxxx/bin./spark-shell spark core核心 RDDSpark源码所在的地方：https://github.com/apache/spark 官网：xxxx.apache.org源码：https://github.com/apache/xxxx spark url查看 http://hadoop000:4040 什么是RDDabstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]]) extends Serializable with Logging 1）RDD是一个抽象类2）带泛型的，可以支持多种类型： String、Person、User RDD：Resilient Distributed Dataset 弹性 分布式 数据集 弹性：遇到故障以后可以进行修复或者跟踪 Represents an immutable：不可变(RDD不可变) partitioned collection of elements ：分区 Array(1,2,3,4,5,6,7,8,9,10) 3个分区： (1,2,3) (4,5,6) (7,8,9,10) that can be operated on in parallel： 并行计算的问题 单机存储/计算==&gt;分布式存储/计算1）数据的存储: 切割 HDFS的Block2）数据的计算: 切割(分布式并行计算) MapReduce/Spark3）存储+计算 : HDFS/S3+MapReduce/Spark ==&gt; OK RDD的五大特性：Internally, each RDD is characterized by five main properties: *1. *A list of partitions 一系列的分区/分片 *2. *A function for computing each split(切片)/partition y = f(x) rdd.map(_+1) :对RDD进行函数就是对RDD里所有的分区进行一个函数 3. A list of dependencies on other RDDs rdd1 ==&gt; rdd2 ==&gt; rdd3 ==&gt; rdd4 存在依赖关系(依赖其他RDD) dependencies:*\* rdda = 5个partition ==&gt;map操作 rddb = 5个partition 当遇到rdda内第三个分区丢失时，spark会通过dependency重新计算数据(从源文件重新读取) *4. *Optionally(可选的), a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) *5. *Optionally, a list of preferred locations(最佳的位置) to compute each split on (e.g. block locations for an HDFS file) 数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算 为什么location有s？ 答: spark会尽可能地将任务分配到数据块所存储的位置。Hadoop中读取数据生成RDD时，preferredLocation[s]返回每一个数据块所在的机器名或者IP地址，如果每一块数据是多份存储的，那么就会返回多个机器地址,以便后续调度的程序根据这个地址更加有效地分配任务. 五大特性源码体现：def compute(split: Partition, context: TaskContext): Iterator[T] 特性二def getPartitions: Array[Partition] 特性一def getDependencies: Seq[Dependency[_]] = deps 特性三def getPreferredLocations(split: Partition): Seq[String] = Nil 特性五val partitioner: Option[Partitioner] = None 特性四 图解RDD SparkContext&amp;SparkConf第一要务：创建SparkContext 连接到Spark“集群”：local、standalone、yarn、mesos 通过SparkContext来创建RDD、广播变量到集群 在创建SparkContext之前还需要创建一个SparkConf对象(conf-&gt;配置) RDD创建方式 Parallelized Collections External Datasets If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes 1）我们上课是在单节点上的：一个节点， hello.txt只要在这台机器上有就行了 2）standalone: Spark集群： 3个节点 local path 都是从节点的本地读取数据 不建议 IDE开发开发pyspark应用程序 1) IDE(Integrated Development Environment ): IDEA pycharm 2) 设置基本参数: python interceptor PYTHONPATH SPARK_HOME 2zip包 3）开发 4）使用local进行本地测试 提交pyspark应用程序($SPARK_HOME) ./spark-submit –master local[2] –name spark0301 /home/hadoop/script/spark0301.py 具体提交的详细说明参见：http://spark.apache.org/docs/latest/submitting-applications.html Spark任务提交全流程 1.Driver端启动SparkSubmit进程，启动后开始向Master进行通信，此时创建了一个对象（SparkContext），接着向Master发送任务消息2.Master接收到任务信息后，开始资源调度，此时会和所有的Worker进行通信，找到空闲的Worker，并通知Worker来拿取任务和启动相应的Executor3.Executor启动后，开始与Driver进行反向注册，接下来Driver开始把任务发送给相应的Executor，Executor开始计算任务 全流程: https://imgchr.com/i/ljBytg 1.调用SparkSubmit类，内部执行submit –&gt; doRunMain -&gt; 通过反射获取应用程序的主类对象 –&gt; 执行主类的main方法。2.构建SparkConf和SparkContext对象，在SparkContext入口做了三件事，创建了SparkEnv对象（创建了ActorSystem对象），TaskScheduler（用来生成并发送task给Executor），DAGScheduler（用来划分Stage）。3.ClientActor将任务信息封装到ApplicationDescription对象里并且提交给Master。4.Master收到ClientActor提交的任务信息后，把任务信息存在内存中，然后又将任务信息放到队列中。5.当开始执行这个任务信息的时候，调用scheduler方法，进行资源的调度。6.将调度好的资源封装到LaunchExecutor并发送给对应的Worker。7.Worker接收到Master发送过来的调度信息（LaunchExecutor）后，将信息封装成一个ExecutorRunner对象。8.封装成ExecutorRunner后，调用ExecutorRunner的start方法，开始启动 CoarseGrainedExecutorBackend对象。9.Executor启动后向DriverActor进行反向注册。10.与DriverActor注册成功后，创建一个线程池（ThreadPool），用来执行任务。11.当所有的Executor注册完成后，意味着作业环境准备好了，Driver端会结束与SparkContext对象的初始化。12.当Driver初始化完成后（创建了sc实例），会继续执行我们提交的App的代码，当触发了Action的RDD算子时，就触发了一个job，这时就会调用DAGScheduler对象进行Stage划分。13.DAGScheduler开始进行Stage划分。14.将划分好的Stage按照区域生成一个一个的task，并且封装到TaskSet对象，然后TaskSet提交到TaskScheduler。15.TaskScheduler接收到提交过来的TaskSet，拿到一个序列化器，对TaskSet序列化，将序列化好的TaskSet封装到LaunchExecutor并提交到DriverActor。16.把LaunchExecutor发送到Executor上。17.Executor接收到DriverActor发送过来的任务（LaunchExecutor），会将其封装成TaskRunner，然后从线程池中获取线程来执行TaskRunner。18.TaskRunner拿到反序列化器，反序列化TaskSet，然后执行App代码，也就是对RDD分区上执行的算子和自定义函数。 a example about SparkContext&amp;SparkConf123456789101112131415from pyspark import SparkConf,SparkContext# 创建SparkConf：设置的是Spark相关的参数信息conf = SparkConf().setMaster("local[2]").setAppName("spark0301")# 创建SparkContextsc = SparkContext(conf=conf)# 业务逻辑data = [1,2,3,4,5]distData = sc.parallelize(data) # parallelize的目的是为了把data转换成RDDprint(distData.collect())# 好的习惯sc.stop() RDD编程官网参考文件http://spark.apache.org/docs/latest/rdd-programming-guide.html RDD Programming Guide RDD Operation transformations: create a new dataset from an existing one RDDA —transformation–&gt; RDDB y = f(x) rddb = rdda.map(....) lazy(*****) (它不会立刻计算，相反，它仅仅记住作用到数据集中---直到遇到action( collect() )) rdda.map().filter()......collect map/filter/group by/distinct/..... actions: return a value to the driver program after running a computation on the dataset count/reduce/collect...... 1) transformation are lazy, nothing actually happens until an action is called; 2) action triggers the computation; 3) action returns values to driver or writes data to external storage;常用的RDD函数map: map(func) 将func函数作用到数据集的每一个元素上，生成一个新的分布式的数据集返回 word =&gt; (word,1)map()是将传入的函数依次作用到序列的每个元素，每个元素都是独自被函数“作用”一次 。 reduce()是将传人的函数作用在序列的第一个元素得到结果后，把这个结果继续与下一个元素作用（累积计算）。 简单说，map是对每个元素进行操作-返回一个列表；reduce是所有元素操作-返回一个结果。 filter: filter(func) 选出所有func返回值为true的元素，生成一个新的分布式的数据集返回 flatMap —-&gt;(压扁以后做map 先拆成多个部分) flatMap(func) 输入的item能够被map到0或者多个items输出，返回值是一个Sequence groupByKey： ​ 把相同的key的数据分发到一起​ [‘hello’, ‘spark’, ‘hello’, ‘world’, ‘hello’, ‘world’]​ (‘hello’,1) (‘spark’,1)…….. *reduceByKey: * ​ reduceByKey的作用对像是(key, value)形式的rdd，而reduce有减少、压缩之意，reduceByKey的作用就是对相同key的数据进行处理，最终每个key只保留一条记录。 ​ 把相同的key的数据分发到一起并进行相应的计算​ mapRdd.reduceByKey(lambda a,b:a+b)​ [1,1] 1+1​ [1,1,1] 1+1=2+1=3​ [1] 1 ​ 需求: 请按wc结果中出现的次数降序排列 sortByKey 按照Key进行排序​ (‘hello’, 3), (‘world’, 2), (‘spark’, 1) 答：完成上面的方法就是key和value交换 即lambda x：x[1],x[0] 后进行sortByKey， 最后再做一步map再交换回来 union： *join： * inner join outer join:left/right/full action: rdd.take(3) 显示前3个 rdd.sum()求和 rdd.max()最大值 rdd.count()计数 saveAsTextFile()以文件的形式保存 如何在本地运行spark(不用IDE，提交到本地)，并写到文件系统中 1.把在IDE中已经测试好的代码建立 .py（/mycode） 2.在到./spark中 执行—&gt;此时可以在控制台上显示 ./spark-submit --master local[2] --name spark04cp /home/hadoop/mycode/spark04cp.py(执行文件) file:///home/hadoop/data/hello.txt(要处理的数据) 3.在代码中加入 saveAsTextFile(sys.argv[2]) 在执行是最后再加入第三个参数，即为保存的目录 ./spark-submit --master local[2] --name spark04cp /home/hadoop/mycode/spark04cp.py(执行程序) file:///home/hadoop/data/hello*.txt(要处理的数据) file:///home/hadoop/data/tmp/wc(保存的处理后数据的位置) 算子综合实例：一、词频案例:wc 1) input: 1/n文件 文件夹 后缀名 hello spark hello hadoop hello welcome 2) 开发步骤分析 文本内容的每一行转成一个个的单词 : flatMap 单词 ==&gt; (单词, 1): map 把所有相同单词的计数相加得到最终的结果: reduceByKey 最后按照个数的升序排列 123456789101112131415161718192021222324252627282930import sysfrom pyspark import SparkConf, SparkContextif __name__ == '__main__': if len(sys.argv) != 3: # 等于1代表只有这个.py运行了，没有别的外部参数 print('Usage:wordcount &lt;input&gt;', file=sys.stderr) sys.exit(-1) # 如果没有文件输入就直接退出 conf = SparkConf() sc = SparkContext(conf=conf) # sys.argv[0] 代表运行的本程序名称 sys.argv[1]代表输入 print(sys.argv[0]) print(sys.argv[1]) def printResult(): counts = sc.textFile(sys.argv[1]) \ .flatMap(lambda line: line.split("\t")) \ .map(lambda x: (x, 1)) \ .reduceByKey(lambda a, b: (a + b))\ .map(lambda x: (x[1], x[0]))\ .sortByKey()\ .map(lambda x: (x[1], x[0])).saveAsTextFile(sys.argv[2]) printResult() sc.stop() 二、TopN 1) input : 1/n文件 文件夹 后缀名 2) 求某个维度的topn 3）开发步骤分析 文本内容的每一行根据需求提取出你所需要的字段： map 单词 ==&gt; (单词, 1): map 把所有相同单词的计数相加得到最终的结果: reduceByKey 取最多出现次数的降序： sortByKey 平均数：统计平均年龄id age3 964 445 676 47 98 开发步骤分析： 1) 取出年龄 map 2）计算年龄综合 reduce 3）计算记录总数 count 4）求平均数 spark运行模式 Local模式： 开发 123--master --name # 应用程序名称--py-files # 上传文件 ./spark-submit --master local[2] --name spark-local /home/hadoop/script/spark0402.py file:///home/hadoop/data/hello.txt file:///home/hadoop/wc/output standalone hdfs: NameNode DataNode yarn: ResourceManager NodeManager 1234567891011121314151617181920master: (主)worker: (从)$SPARK_HOME/conf/slaves hadoop000 假设你有5台机器，就应该进行如下slaves的配置 hadoop000 hadoop001 hadoop002 hadoop003 hadoop004 如果是多台机器，那么每台机器都在相同的路径下部署spark启动spark集群 $SPARK_HOME/sbin/start-all.sh ps: 要在spark-env.sh中添加JAVA_HOME，否则会报错 检查： jps： Master和Worker进程，就说明我们的standalone模式安装成功 webui： ./spark-submit --master spark://hadoop000:7077 --name spark-standalone /home/hadoop/script/spark0402.py hdfs://hadoop000:8020/wc.txt hdfs://hadoop000:8020/wc/output 如果使用standalone模式，而且你的节点个数大于1的时候，如果你使用本地文件测试，必须要保证每个节点上都有本地测试文件 (或者选择在hdfs内的文件)yarn mapreduce yarn spark on yarn 70% spark作为客户端而已，他需要做的事情就是提交作业到yarn上去执行 yarn vs standalone yarn： 你只需要一个节点，然后提交作业即可 这个是不需要spark集群的（不需要启动master和worker的） standalone：你的spark集群上每个节点都需要部署spark，然后需要启动spark集群（需要master和worker） ./spark-submit --master yarn --name spark-yarn /home/hadoop/script/spark0402.py hdfs://hadoop000:8020/wc.txt hdfs://hadoop000:8020/wc/output When running with master ‘yarn’ either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment 作业：试想：为什么需要指定HADOOP_CONF_DIR或者YARN_CONF_DIR 如何使得这个信息规避掉Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME yarn支持client和cluster模式：driver运行在哪里 client：提交作业的进程是不能停止的，否则作业就挂了 cluster：提交完作业，那么提交作业端就可以断开了，因为driver是运行在am里面的 Error: Cluster deploy mode is not applicable to Spark shells 12pyspark/spark-shell : 交互式运行程序 clientspark-sql 如何查看已经运行完的yarn的日志信息： yarn logs -applicationId (此ID在hadoop000:8088/cluster中可以查看)Log aggregation has not completed or is not enabled.参见：https://coding.imooc.com/class/chapter/128.html#Anchor JobHistory使用 不管你的spark应用程序运行在哪里，你的spark代码都是一样的，不需要做任何的修改和调整，所以spark使用起来是非常方便的！！！！！！ ##spark core 进阶 Spark核心概述 ​ ​ 用户提交Job后会生成SparkContext对象，SparkContext向Cluster Manager（在Standalone模式下是Spark Master）申请Executor资源，并将Job分解成一系列可并行处理的task，然后将task分发到不同的Executor上运行，Executor在task执行完后将结果返回到SparkContext。 (上图SparkContext 与 executor为什么是双向的是因为 当连接的时候， SparkContext acquires executors on nodes in the cluster, which are processes that run computations and store data for your application，同时 executor返回心跳信息，当一个exceutor挂掉的时候可以开启另外一个exceutor http://spark.apache.org/docs/latest/cluster-overview.html) 1.Application ：基于Spark的应用程序 = 1 driver + executors User program built on Spark. Consists of a driver program and executors on the cluster. spark0402.py （这就是一个应用程序） pyspark/spark-shell（这也是） 2.Driver program The process running the main() function of the application creating the SparkContext 3.Cluster manager An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN) spark-submit –master local[2]/spark://hadoop000:7077/yarn(后面这些东西就是集群管理) 4.Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Yarn-cluster VS Yarn-client*5.Worker node * Any node that can run application code in the cluster standalone: slave节点 slaves配置文件 yarn: nodemanager *6.Executor * A process launched for an application on a worker node runs tasks keeps data in memory or disk storage across them Each application has its own executors. *7.Task * A unit of work that will be sent to one executor 8.Job ** A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. //一个action对应一个job，理解：job里面多个task，运行在executor里**// 9.Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. 一个stage的边界往往是从某个地方取数据开始，到shuffle的结束 另外的解释方式 job定义：我们都知道，在spark rdd中，有action、transform操作，当真正触发action时，才真正执行计算，此时产生一个job任务。 stage定义：以shuffle为界，当在一个job任务中涉及shuffle操作时，会进行stage划分，产生一个或多个stage。 task定义： 一个stage可能包含一个或者多个task任务，task任务与partition、executor息息相关，即并行度。 partition定义： partition个数即rdd的分区数，不同的数据源读进来的数据分区数默认不同，可以通过repartition进行重分区操作。 executor定义： executor运行在work上，一个work可以运行一个或多个executor，一个executor可以运行一个或者多个task（取决于executor的core个数，默认是一个task占用一个core，即有多少个core就可以启动多少个task任务） 各个之间的关系图： ​ 一个work有一个或多个executor​ 一个executor有一个或者多个task（取决于executor的core个数）​ 一个task对应一个partition分区，即并行度（官网推荐partition分区数设置是task个数的2~3倍，充分利用资源）​ 一个stage有一个或多个task任务​ 一个job有一个或多个stage 理解：job里面多个task，运行在executor里 hadoop与spark对比 10.Spark Cache rdd.cache(): StorageLevel ​ cache它和tranformation: lazy 没有遇到action是不会提交作业到spark上运行的​ 如果一个RDD在后续的计算中可能会被使用到，那么建议cache​​ cache底层调用的是persist方法，传入的参数是：StorageLevel.MEMORY_ONLY​ cache=persist​​ unpersist: 立即执行的 11.Spark Lineage ​ 是一种RDD之间的依赖关系，可以有效的帮助容错。(当一个里面的RDD内的paritition丢失时，因为Lineage记录paritition的血缘关系) 12.Spark Dependency(依赖) ​ 窄依赖：一个父RDD的partition至多被子RDD的某个partition使用一次 (pipiline流水线) ​ 宽依赖：一个父RDD的partition会被子RDD的partition使用多次，有shuffle (宽依赖的容错效果更差) hello,1 hello,1 hello(相同的放在一起处理) world,1 hello,1 world world,1RDD的shuffle以及依赖关系 正如上图，hdfs在三台机器上运行，左边前3个操作都是窄依赖(lines、words、pairs), 当遇到reduceByKey(宽依赖)时，会进行shuffle操作，同时也会变成两个stage。 sc.textFile(“file:///home/hadoop/data/hello.txt”).flatMap(lambda line: line.split(“\t”)).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).collect() Spark Core调优HistorySever 优化12$SPARK_HOME/sbin/./start-history-server.sh 可以在./start-history-server.sh参看历史信息 序列化优化1.java 速度慢，简单，默认 2.kryo速度快，麻烦 内存管理优化两大类：执行和存储 (一般各占一半) 广播变量优化一个机器一个副本，可以减小副本的大小 ###数据本地性优化 如果数据和节点在同一台机器上时，运行速度最快 移动计算而不是移动数据！！ Spark SQLSpark SQL SQL: MySQL、Oracle、DB2、SQLServer 很多小伙伴熟悉SQL语言 数据量越来越大 ==&gt; 大数据(Hive、Spark Core-基于RDD) 直接使用SQL语句来对大数据进行分析：这是大家所追逐的梦想 person.txt ==&gt; 存放在HDFS 1,zhangsan,30 2,lisi,31 3,wangwu,32 hive表：person id:int name:string age:int 导入数据： load ..... 统计分析： select ... from person SQL on Hadoop Hive Impala: Cloudera Presto Drill ….. 12345678Hive: on MapReduce (性能差) SQL =(翻译)=&gt; MapReduce =(提交到)=&gt; Hadoop ClusterSpark SQL: on SparkHive on Spark (与上面的不同)共同点： metastore mysql Spark SQL不仅仅是SQL这么简单的事情，它还能做更多的事情 Hive: SQL Spark SQL: SQL Spark SQL提供的操作数据的方式 SQL DataFrame API Dataset API ===&gt; 一个用于处理结构化数据的Spark组件，强调的是“结构化数据”，而非“SQL” Spark RDD VS MapReduceR/Pandas : one machine (单机的) ==&gt; DataFrame：让小伙伴们感觉像开发单机版应用程序一样来开发分布式应用程序 A DataFrame is a Dataset organized into named columns (可以简单的把DataFrame理解成关系型数据库中的一个表，这是表层，底层其实做了更深的优化)以列(列名、列类型、列值)的形式构成分布式的数据集 面试题：RDD与DataFrame的区别12345 schema: 在SQL环境下，schema就是数据库对象的集合 区别 RDD与DataFrame方法一： Inferring the Schema Using Reflection 方法二： 下面的已运行过 8_8.py 123456789101112131415161718192021222324252627282930313233343536# Import data typesfrom pyspark.sql.types import *from pyspark.sql import SparkSessionspark = SparkSession.builder.appName('spark0801').getOrCreate()sc = spark.sparkContext# Load a text file and convert each line to a Row.lines = sc.textFile("file:///home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.txt")parts = lines.map(lambda l: l.split(","))# Each line is converted to a tuple.people = parts.map(lambda p: (p[0], p[1].strip()))# The schema is encoded in a string.schemaString = "name age"fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]schema = StructType(fields)# Apply the schema to the RDD.schemaPeople = spark.createDataFrame(people, schema)# Creates a temporary view using the DataFrameschemaPeople.createOrReplaceTempView("people")print(schemaPeople.printSchema())# SQL can be run over DataFrames that have been registered as a table.# results = spark.sql("SELECT name FROM people")print(schemaPeople.show())sc.stop()# +-------+# | name|# +-------+# |Michael|# | Andy|# | Justin|# +-------+ Spark Streamingis an extension of the core Spark API enables scalable, high-throughput, fault-tolerant stream processing of live data streams流： Java SE IO 输入: 山沟沟、下水道… Kafka, Flume, Kinesis, or TCP sockets // TODO… 业务逻辑处理 输出: 痛、瓶子…. filesystems, databases, and live dashboards 在线机器学习 Q:安装完Spark之后能否直接使用Spark Streaming?A:YES 常用实时流处理框架对比 Storm：真正的实时流处理 Tuple Java Spark Streaming：并不是真正的实时流处理，而是一个mini batch操作(批处理) Scala、Java、Python 使用Spark一栈式解决问题 Flink: 底层是流处理，可以做到批处理(和spark相反) Kafka Stream Spark Streaming它的职责所在 receives live input data streams divides the data into batches batches are then processed by the Spark engine to generate the final stream of results in batches. Spark Core的核心抽象叫做：RDD 5大特性、对应源码中的5个方法是什么Spark Streaming的核心抽象叫做：DStream represents a continuous stream of data DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs. Azkaban基础篇(工作流插件)（必须在/bin/az..启动，否则报错）工作流概述 请假 OA 1 ：部门经理审批 3 ：部门经理审批 ==&gt; HR 5 ：部门经理审批 ==&gt; HR ==&gt; 老大 10：….. 借款： 涉及金额 Spark SQL/Hadoop用于做离线统计处理ETL(ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。)1) 数据抽取： Sqoop把RDBMS中的数据抽取到Hadoop Flume进行日志、文本数据的采集，采集到Hadoop2) 数据处理 Hive/MapReduce/Spark/……3) 统计结果入库 数据就存放到HDFS(Hive/Spark SQL/文件) 启动一个Server: HiveServer2 / ThriftServer jdbc的方式去访问统计结果 使用Sqoop把结果导出到RDBMS中 这些作业之间是存在时间先后依赖关系的Step A ==&gt; Step B ==&gt; Step C crontab定时调度为了更好的组织起这样的复杂执行计算的关系===&gt; 这就需要一个工作流调度系统来进行依赖关系作业的调度 Linux crontab + shell 优点：简单、易用 缺点： 维护 依赖 step a: 01:30 30分钟 step b: 02:10 30分钟 step c: 02:50 30分钟 ….. 资源利用率 集群在0130压力非常大，资源没有申请到 常用的调度框架 Azkaban：轻量级 Oozie：重量级 cm hue xml 宙斯(Zeus) Azkaban概述 Open-source Workflow Manager 批处理工作流，用于跑Hadoop的job 提供了一个易于使用的用户界面来维护和跟踪你的工作流程 Azkaban架构 Relational Database (MySQL) AzkabanWebServer AzkabanExecutorServer Azkaban运行模式 solo-server 数据信息存储在H2==&gt;MySQL webserver和execserver是运行在同一个进程中婢女 the heavier weight two server mode 数据信息存储在MySQL，在生产上一定要做主备 webserver和execserver是运行在不同的进程中的 distributed multiple-executor mode Azkaban编译：万世开头难，务必要保证你的网络速度不错(这里的编译是在对源码进行修改以后进行的) 1） 去github上下载源码包 2） ./gradlew build installDist 3） 建议搭建先去下载gradle-4.1-all.zip 然后整合azkaban源码中来，避免在编译的过程中去网络上下载，导致编译速度非常慢 4） 编译成功之后，去对应的目录下找到对应模式的安装包即可 Azkaban环境搭建 1) 解压编译后的安装包到~/app 2）启动azkaban $AZKABAN_HOME/bin/azkaban-solo-start.sh 验证：jps AzkabanSingleServer ip:8081 ES使用1234# bin目录下./elasticsearch# 查看192.168.211.4:9200 kibana使用123bin/kibana# 查看192.168.211.4:5601 实战大数据项目开发流程1) 调研 业务2) 需求分析 项目的需求 显示 隐式 甘特图：项目周期管理3) 方案设计 概要设计 详细设计 基本要求 系统要求：扩展性、容错性、高可用(HDFS YARN HA???)、定制化4) 功能开发 开发 单元测试 junit5) 测试 测试环境 QA 功能、性能、压力 用户测试6) 部署上线 试运行 DIFF “双活” 正式上线7) 运维 7*248) 后期迭代开发 大数据企业级应用1) 数据分析 商业 自研2）搜索/引擎 Lucene/Solr/ELK3）机器学习4) 精准营销5) 人工智能 企业级大数据分析平台1) 商业 2) 自研 Apache CDH HDP 数据量预估及集群规划Q: 一条日志多大、多少个字段、一天多少数据300500字节 * 1000W * 5 * 5 = 100GHDFS 3副本 * 100G * (23年) 服务器一台：磁盘多少？ ==&gt; Node数量 集群规模：数据量 + 存储周期 集群机器规模：DN: 数据量大小/每个Node的磁盘大小 NN: 2 RM: 2 NM: DN ZK: 3/5/7/9 GATEWAY: 资源设置：cpu/memory/disk/network 作业规划：MapReduce/Hive/Spark Server: ***** 调度：AZ、OOZIE数据来源：http://stateair.net/web/historical/1/1.html 根据北京的数据进行统计分析 同时间：北京 vs 广州 vs 成都 空气质量指数 pm2.5 健康建议0-50 健康51-100 中等101-150 对敏感人群不健康151-200 不健康201-300 非常不健康301-500 危险 500 爆表 数据分析==&gt;es==&gt;kibana data2017 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2017_HourlyPM25_created20170803.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2016 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2016_HourlyPM25_created20170201.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2015 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“file:///home/hadoop/data/Beijing_2015_HourlyPM25_created20160201.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)data2017.show()data2016.show()data2015.show()def get_grade(value): if value &lt;=50 and value &gt;=0: return “健康” elif value &lt;= 100: return “中等” elif value &lt;= 150: return “对敏感人群不健康” elif value &lt;= 200: return “不健康” elif value &lt;= 300: return “非常不健康” elif value &lt;= 500: return “危险” elif value &gt; 500: return “爆表” else: return None # 进来一个Value，出去一个Gradegroup2017 = data2017.withColumn(“Grade”,grade_function_udf(data2017[‘Value’])).groupBy(“Grade”).count()group2016 = data2016.withColumn(“Grade”,grade_function_udf(data2016[‘Value’])).groupBy(“Grade”).count()group2015 = data2015.withColumn(“Grade”,grade_function_udf(data2015[‘Value’])).groupBy(“Grade”).count() group2017.show()group2016.show()group2015.show() 使用SparkSQL将统计结果写入到ES中去 from pyspark.sql.functions import *from pyspark.sql.types import * def get_grade(value): if value &lt;= 50: return “健康” elif value &lt;= 100: return “中等” elif value &lt;= 150: return “对敏感人群不健康” elif value &lt;= 200: return “不健康” elif value &lt;= 300: return “非常不健康” elif value &lt;= 500: return “危险” elif value &gt; 500: return “爆表” else: return None data2017 = spark.read.format(“csv”).option(“header”,”true”).option(“inferSchema”,”true”).load(“/data/Beijing_2017_HourlyPM25_created20170803.csv”).select(“Year”,”Month”,”Day”,”Hour”,”Value”,”QC Name”)grade_function_udf = udf(get_grade, StringType())group2017 = data2017.withColumn(“Grade”, grade_function_udf(data2017[‘Value’])).groupBy(“Grade”).count()result2017_2 = group2017.select(“Grade”, “count”, group2017[‘count’] / data2017.count()*100) result2017_2=group2017.select(“Grade”, “count”).withColumn(“precent”,group2017[‘count’] / data2017.count()*100) result2017_2.selectExpr(“Grade as grade”, “count”, “precent”).write.format(“org.elasticsearch.spark.sql”).option(“es.nodes”,”192.168.199.102:9200”).mode(“overwrite”).save(“weaes/weather”) 练习：1) 同一个城市不同年份的对比2）相同年份的不同城市的对比 3) 月份为统计维度：3-1 3-24) 小时为统计维度 curl -XPOST ‘http://hadoop000:9200/imooc_es/student/1&#39; -H ‘Content-Type: application/json’ -d ‘{“name”:”imooc”,“age”:5,“interests”:[“Spark”,”Hadoop”]}’ ek后台启动： nohup …. &amp;]]></content>
  </entry>
  <entry>
    <title><![CDATA[克隆虚拟机]]></title>
    <url>%2F2019%2F12%2F17%2F%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181.克隆虚拟机2.克隆后的虚拟机配置sudo -i修改网卡信息vi /etc/udev/rules.d/70-persistent-net.rules 修改主机名vi /etc/sysconfig/network修改ip信息(改UUID的一个数字、ipaddr末尾改一个、HWADDR换成网卡的MAC地址)vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改映射vi /etc/hosts重启网卡service network restart]]></content>
  </entry>
  <entry>
    <title><![CDATA[大数据技术原理与应用]]></title>
    <url>%2F2019%2F11%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hadoop的启动12cd /usr/local/hadoop./sbin/start-dfs.sh #启动hadoop HDFSHDFS的启动 12cd /usr/local/hadoop../bin/hdfs dfs +命令 用shell命令将本地文件上传到HDFS中 1. 目录操作需要注意的是，Hadoop系统安装好以后，第一次使用HDFS时，需要首先在HDFS中创建用户目录。本教程全部采用hadoop用户登录Linux系统，因此，需要在HDFS中为hadoop用户创建一个用户目录，命令如下： 1cd /usr/local/hadoop./bin/hdfs dfs –mkdir –p /user/hadoop Shell 命令 该命令中表示在HDFS中创建一个“/user/hadoop”目录，“–mkdir”是创建目录的操作，“-p”表示如果是多级目录，则父目录和子目录一起创建，这里“/user/hadoop”就是一个多级目录，因此必须使用参数“-p”，否则会出错。“/user/hadoop”目录就成为hadoop用户对应的用户目录，可以使用如下命令显示HDFS中与当前用户hadoop对应的用户目录下的内容： 1./bin/hdfs dfs –ls . Shell 命令 该命令中，“-ls”表示列出HDFS某个目录下的所有内容，“.”表示HDFS中的当前用户目录，也就是“/user/hadoop”目录，因此，上面的命令和下面的命令是等价的： 1./bin/hdfs dfs –ls /user/hadoop Shell 命令 如果要列出HDFS上的所有目录，可以使用如下命令： 1./bin/hdfs dfs –ls Shell 命令 下面，可以使用如下命令创建一个input目录： 1./bin/hdfs dfs –mkdir input Shell 命令 在创建个input目录时，采用了相对路径形式，实际上，这个input目录创建成功以后，它在HDFS中的完整路径是“/user/hadoop/input”。如果要在HDFS的根目录下创建一个名称为input的目录，则需要使用如下命令： 1./bin/hdfs dfs –mkdir /input Shell 命令 可以使用rm命令删除一个目录，比如，可以使用如下命令删除刚才在HDFS中创建的“/input”目录（不是“/user/hadoop/input”目录）： 1./bin/hdfs dfs –rm –r /input Shell 命令 上面命令中，“-r”参数表示如果删除“/input”目录及其子目录下的所有内容，如果要删除的一个目录包含了子目录，则必须使用“-r”参数，否则会执行失败。 2. 文件操作12345在实际应用中，经常需要从本地文件系统向HDFS中上传文件，或者把HDFS中的文件下载到本地文件系统中。首先，使用vim编辑器，在本地Linux文件系统的“/home/hadoop/”目录下创建一个文件myLocalFile.txt，里面可以随意输入一些单词，比如，输入如下三行：HadoopSparkXMU DBLAB 然后，可以使用如下命令把本地文件系统的“/home/hadoop/myLocalFile.txt”上传到HDFS中的当前用户目录的input目录下，也就是上传到HDFS的“/user/hadoop/input/”目录下： 1./bin/hdfs dfs -put /home/hadoop/myLocalFile.txt input 可以使用ls命令查看一下文件是否成功上传到HDFS中，具体如下： 1./bin/hdfs dfs –ls input1 该命令执行后会显示类似如下的信息： 12Found 1 items -rw-r--r-- 1 hadoop supergroup 36 2017-01-02 23:55 input/ myLocalFile.txt 下面使用如下命令查看HDFS中的myLocalFile.txt这个文件的内容： 1./bin/hdfs dfs –cat input1/myLocalFile.txt 下面把HDFS中的myLocalFile.txt文件下载到本地文件系统中的“/home/hadoop/下载/”这个目录下，命令如下： 1./bin/hdfs dfs -get input1/myLocalFile.txt /home/hadoop/下载 可以使用如下命令，到本地文件系统查看下载下来的文件myLocalFile.txt： 1cd ~cd 下载lscat myLocalFile.txt 最后，了解一下如何把文件从HDFS中的一个目录拷贝到HDFS中的另外一个目录。比如，如果要把HDFS的“/user/hadoop/input/myLocalFile.txt”文件，拷贝到HDFS的另外一个目录“/input”中（注意，这个input目录位于HDFS根目录下），可以使用如下命令： 1./bin/hdfs dfs -cp input/myLocalFile.txt /input 可在 http://localhost:50070/ 中查看自己上传的文件 ###3.用pyhdfs对文件进行操作：(先启动hadoop) 1234567891011121314import pyhdfsclient = pyhdfs.HdfsClient(hosts="localhost,50070",user_name="hadoop")print(client.listdir("/user/hadoop/input")) # 返回指定目录下的所有文件response = client.open("/user/hadoop/input/test.txt") # 打开文件 并读取print(response.read())print(client.exists('/user/hadoop/input/test.txt')) # 判断文件是否存在print(client.get_home_directory()) # 返回 用户的根目录print(client.get_active_namenode()) # 返回可用的namenode节点 HBase启动HBase 1234567891.启动hadoop (启动后输入jps后可以看大盘Name(data/secondary)Node)ssh localhostcd /usr/local/hadoop./sbin/start-dfs.sh2.启动HBasecd /usr/local/hbasebin/start-hbase.sh进入shell界面bin/hbase shell HBase Shell 基本使用 MAPREDUCEMapReduce理解 HIVE采用MySQL数据库保存Hive的元数据，而不是采用Hive自带的derby来存储元数据。 hive的启动(先启动hadoop)： 1234cd /usr/local/hadoop./sbin/start-all.shcd /usr/local/hive./bin/hive hive基本命令 (sql语言) Hive简单编程实践1.先上传本地文件到HDFS中(参考前面的) 2.进入HIVE 1234567create table docs(line string);load data inpath '文件夹或者文件目录' overwrite into table docs;create table word_count as select word, count(1) as count from(select explode(split(line,' '))as word from docs) wgroup by wordorder by word; 执行后，用select语句查看，结果如下：]]></content>
      <tags>
        <tag>BD</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习]]></title>
    <url>%2F2019%2F10%2F14%2FMySQL%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[名词 VIM一些命令 退出vim的快捷键，不需要进入命令编辑模式 按住shift CTRL+Z 退出 zz 保存退出 zq 不保存退出，q表示放弃 之所以按住shift，其实是切换大小写 在命令编辑模式下： :q 不保存退出 :q! 不保存强制退出 :wq 保存退出，w表示写入，不论是否修改，都会更改时间戳 :x 保存退出，如果内容未改，不会更改时间戳 DB(database) DBMS(DB Management System)数据库管理系统，C/S 客户端/服务端 SQL(Structure Query L)语言 SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。 \1. 数据查询语言DQL数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：SELECT &lt;字段名表&gt;FROM &lt;表或视图名&gt;WHERE &lt;查询条件&gt; 2 .数据操纵语言DML数据操纵语言DML主要有三种形式：1) 插入：INSERT2) 更新：UPDATE3) 删除：DELETE \3. 数据定义语言DDL数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：CREATE TABLE/VIEW/INDEX/SYN/CLUSTER| | | | |表 视图 索引 同义词 簇 DDL操作是隐性提交的！不能rollback \4. 数据控制语言DCL数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如：1) GRANT：授权。 2) ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK; 3) COMMIT [WORK]：提交。 列—&gt;字段 行–&gt;记录 DBA数据库管理员(职务) cmd进sql C:\Windows\system32&gt;mysql -h localhost -P3306 -u root -p 连接本机可以简化为 C:\Windows\system32&gt;mysql -u root -p sql命令后面加; mysql中+只作为 数值加法 CONCAT( , )是拼接 DESC 表名 ； 显示表的结构 MySQL的常见命令12345678910111213141516171.查看当前所有的数据库show databases;2.打开指定的库use 库名;3.查看当前库的所有表show tables;4.查看其它库的所有表show tables from 库名;5.创建表create table 表名( 列名 列类型, 列名 列类型， 。。。);6.查看表结构desc 表名; 12345677.查看服务器的版本方式一：登录到mysql服务端select version();方式二：没有登录到mysql服务端mysql --version或mysql --V MySQL的语法规范12345671.不区分大小写,但建议关键字大写， 表名、列名小写2.每条命令最好用分号结尾3.每条命令根据需要，可以进行缩进 或换行4.注释 单行注释：#注释文字 单行注释：-- 注释文字 多行注释：/* 注释文字 */ SQL的常见命令1234567891011show databases； 查看所有的数据库use 库名； 打开指定 的库show tables ; 显示库中的所有表show tables from 库名;显示指定库中的所有表create table 表名( 字段名 字段类型, 字段名 字段类型); 创建表desc 表名; 查看指定表的结构select * from 表名;显示表中的所有数据 DQL语言的学习进阶1：基础查询123456789语法：SELECT 要查询的东西【FROM 表名】;特点：①通过select查询完的结果 ，是一个虚拟的表格，不是真实存在② 要查询的东西 可以是常量值、可以是表达式、可以是字段、可以是函数可以直接双击表 不用打字了!F12可以整理格式 进阶2：条件查询1234567891011121314151617181920212223242526272829条件查询：根据条件过滤原始表的数据，查询到想要的数据语法：select 要查询的字段|表达式|常量值|函数from 表where 条件 ;分类：一、条件表达式 示例：salary&gt;10000 条件运算符： &gt; &lt; &gt;= &lt;= = != &lt;&gt;二、逻辑表达式示例：salary&gt;10000 &amp;&amp; salary&lt;20000逻辑运算符： and（&amp;&amp;）:两个条件如果同时成立，结果为true，否则为false or(||)：两个条件只要有一个成立，结果为true，否则为false not(!)：如果条件成立，则not后为false，否则为true三、模糊查询示例：last_name like &apos;a%&apos;IS NULL IS NOT NULLIN安全等于&lt;=&gt; 进阶3：排序查询123456789语法：select 要查询的东西from 表where 条件order by 排序的字段|表达式|函数|别名 【asc|desc】 进阶4：常见函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849一、单行函数1、字符函数 concat拼接 substr截取子串 upper转换成大写 lower转换成小写 trim去前后指定的空格和字符 ltrim去左边空格 rtrim去右边空格 replace替换 lpad左填充 rpad右填充 instr返回子串第一次出现的索引 length 获取字节个数 2、数学函数 round 四舍五入 rand 随机数 floor向下取整 ceil向上取整 mod取余 truncate截断3、日期函数 now当前系统日期+时间 curdate当前系统日期 curtime当前系统时间 str_to_date 将字符转换成日期 date_format将日期转换成字符 datadiff(&apos;2019-02-01&apos;,&apos;2019-1-1&apos;)前面的日期减后面的 4、流程控制函数 if(判断条件，1，0) case 要判断的字段和表达式 when 常量1 then 要显示的值1或语句1 when 常量2 then 要显示的值2或语句2 else 要显示的值n end (在最后一条后面加；) 多重if case when 条件1 then 要显示的值1或语句1 when 条件2 then 要显示的值2或语句2 else 要显示的值n或语句n end 5、其他函数 version版本 database当前库 user当前连接用户 ​ 二、分组函数 1234567891011121314151617181920sum 求和max 最大值min 最小值avg 平均值count 计数特点：1、以上五个分组函数都忽略null值，除了count(*)2、sum和avg一般用于处理数值型 max、min、count可以处理任何数据类型 3、都可以搭配distinct使用，用于统计去重后的结果(只能去重一个)4、count的参数可以支持： 字段、*、常量值，一般放1 建议使用 count(*) -----可以用来统计个数 eg：查询部门编号为90的员工个数 SELECT COUNT(*) 个数 FROM employees WHERE department_id = 90;6、和分组函数一同查询的字段有限制（意义上的）要求是 group by 后面的字段 进阶5：分组查询1234语法：select 查询的字段，分组函数from 表group by 分组的字段 ​ 特点：​ 1、可以按单个字段分组​ 2、和分组函数一同查询的字段最好是分组后的字段​ 3、分组筛选​ 针对的表 位置 关键字​ 分组前筛选： 原始表 group by的前面 where​ 分组后筛选： 分组后的结果集 group by的后面 having​​ 4、可以按多个字段分组，字段之间用逗号隔开​ 5、可以支持排序​ 6、having后可以支持别名 进阶6：多表连接查询12笛卡尔乘积：如果连接条件省略或无效则会出现解决办法：添加上连接条件 一、传统模式下的连接 ：等值连接——非等值连接 12341.等值连接的结果 = 多个表的交集2.n表连接，至少需要n-1个连接条件3.多个表不分主次，没有顺序要求4.一般为表起别名，提高阅读性和性能 二、sql99语法：通过join关键字实现连接 123456789101112131415161718含义：1999年推出的sql语法支持：等值连接、非等值连接 （内连接）外连接交叉连接语法：select 字段，...from 表1【inner|left outer|right outer|cross】join 表2 on 连接条件【inner|left outer|right outer|cross】join 表3 on 连接条件【where 筛选条件】【group by 分组字段】【having 分组后的筛选条件】【order by 排序的字段或表达式】好处：语句上，连接条件和筛选条件实现了分离，简洁明了！ ​三、自连接 案例：查询员工名和直接上级的名称 sql99 123SELECT e.last_name,m.last_nameFROM employees eJOIN employees m ON e.`manager_id`=m.`employee_id`; sql92 123SELECT e.last_name,m.last_nameFROM employees e,employees m WHERE e.`manager_id`=m.`employee_id`; 进阶8：分页查询应用场景： 1实际的web项目中需要根据用户的需求提交对应的分页查询的sql语句 语法： 1234567select 字段|表达式,...from 表【where 条件】【group by 分组字段】【having 条件】【order by 排序的字段】limit 【起始的条目索引，】条目数; 特点： 123456781.起始条目索引从0开始2.limit子句放在查询语句的最后3.公式：select * from 表 limit （page-1）*sizePerPage,sizePerPage假如:每页显示条目数sizePerPage要显示的页数 page 进阶9：联合查询引入： union 联合、合并 语法： 12345select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】select 字段|常量|表达式|函数 【from 表】 【where 条件】 union 【all】.....select 字段|常量|表达式|函数 【from 表】 【where 条件】 特点： 1231、多条查询语句的查询的列数必须是一致的2、多条查询语句的查询的列的类型几乎相同3、union代表去重，union all代表不去重 黑马程序员123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595-- 数据库的操作 -- 链接数据库 mysql -uroot -p mysql -uroot -pmysql -- 退出数据库 exit/quit/ctrl+d -- sql语句最后需要有分号;结尾 -- 显示数据库版本 select version(); -- 显示时间 select now(); -- 查看所有数据库 show databases; -- 创建数据库 -- create database 数据库名 charset=utf8; create database python04; create database python04new charset=utf8; -- 查看创建数据库的语句 -- show crate database .... show create database python04; -- 查看当前使用的数据库 select database(); -- 使用数据库 -- use 数据库的名字 use python04new; -- 删除数据库 -- drop database 数据库名; drop database python04;-- 数据表的操作 -- 查看当前数据库中所有表 show tables; -- 创建表 -- auto_increment表示自动增长 -- not null 表示不能为空 -- primary key 表示主键 -- default 默认值 -- create table 数据表名字 (字段 类型 约束[, 字段 类型 约束]); create table xxxxx(id int, name varchar(30)); create table yyyyy(id int primary key not null auto_increment, name varchar(30)); create table zzzzz( id int primary key not null auto_increment, name varchar(30) ); -- 查看表结构 -- desc 数据表的名字; desc xxxxx; -- 创建students表(id、name、age、high、gender、cls_id) create table students( id int unsigned not null auto_increment primary key, name varchar(30), age tinyint unsigned default 0, high decimal(5,2), gender enum(&quot;男&quot;, &quot;女&quot;, &quot;中性&quot;, &quot;保密&quot;) default &quot;保密&quot;, cls_id int unsigned ); insert into students values(0, &quot;老王&quot;, 18, 188.88, &quot;男&quot;, 0); select * from students; -- 创建classes表(id、name) create table classes( id int unsigned not null auto_increment primary key, name varchar(30) ); insert into classes values(0, &quot;python04大神&quot;); select * from classes; -- 查看表的创建语句 -- show create table 表名字; show create table students; -- 修改表-添加字段 -- alter table 表名 add 列名 类型; alter table students add birthday datetime; -- 修改表-修改字段：不重命名版 -- alter table 表名 modify 列名 类型及约束; alter table students modify birthday date; -- 修改表-修改字段：重命名版 -- alter table 表名 change 原名 新名 类型及约束; alter table students change birthday birth date default &quot;2000-01-01&quot;; -- 修改表-删除字段 -- alter table 表名 drop 列名; alter table students drop high; -- 删除表 -- drop table 表名; -- drop database 数据库; -- drop table 数据表; drop table xxxxx; -- 增删改查(curd) -- 增加 -- 全列插入 -- insert [into] 表名 values(...) -- 主键字段 可以用 0 null default 来占位 -- 向classes表中插入 一个班级 insert into classes values(0, &quot;菜鸟班&quot;); +--------+-------------------------------------+------+-----+------------+----------------+ | Field | Type | Null | Key | Default | Extra | +--------+-------------------------------------+------+-----+------------+----------------+ | id | int(10) unsigned | NO | PRI | NULL | auto_increment | | name | varchar(30) | YES | | NULL | | | age | tinyint(3) unsigned | YES | | 0 | | | gender | enum(&apos;男&apos;,&apos;女&apos;,&apos;中性&apos;,&apos;保密&apos;) | YES | | 保密 | | | cls_id | int(10) unsigned | YES | | NULL | | | birth | date | YES | | 2000-01-01 | | +--------+-------------------------------------+------+-----+------------+----------------+ -- 向students表插入 一个学生信息 insert into students values(0, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); insert into students values(null, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); insert into students values(default, &quot;小李飞刀&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); -- 失败 -- insert into students values(default, &quot;小李飞刀&quot;, 20, &quot;第4性别&quot;, 1, &quot;1990-02-01&quot;); -- 枚举中 的 下标从1 开始 1---“男” 2---&gt;&quot;女&quot;.... insert into students values(default, &quot;小李飞刀&quot;, 20, 1, 1, &quot;1990-02-01&quot;); -- 部分插入 -- insert into 表名(列1,...) values(值1,...) insert into students (name, gender) values (&quot;小乔&quot;, 2); -- 多行插入 insert into students (name, gender) values (&quot;大乔&quot;, 2),(&quot;貂蝉&quot;, 2); insert into students values(default, &quot;西施&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;), (default, &quot;王昭君&quot;, 20, &quot;女&quot;, 1, &quot;1990-01-01&quot;); -- 修改 -- update 表名 set 列1=值1,列2=值2... where 条件; update students set gender=1; -- 全部都改 update students set gender=1 where name=&quot;小李飞刀&quot;; -- 只要name是小李飞刀的 全部的修改 update students set gender=1 where id=3; -- 只要id为3的 进行修改 update students set age=22, gender=1 where id=3; -- 只要id为3的 进行修改 -- 查询基本使用 -- 查询所有列 -- select * from 表名; select * from students; ---定条件查询 select * from students where name=&quot;小李飞刀&quot;; -- 查询 name为小李飞刀的所有信息 select * from students where id&gt;3; -- 查询 name为小李飞刀的所有信息 -- 查询指定列 -- select 列1,列2,... from 表名; select name,gender from students; -- 可以使用as为列或表指定别名 -- select 字段[as 别名] , 字段[as 别名] from 数据表 where ....; select name as 姓名,gender as 性别 from students; -- 字段的顺序 select id as 序号, gender as 性别, name as 姓名 from students; -- 删除 -- 物理删除 -- delete from 表名 where 条件 delete from students; -- 整个数据表中的所有数据全部删除 delete from students where name=&quot;小李飞刀&quot;; -- 逻辑删除 -- 用一个字段来表示 这条信息是否已经不能再使用了 -- 给students表添加一个is_delete字段 bit 类型 alter table students add is_delete bit default 0; update students set is_delete=1 where id=6;-- 数据的准备 -- 创建一个数据库 create database python_test charset=utf8; -- 使用一个数据库 use python_test; -- 显示使用的当前数据是哪个? select database(); -- 创建一个数据表 -- students表 create table students( id int unsigned primary key auto_increment not null, name varchar(20) default &apos;&apos;, age tinyint unsigned default 0, height decimal(5,2), gender enum(&apos;男&apos;,&apos;女&apos;,&apos;中性&apos;,&apos;保密&apos;) default &apos;保密&apos;, cls_id int unsigned default 0, is_delete bit default 0 ); -- classes表 create table classes ( id int unsigned auto_increment primary key not null, name varchar(30) not null );-- 查询 -- 查询所有字段 -- select * from 表名; select * from students; select * from classes; select id, name from classes; -- 查询指定字段 -- select 列1,列2,... from 表名; select name, age from students; -- 使用 as 给字段起别名 -- select 字段 as 名字.... from 表名; select name as 姓名, age as 年龄 from students; -- select 表名.字段 .... from 表名; select students.name, students.age from students; -- 可以通过 as 给表起别名 -- select 别名.字段 .... from 表名 as 别名; select students.name, students.age from students; select s.name, s.age from students as s; -- 失败的select students.name, students.age from students as s; -- 消除重复行 -- distinct 字段 select distinct gender from students;-- 条件查询 -- 比较运算符 -- select .... from 表名 where ..... -- &gt; -- 查询大于18岁的信息 select * from students where age&gt;18; select id,name,gender from students where age&gt;18; -- &lt; -- 查询小于18岁的信息 select * from students where age&lt;18; -- &gt;= -- &lt;= -- 查询小于或者等于18岁的信息 -- = -- 查询年龄为18岁的所有学生的名字 select * from students where age=18; -- != 或者 &lt;&gt; -- 逻辑运算符 -- and -- 18到28之间的所以学生信息 select * from students where age&gt;18 and age&lt;28; -- 失败select * from students where age&gt;18 and &lt;28; -- 18岁以上的女性 select * from students where age&gt;18 and gender=&quot;女&quot;; select * from students where age&gt;18 and gender=2; -- or -- 18以上或者身高查过180(包含)以上 select * from students where age&gt;18 or height&gt;=180; -- not -- 不在 18岁以上的女性 这个范围内的信息 -- select * from students where not age&gt;18 and gender=2; select * from students where not (age&gt;18 and gender=2); -- 年龄不是小于或者等于18 并且是女性 select * from students where (not age&lt;=18) and gender=2; -- 模糊查询 -- like -- % 替换1个或者多个 -- _ 替换1个 -- 查询姓名中 以 &quot;小&quot; 开始的名字 select name from students where name=&quot;小&quot;; select name from students where name like &quot;小%&quot;; -- 查询姓名中 有 &quot;小&quot; 所有的名字 select name from students where name like &quot;%小%&quot;; -- 查询有2个字的名字 select name from students where name like &quot;__&quot;; -- 查询有3个字的名字 select name from students where name like &quot;__&quot;; -- 查询至少有2个字的名字 select name from students where name like &quot;__%&quot;; -- rlike 正则 -- 查询以 周开始的姓名 select name from students where name rlike &quot;^周.*&quot;; -- 查询以 周开始、伦结尾的姓名 select name from students where name rlike &quot;^周.*伦$&quot;; -- 范围查询 -- in (1, 3, 8)表示在一个非连续的范围内 -- 查询 年龄为18、34的姓名 select name,age from students where age=18 or age=34; select name,age from students where age=18 or age=34 or age=12; select name,age from students where age in (12, 18, 34); -- not in 不非连续的范围之内 -- 年龄不是 18、34岁之间的信息 select name,age from students where age not in (12, 18, 34); -- between ... and ...表示在一个连续的范围内 -- 查询 年龄在18到34之间的的信息 select name, age from students where age between 18 and 34; -- not between ... and ...表示不在一个连续的范围内 -- 查询 年龄不在在18到34之间的的信息 select * from students where age not between 18 and 34; select * from students where not age between 18 and 34; -- 失败的select * from students where age not (between 18 and 34); -- 空判断 -- 判空is null -- 查询身高为空的信息 select * from students where height is null; select * from students where height is NULL; select * from students where height is Null; -- 判非空is not null select * from students where height is not null;-- 排序 -- order by 字段 -- asc从小到大排列，即升序 -- desc从大到小排序，即降序 -- 查询年龄在18到34岁之间的男性，按照年龄从小到到排序 select * from students where (age between 18 and 34) and gender=1; select * from students where (age between 18 and 34) and gender=1 order by age; select * from students where (age between 18 and 34) and gender=1 order by age asc; -- 查询年龄在18到34岁之间的女性，身高从高到矮排序 select * from students where (age between 18 and 34) and gender=2 order by height desc; -- order by 多个字段 -- 查询年龄在18到34岁之间的女性，身高从高到矮排序, 如果身高相同的情况下按照年龄从小到大排序 select * from students where (age between 18 and 34) and gender=2 order by height desc,id desc; -- 查询年龄在18到34岁之间的女性，身高从高到矮排序, 如果身高相同的情况下按照年龄从小到大排序, -- 如果年龄也相同那么按照id从大到小排序 select * from students where (age between 18 and 34) and gender=2 order by height desc,age asc,id desc; -- 按照年龄从小到大、身高从高到矮的排序 select * from students order by age asc, height desc;-- 聚合函数 -- 总数 -- count -- 查询男性有多少人，女性有多少人 select * from students where gender=1; select count(*) from students where gender=1; select count(*) as 男性人数 from students where gender=1; select count(*) as 女性人数 from students where gender=2; -- 最大值 -- max -- 查询最大的年龄 select age from students; select max(age) from students; -- 查询女性的最高 身高 select max(height) from students where gender=2; -- 最小值 -- min -- 求和 -- sum -- 计算所有人的年龄总和 select sum(age) from students; -- 平均值 -- avg -- 计算平均年龄 select avg(age) from students; -- 计算平均年龄 sum(age)/count(*) select sum(age)/count(*) from students; -- 四舍五入 round(123.23 , 1) 保留1位小数 -- 计算所有人的平均年龄，保留2位小数 select round(sum(age)/count(*), 2) from students; select round(sum(age)/count(*), 3) from students; -- 计算男性的平均身高 保留2位小数 select round(avg(height), 2) from students where gender=1; -- select name, round(avg(height), 2) from students where gender=1;-- 分组 -- group by -- 按照性别分组,查询所有的性别 select name from students group by gender; select * from students group by gender; select gender from students group by gender; -- 失败select * from students group by gender; -- 计算每种性别中的人数 select gender,count(*) from students group by gender; -- 计算男性的人数 select gender,count(*) from students where gender=1 group by gender; -- group_concat(...) -- 查询同种性别中的姓名 select gender,group_concat(name) from students where gender=1 group by gender; select gender,group_concat(name, age, id) from students where gender=1 group by gender; select gender,group_concat(name, &quot;_&quot;, age, &quot; &quot;, id) from students where gender=1 group by gender; -- having -- 查询平均年龄超过30岁的性别，以及姓名 having avg(age) &gt; 30 select gender, group_concat(name),avg(age) from students group by gender having avg(age)&gt;30; -- 查询每种性别中的人数多于2个的信息 select gender, group_concat(name) from students group by gender having count(*)&gt;2;-- 分页 -- limit start, count -- 限制查询出来的数据个数 select * from students where gender=1 limit 2; -- 查询前5个数据 select * from students limit 0, 5; -- 查询id6-10（包含）的书序 select * from students limit 5, 5; -- 每页显示2个，第1个页面 select * from students limit 0,2; -- 每页显示2个，第2个页面 select * from students limit 2,2; -- 每页显示2个，第3个页面 select * from students limit 4,2; -- 每页显示2个，第4个页面 select * from students limit 6,2; -- -----&gt; limit (第N页-1)*每个的个数, 每页的个数; -- 每页显示2个，显示第6页的信息, 按照年龄从小到大排序 -- 失败select * from students limit 2*(6-1),2; -- 失败select * from students limit 10,2 order by age asc; select * from students order by age asc limit 10,2; select * from students where gender=2 order by height desc limit 0,2;-- 连接查询 -- inner join ... on -- select ... from 表A inner join 表B; select * from students inner join classes; -- 查询 有能够对应班级的学生以及班级信息 select * from students inner join classes on students.cls_id=classes.id; -- 按照要求显示姓名、班级 select students.*, classes.name from students inner join classes on students.cls_id=classes.id; select students.name, classes.name from students inner join classes on students.cls_id=classes.id; -- 给数据表起名字 select s.name, c.name from students as s inner join classes as c on s.cls_id=c.id; -- 查询 有能够对应班级的学生以及班级信息，显示学生的所有信息，只显示班级名称 select s.*, c.name from students as s inner join classes as c on s.cls_id=c.id; -- 在以上的查询中，将班级姓名显示在第1列 select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id; -- 查询 有能够对应班级的学生以及班级信息, 按照班级进行排序 -- select c.xxx s.xxx from student as s inner join clssses as c on .... order by ....; select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id order by c.name; -- 当时同一个班级的时候，按照学生的id进行从小到大排序 select c.name, s.* from students as s inner join classes as c on s.cls_id=c.id order by c.name,s.id; -- left join -- 查询每位学生对应的班级信息 select * from students as s left join classes as c on s.cls_id=c.id; -- 查询没有对应班级信息的学生 -- select ... from xxx as s left join xxx as c on..... where ..... -- select ... from xxx as s left join xxx as c on..... having ..... select * from students as s left join classes as c on s.cls_id=c.id having c.id is null; select * from students as s left join classes as c on s.cls_id=c.id where c.id is null; -- right join on -- 将数据表名字互换位置，用left join完成-- 自关联 -- 省级联动 url:http://demo.lanrenzhijia.com/2014/city0605/ -- 查询所有省份 select * from areas where pid is null; -- 查询出山东省有哪些市 select * from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;山东省&quot;; select province.atitle, city.atitle from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;山东省&quot;; -- 查询出青岛市有哪些县城 select province.atitle, city.atitle from areas as province inner join areas as city on city.pid=province.aid having province.atitle=&quot;青岛市&quot;; select * from areas where pid=(select aid from areas where atitle=&quot;青岛市&quot;)-- 子查询 -- 标量子查询 -- 查询出高于平均身高的信息 -- 查询最高的男生信息 select * from students where height = 188; select * from students where height = (select max(height) from students); -- 列级子查询 -- 查询学生的班级号能够对应的学生信息 -- select * from students where cls_id in (select id from classes); Python 中操作 MySQL 步骤 引入模块 在py文件中引入pymysql模块 1from pymysql import * Connection 对象 用于建立与数据库的连接 创建对象：调用connect()方法 1conn=connect(参数列表) 参数host：连接的mysql主机，如果本机是’localhost’ 参数port：连接的mysql主机的端口，默认是3306 参数database：数据库的名称 参数user：连接的用户名 参数password：连接的密码 参数charset：通信采用的编码方式，推荐使用utf8 对象的方法 close()关闭连接 commit()提交 cursor()返回Cursor对象，用于执行sql语句并获得结果 Cursor对象 用于执行sql语句，使用频度最高的语句为select、insert、update、delete 获取Cursor对象：调用Connection对象的cursor()方法 1cs1=conn.cursor() 对象的方法 close()关闭 execute(operation [, parameters ])执行语句，返回受影响的行数，主要用于执行insert、update、delete语句，也可以执行create、alter、drop等语句 fetchone()执行查询语句时，获取查询结果集的第一个行数据，返回一个元组 fetchall()执行查询时，获取结果集的所有行，一行构成一个元组，再将这些元组装入一个元组返回 对象的属性 rowcount只读属性，表示最近一次execute()执行后受影响的行数 connection获得当前连接对象 MySQL常用操作注意：MySQL中每个命令后都要以英文分号；结尾。1、显示数据库mysql&gt; show databases;MySql刚安装完有两个数据库：mysql和test。mysql库非常重要，它里面有MySQL的系统信息，我们改密码和新增用户，实际上就是用这个库中的相关表进行操作。 2、显示数据库中的表mysql&gt; use mysql; （打开库，对每个库进行操作就要打开此库）Database changedmysql&gt; show tables; 3、显示数据表的结构：describe 表名; 4、显示表中的记录：select * from 表名;例如：显示mysql库中user表中的纪录。所有能对MySQL用户操作的用户都在此表中。select * from user; 5、建库：create database 库名;例如：创建一个名字位aaa的库mysql&gt; create database aaa; 6、建表：use 库名；create table 表名 (字段设定列表)；例如：在刚创建的aaa库中建立表person,表中有id(序号，自动增长)，xm（姓名）,xb（性别）,csny（出身年月）四个字段use aaa;mysql&gt; create table person (id int(3) auto_increment not null primary key, xm varchar(10),xb varchar(2),csny date);可以用describe命令察看刚建立的表结构。mysql&gt; describe person; 7、增加记录例如：增加几条相关纪录。mysql&gt;insert into person values(null,’张三’,’男’,’1997-01-02′);mysql&gt;insert into person values(null,’李四’,’女’,’1996-12-02′);注意，字段的值（’张三’,’男’,’1997-01-02’）是使用两个英文的单撇号包围起来，后面也是如此。因为在创建表时设置了id自增，因此无需插入id字段，用null代替即可。可用select命令来验证结果。mysql&gt; select * from person; 8、修改纪录例如：将张三的出生年月改为1971-01-10mysql&gt; update person set csny=’1971-01-10′ where xm=’张三’; 9、删除纪录例如：删除张三的纪录。mysql&gt; delete from person where xm=’张三’; 10、删库和删表drop database 库名;drop table 表名； 11、查看mysql版本在mysql5.0中命令如下：show variables like ‘version’;或者：select version();]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法python版]]></title>
    <url>%2F2019%2F09%2F15%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84python%2F</url>
    <content type="text"><![CDATA[1. 基础概念 split() 通过指定分隔符对字符串进行切片，如果参数 num 有指定值，则分隔 num+1 个子字符串 split() 方法语法： 1str.split(str="", num=string.count(str)) 链表是一种常见的基础数据结构，结构体指针在这里得到了充分的利用。链表可以动态的进行存储分配，也就是说，链表是一个功能极为强大的数组，他可以在节点中定义多种数据类型，还可以根据需要随意增添，删除，插入节点。链表都有一个头指针，一般以head来表示，存放的是一个地址。链表中的节点分为两类，头结点和一般节点，头结点是没有数据域的。链表中每个节点都分为两部分，一个数据域，一个是指针域。说到这里你应该就明白了，链表就如同车链子一样，head指向第一个元素：第一个元素又指向第二个元素；……，直到最后一个元素，该元素不再指向其它元素，它称为“表尾”，它的地址部分放一个“NULL”（表示“空地址”），链表到此结束。 作为有强大功能的链表，对他的操作当然有许多，比如：链表的创建，修改，删除，插入，输出，排序，反序，清空链表的元素，求链表的长度等等。 初学链表，一般从单向链表开始 12---&gt;NULLhead 这是一个空链表。 12 ----&gt;[p1]----&gt;[p2]...----&gt;[pn]----&gt;[NULL]head p1-&gt;next p2-&gt;next pn-&gt;next 有n个节点的链表。 创建链表 1234typedef struct student&#123; int score; struct student *next;&#125; LinkList; 一般创建链表我们都用typedef struct，因为这样定义结构体变量时，我们就可以直接可以用LinkList *a;定义结构体类型变量了。 初始化一个链表，n为链表节点个数。 12345678910111213LinkList *creat(int n)&#123; LinkList *head, *node, *end;//定义头节点，普通节点，尾部节点； head = (LinkList*)malloc(sizeof(LinkList));//分配地址 end = head; //若是空链表则头尾节点一样 for (int i = 0; i &lt; n; i++) &#123; node = (LinkList*)malloc(sizeof(LinkList)); //动态内存申请，将结构指针变成结构体变量 scanf("%d", &amp;node-&gt;score); end-&gt;next = node; end = node; &#125; end-&gt;next = NULL;//结束创建 return head;&#125; 修改链表节点值 修改链表节点值很简单。下面是一个传入链表和要修改的节点，来修改值的函数。 123456789101112131415void change(LinkList *list,int n) &#123;//n为第n个节点 LinkList *t = list; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; puts("输入要修改的值"); scanf("%d", &amp;t-&gt;score); &#125; else &#123; puts("节点不存在"); &#125;&#125; 删除链表节点 删除链表的元素也就是把前节点的指针域越过要删除的节点指向下下个节点。即：p-&gt;next = q-&gt;next;然后放出q节点的空间，即free(q); 12345678910111213141516void delet(LinkList *list, int n) &#123; LinkList *t = list, *in; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; in = t; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; in-&gt;next = t-&gt;next; free(t); &#125; else &#123; puts("节点不存在"); &#125;&#125; 插入链表节点 我们可以看出来，插入节点就是用插入前节点的指针域链接上插入节点的数据域，再把插入节点的指针域链接上插入后节点的数据域。根据图，插入节点也就是：e-&gt;next = head-&gt;next; head-&gt;next = e; 增加链表节点用到了两个结构体指针和一个int数据。 123456789101112131415161718void insert(LinkList *list, int n) &#123; LinkList *t = list, *in; int i = 0; while (i &lt; n &amp;&amp; t != NULL) &#123; t = t-&gt;next; i++; &#125; if (t != NULL) &#123; in = (LinkList*)malloc(sizeof(LinkList)); puts("输入要插入的值"); scanf("%d", &amp;in-&gt;score); in-&gt;next = t-&gt;next;//填充in节点的指针域，也就是说把in的指针域指向t的下一个节点 t-&gt;next = in;//填充t节点的指针域，把t的指针域重新指向in &#125; else &#123; puts("节点不存在"); &#125;&#125; 输出链表 输出链表很简单，边遍历边输出就行了。 123 while (h-&gt;next != NULL) &#123;h = h-&gt;next;printf("%d ", h-&gt;score); python 初始化链表/列表输入 1.只有一个整数：a = int(input) 2.一行多个整数并用空格分开：a,b = map(int,input().split()) 3.数据较多时可用 列表存储：num = list(map(int,input().split())) 4.关于初始化链表： 12345678910class Node: def __init__(self,x): self.val = x self.next = Nonenum = list(map(int,input().split(','))) #假设输入的每个元素按逗号隔开node = Node(-1)tep = nodefor i in num: tep.next = Node(i) tep = tep.next 栈 栈的存储结构通常由一个一维数组和一个记录栈顶元素位置的变量组成。LIFO 栈的顺序存储结构通常由一个一维数组和一个记录栈顶元素位置的变量组成 队列 队列：具有一定操作约束的线性表，一端插入，另一端删除.FIFO 队列的顺序存储结构通常由一个一维数组和一个记录队列头元素位置的变量front以及一个记录队列尾元素位置的变量rear组成。 树 儿子-兄弟表示法 旋转45后又叫二叉树 每个都是两个指针域 满二叉树： 除最后一层无任何子节点外，每一层上的所有结点都有两个子结点二叉树。 国内教程定义：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为K，且结点总数是(2^k) -1 ，则它就是满二叉树。 节点： 就是一个图中的0、1、2~~14，这些就叫节点。 叶子节点： 就是没有子节点的节点，比如图中的7、8、9~~14这些，0、1、2、3这些就不是叶子节点。 拓展：二叉树相关术语 树的结点（node）：包含一个数据元素及若干指向子树的分支； 孩子结点（child node）：结点的子树的根称为该结点的孩子； 双亲结点：B 结点是A 结点的孩子，则A结点是B 结点的双亲； 兄弟结点：同一双亲的孩子结点； 堂兄结点：同一层上结点； 祖先结点: 从根到该结点的所经分支上的所有结点子孙结点：以某结点为根的子树中任一结点都称为该结点的子孙 结点层：根结点的层定义为1；根的孩子为第二层结点，依此类推； 树的深度：树中最大的结点层 结点的度：结点子树的个数 树的度： 树中最大的结点度。 叶子结点：也叫终端结点，是度为 0 的结点； 分枝结点：度不为0的结点； 有序树：子树有序的树，如：家族树； 无序树：不考虑子树的顺序； 注意在POP后T为栈顶值。 二叉搜索树（BST， Binary Search Tree），也称二叉排序树或二叉查找树 二叉搜索树：一棵二叉树，可以为空；如果不为空，满足以下性质： 非空左子树的所有键值小于其根结点的键值。 非空右子树的所有键值大于其根结点的键值。 左、右子树都是二叉搜索树。 平衡二叉树 平均查找长度ASL “平衡因子（ Balance Factor，简称BF） : BF(T) = hL-hR，其中hL和hR分别为T的左、右子树的高度。 平衡二叉树（ Balanced Binary Tree）（ AVL树）空树，或者任一结点左、右子树高度差的绝对值不超过1，即|BF(T) |≤ 1 图​ 个人以为，BFS就像是再画一个半径为R++的圆，每画一次，这个圆就一点点的扩大，这样的好处在于他能够巨细无遗地扫描到你想要的元素；DFS就像是我们在画阴影的时候的方法，先沿对角线画一条斜线，然后在他的左边或者右边不断地画斜线填充，直到斜线接触到你想要的点；。 ​ 综上而言，在空间效率问题上，小范围而言BFS由于是采用队列的方式二优于DFS的递归方式，但是如果数据量扩大，这个真的不好说，这其中应该会有一个临界值，让两者的效率逆转。 ​ 而在时间效率上的话，一定程度上DFS的时间效率优于BFS。个人认为，BFS的主要作用在于扫描与找方向，DFS的作用主要在于在找到方向之后的建立最短 路径。 1.dfs(深度优先搜索)是两个搜索中先理解并使用的，其实就是暴力把所有的路径都搜索出来，它运用了回溯，保存这次的位置，深入搜索，都搜索完了便回溯回来，搜下一个位置，直到把所有最深位置都搜一遍，要注意的一点是，搜索的时候有记录走过的位置，标记完后可能要改回来； 回溯法是一种搜索法，按条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法； 例如这张图，从1开始到2，之后到5，5不能再走了，退回2，到6，退回2退回1，到3，一直进行； 理解这种方法比较简单，难的是要怎么用 123456789101112131415void dfs(int deep)&#123; int x=deep/n,y=deep%n; if(符合某种要求||已经不能在搜了) &#123; 做一些操作； return ; &#125; if(符合某种条件且有地方可以继续搜索的)//这里可能会有多种条件，可能要循环什么的 &#123; a[x][y]='x';//可能要改变条件，这个是瞎写的 dfs(deep+1,sum+1);//搜索下一层 a[x][y]='.';//可能要改回条件，有些可能不用改比如搜地图上有多少块连续的东西 &#125;&#125; 2.bfs(宽度/广度优先搜索)，这个一直理解了思想，不会用，后面才会的，思想，从某点开始，走四面可以走的路，然后在从这些路，在找可以走的路，直到最先找到符合条件的，这个运用需要用到队列(queue)，需要稍微掌握这个才能用bfs. 还是这张图，从1开始搜，有2，3，4几个点，存起来，从2开始有5，6，存起来，搜3，有7，8，存起来，搜4，没有了；现在开始搜刚刚存的点，从5开始，没有，然后搜6.。。一直进行，直到找到；]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法题目python版]]></title>
    <url>%2F2019%2F09%2F15%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84python-%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[leetcode两个数的和12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: dist=&#123;&#125; for i in range(len(nums)): if (target - nums[i]) not in dist: #对健判断 dist[nums[i]] = i # num[i]是键 i是值 else: return [dist[target - nums[i]],i] #返回的是键的=值 总结：Python 字典 in 操作符用于判断键是否存在于字典中 1. 最大子列和问题 给定K个整数组成的序列{ N1, N2, …, N**K }，“连续子列”被定义为{ N**i, N**i+1, …, N**j }，其中 1≤i≤j≤K。“最大子列和”则被定义为所有连续子列元素的和中最大者。例如给定序列{ -2, 11, -4, 13, -5, -2 }，其连续子列{ 11, -4, 13 }有最大的和20。现要求你编写程序，计算给定整数序列的最大子列和。 本题旨在测试各种不同的算法在各种数据情况下的表现。各组测试数据特点如下： 数据1：与样例等价，测试基本正确性； 数据2：102个随机整数； 数据3：103个随机整数； 数据4：104个随机整数； 数据5：105个随机整数； 输入格式: 输入第1行给出正整数K (≤100000)；第2行给出K个整数，其间以空格分隔。 输出格式: 在一行中输出最大子列和。如果序列中所有整数皆为负数，则输出0。 输入样例: 126-2 11 -4 13 -5 -2 程序： 123456789101112num = int(input('请给出正整数K'))N = [int(x) for x in input('请给出K个数').split()]print(N)ThisSum = MaxSum = 0for i in range(num): ThisSum += N[i] if ThisSum &gt; MaxSum: MaxSum = ThisSum elif(ThisSum &lt; 0): ThisSum = 0print(MaxSum) 2.两个有序链表序列的合并 已知两个非降序链表序列S1与S2，设计函数构造出S1与S2合并后的新的非降序链表S3。 输入格式: 输入分两行，分别在每行给出由若干个正整数构成的非降序序列，用−1表示序列的结尾（−1不属于这个序列）。数字用空格间隔。 输出格式: 在一行中输出合并后新的非降序链表，数字间用空格分开，结尾不能有多余空格；若新链表为空，输出NULL。 输入样例: 121 3 5 -12 4 6 8 10 -1 输出样例: 11 2 3 4 5 6 8 10 12345678910111213141516171819202122232425262728293031323334353637class Solution: def Merge(self, pHead1, pHead2): if not pHead1: return pHead2 if not pHead2: return pHead1 if pHead1.val &lt;= pHead2.val: pHead1.next = self.Merge(pHead1.next, pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1, pHead2.next) return pHead2 def getNewChart(self, list): if list: node = ListNode(list.pop(0)) # pop(0)是移除你的words中的第一个元素，并返回 # 被移除的元素的值，也就是说返回的是你words中的第一个元素。 node.next = self.getNewChart(list) return nodeclass ListNode: def __init__(self, x): self.val = x self.next = Noneif __name__ == '__main__': list1 = [1, 3, 5] list2 = [0, 1, 4] testList1 = Solution().getNewChart(list1) testList2 = Solution().getNewChart(list2) final = Solution().Merge(testList1, testList2) while final: print(final.val, end=" ") final = final.next]]></content>
      <tags>
        <tag>python</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib]]></title>
    <url>%2F2019%2F09%2F05%2Fmatplotlib%2F</url>
    <content type="text"><![CDATA[GUI编程–matplotlib绘图 numpyarray 12345678910111213141516import numpy as npt1 = np.array([1, 2, 3]) # 存放数组t2 = np.array(range(1,4),dtype = float) # 指定数组类型t6 = t5.astype('int8') # 调整数据类型#random.random() 取小数np.round(t7,2) # 取t7里小数的前两位t1.shape # 返回一个元祖 返回元祖的含义注意！ 第一个是0轴，第二个是1轴...t1.reshape((3, 4)) # 转入一个元祖 把1*12 变成3*4t5.flatte() # 把数据转换成一维的 轴 二维数组里 axis=0 代表行 1代表列 numpy读取数据(.csv) np.loadtxt(fram.dtype = np.float,delimiter = None, skipprows = 0, usecoles = None, unpack=False) fram 文件、字符或者生成器，可以是.gz (文件名) delimiter 分隔字符串，默认空格，可改成， skiprows 跳过前x行，一般跳过第一行表头 usecols 读取指定的列，索引，元祖类型 unpack 如True读入属性分别写入不同数组变量，False 只写入一个数组变量，默认False。(类似于把矩阵旋转了) t1.transpose()数组转置 或者t1.T 索引 切片 t2[2]取第二行 , 取多行t2[2,6,9] 取行t2[1,:] 取列t2[:,[0,2,7]] numpy中数值修改 t2[t2&lt;10] = 3 np.where(t&lt;10,0,10) 小于10是0，大于10是10 t.clip(10,18) 小于10 是10 大于18是18 数组的拼接 np.vstack((t1,t2)) 竖直拼接 np.hstack((t1,t2)) 水平拼接 行列交换t[[1,2],:] = t[[2,1],:] 构造全为0的数组 np.zero((us_data.shape[0],1)) 构造全为1的数组 np.ones((us_data.shape[0],1)) 获取最大值最小值的位置np.argmax(t,axis=0) /np.argmin(t,axis=1) 创建一个对角线为1的正方形数组(方阵)：np.eye(3) numpy的注意点copy和view a=b 完全不复制，a和b相互影响 a = b[:],视图的操作，一种切片，会创建新的对象a，但是a的数据完全由b保管，他们两个的数据变化是一致的， a = b.copy(),复制，a和b互不影响 常用的统计函数 求和：t.sum(axis=None) 指定一个轴 均值：t.mean(a,axis=None) 受离群点的影响较大 中值：np.median(t,axis=None) 最大值：t.max(axis=None) 最小值：t.min(axis=None) 极值：np.ptp(t,axis=None) 即最大值和最小值只差 标准差：t.std(axis=None) FuncAnimation动态绘图1、函数FuncAnimation(fig,func,frames,init_func,interval,blit)是绘制动图的主要函数，其参数如下： a.fig 绘制动图的画布名称 b.func自定义动画函数，即下边程序定义的函数update c.frames动画长度，一次循环包含的帧数，在函数运行时，其值会传递给函数update(n)的形参“n” d.init_func自定义开始帧，即传入刚定义的函数init,初始化函数 e.interval更新频率，以ms计 f.blit选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显 12345678910111213141516171819202122import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.animation import FuncAnimationfig, ax = plt.subplots() #生成子图，相当于fig = plt.figure(),ax = fig.add_subplot(),其中ax的函数参数表示把当前画布进行分割，例：fig.add_subplot(2,2,2).表示将画布分割为两行两列 #ax在第2个子图中绘制，其中行优先，xdata, ydata = [], [] #初始化两个数组ln, = ax.plot([], [], 'r-', animated=False) #第三个参数表示画曲线的颜色和线型，具体参见：https://blog.csdn.net/tengqingyong/article/details/78829596def init(): ax.set_xlim(0, 2*np.pi) #设置x轴的范围pi代表3.14...圆周率， ax.set_ylim(-1, 1) #设置y轴的范围 return ln, #返回曲线def update(n): xdata.append(n) #将每次传过来的n追加到xdata中 ydata.append(np.sin(n)) ln.set_data(xdata, ydata) #重新设置曲线的值 return ln,ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 10), #这里的frames在调用update函数是会将frames作为实参传递给“n” init_func=init, blit=True)plt.show() PS:一般来说一个动图有两类函数，一类是初始化函数，另一类是需要更新的函数！！ Figure构造器参数说明 class matplotlib.figure.Figure( figsize=None, #Figure的大小，单位是英寸 dpi=None, #分辨率（每英寸的点数） facecolor=None, #修饰的颜色 edgecolor=None, #边界颜色 linewidth=0.0, #线条宽度 frameon=None, #布尔值，是否绘制框架（Frame） subplotpars=None, #子图的参数 tight_layout=None, #取值布尔或者字典，缺省自动布局，False 使用 subplotpars参数，True就使用tight_layout，如果是字典，则包含如下字段：pad, w_pad, h_pad, 与 rect constrained_layout=None) #True就使用constrained_layout，会自动调整plot的位置。]]></content>
      <tags>
        <tag>pyqt</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[柔性负载-杨明]]></title>
    <url>%2F2019%2F08%2F25%2F%E6%9F%94%E6%80%A7%E8%B4%9F%E8%BD%BD-%E6%9D%A8%E6%98%8E%2F</url>
    <content type="text"><![CDATA[关于杨明老师柔性负载的文献综述制作人：颜世伟 制作时间：2019.8 --- 一、机械谐振建模 [1] 电机和执行机构通过传动轴系联接，传动轴系具有一定的抗扭刚度 K 和阻尼系数 Cw。当传动轴系发生扭转形变时轴系将产生转矩 Tw，此转矩对于电机来说可看作是电机的负载转矩，而对于执行机构来说可看作是驱动转矩。伺服驱动器控制电机运行，为电机的转轴提供电磁转矩 Te。在电机端电磁转矩 Te和传动轴系转矩 Tw作用于转动惯量为 J1、阻尼系数为 C1的电机转轴。在执行机构端，执行机构具有大小为 J2的等效转动惯量以及阻尼系数 C2，传动轴系转矩 Tw与负载转矩Tl共同作用于执行机构最终决定了负载转速。 系统中的阻尼系数很小，可忽略阻尼系数从而对系统模型进行化简得到 传动装置的数学表达： 实际系统模型框图： 根据系统的传递函数画处Bode图(A: ω1/ ωref传递函数的幅频特性曲线. B：ω2/ ω1 . C为A+B) 可以看出闭环系统中存在一个谐振点，系统对于此频率点的响应比较强烈，存在机械谐振。并且谐振频率受系统中的机械谐振频率和振幅主要受到负载转动惯量 J2和传动轴扭转弹性系数 K 两个参数影响. 二、双惯量弹性伺服系统外部机械参数辨识方法综述[5] 三、谐振机理分析及谐振特征快速辨识[6]目的：通过辨识的谐振频率可以确定陷波滤波器参数 主要思路：首先对比连续系统开、闭环幅频特性与固有谐振特征的定量关系，确定谐振模式; 进一步针对离散系统，分析控制器刚度对离散闭环系统谐振的影响，确定离散系统持续振荡状态下谐振频率即为 NTF (共轭极点为自然振动频率点)频率。 谐振原理及模型 电机转速与电机电磁转矩之间的传递函数： 从上式看出，机械谐振点在传递函数上引入了一对共轭的零极点，共轭零点为抗谐振频率点 AＲF( anti-resonance frequency) ，共轭极点为自然振动频率点 NTF( natural torsional frequency)。 系统的谐振特征 1.开环系统的谐振(速度开环，电流闭环) 计算得系统的阶跃响应为 斜坡输出的基础上叠加 NTF 频率的振荡，振荡幅值与NTF 频率成反比。而且振荡频率值与负载转矩无关，所以在分析谐振频率时可以不考虑负载转矩对系统的影响。证明在速度开环情况下，系统会以NTF 谐振频率振荡。 2.闭环系统的振荡(电流环近似为1) 速度控制器的传递函数为下式 其特征方程可以化简为 系统的闭环带宽主要受到min(w1,w2),且min(w1,w2)&lt;wARF(共轭零点的谐振频率)。 进一步画出弹性系统 wm/ Te、开环系统及闭环系统的幅频曲线如下： 通过弹性虚脱的幅频特性可以看出：在速度闭环系统中，由于受到闭环控制作用的影响，NTF 频率大于 0 dB 的增益会被明显抑制，所以此时的谐振主要是以接近 AＲF 频率的振荡频率 fe在振动，而且该频率振动也会逐渐地趋于稳定。 带有速度输出限幅的闭环系统响应如下 可以看出柔性负载引入对系统的最显著影响就是降低了系统的带宽，使得系统无法进一步提高性能。振荡频率明显分为两段: 当速度调节器饱和，系统处于速度控制开环阶段，此时电机和负载侧都以 NTF 频率振荡; 当电机速度达到给定速度，速度调节器退饱和，进入速度控制闭环阶段。在对于大惯量系统控制器刚度随之较大时，该阶段就能以AＲF 谐振频率衰减振荡，直至转速达到给定。 谐振特征辨识 将信号(伪随机序列信号/Chirp信号)幅值变为一倍额定电流值输入作为 q 轴给定输入系统。 再通过计算每个频率处的给定信号与激励信号的幅值比和相位差就可以得到被测系统的幅频和相频特性。系统的幅频及相频特性可按下式计算。按上述方式就可以绘制出系统的频率特性 Bode 图。 结论:1) 闭环系统的带宽受到弹性系统的限制，加大连续系统刚度只会使系统带宽及谐振频率趋近ARF谐振频率。所以在大惯量伺服系统中，由于控制器刚度较高，可以将谐振频率近似为 ARF频率。2) 由于刚度的增加可能会使离散闭环系统的稳定裕度为负，进入发散状态。由于速度限幅的作用，使系统进入非线性振荡状态。此时振荡频率为NTF 频率叠加二分之一采样频率，经过采样滤波的实际系统体现的就为 NTF 谐振频率。 参考文献[1] 杨明, 胡浩, 徐殿国. 永磁交流伺服系统机械谐振成因及其抑制[J]. 电机与控制学报, 2012, 16(1):79-84. [2] 杨明, 郝亮, 徐殿国. 基于自适应陷波滤波器的在线机械谐振抑制[J]. 哈尔滨工业大学学报, 2014, 46(4):63-69. [3] 王璨, 杨明, 徐殿国. 基于PI控制的双惯量弹性系统机械谐振的抑制[J]. 电气传动, 2015(1). [4] 杨明, 王璨, 徐殿国. 基于轴矩限幅控制的机械谐振抑制技术[J]. 电机与控制学报, 2015, 19(4):58-64. [5] 王璨, 杨明, 栾添瑞. 双惯量弹性伺服系统外部机械参数辨识综述[J]. 中国电机工程学报, 2016, 36(3):804-817. [6] 杨明, 郝亮, 徐殿国. 双惯量弹性负载系统机械谐振机理分析及谐振特征快速辨识[J]. 电机与控制学报, 2016, 20(04):112-120. [7] 郎志, 杨明, 徐殿国. 双惯量弹性系统负载扰动观测器设计研究[J]. 电工技术学报, 2016(S2):90-97. [8] Beinke S，Wertz H，Schutte F，et al．Identification of nonlinear two-mass systems for self-commissioning speed control of electrical drives[C]//Proceedings of the 24th Annual Conference of the IEEE Industrial Electronics Society．Aachen：IEEE，1998：2251-2256． [9] GuoY J，HuangL P，Muramatsu M．Research on inertia identification and auto-tuning of speed controller for AC servo system[C]//Proceedings of the Power Conversion Conference．Osaka：IEEE，2002：896-901 [10] Östring M ， Gunnarsson S ， Norrlöf M ． Closed-loop identification of an industrial robot containing flexibilities [J]．Control Engineering Practice，2003，11(3)：291-300 [11] Östring M．Closed loop identification of the physical parameters of an industrial robot[C]//Proceedings of the 32nd International Symposium on Robotics ． Seoul ，Korea，2000． [12]Dhaouadi R，KuboK．Transfer function and parameters identification of a motor drive system using adaptive filtering[C]//Proceedings of the 4th International Workshop on Advanced Motion Control．Mie：IEEE，1996：588-593． [13] Eker I，Vural M．Experimental online identification of a three-mass mechanical system[C]//Proceedings of 2003 IEEE Conference on Control Applications．Istanbul，Turkey：IEEE，2003：60-65． [14] Landau I D，Karimi A．An extended output error recursive algorithm for identification in closed loop[C]//Proceedings of the 35th IEEE Conference on Decision and Control．Kobe：IEEE，1996：1405-1410． [15] Eker I，Vural M．Experimental online identification of a three-mass mechanical system[C]//Proceedings of 2003 IEEE Conference on Control Applications．Istanbul，Turkey：IEEE，2003：60-65． [16] Landau I D，Karimi A．An extended output error recursive algorithm for identification in closed loop[C]//Proceedings of the 35th IEEE Conference on Decision and Control．Kobe：IEEE，1996：1405-1410． [17] Zoubek H，Pacas M．A method for speed-sensorless identification of two-mass-systems[C]//Proceedings of the 2010 IEEE Energy Conversion Congress and Exposition．Atlanta，GA：IEEE，2010：4461-4468． [18] YoshiokaY ， HanamotoT ． Estimation of a multimass system using the LWTLS and a coefficient diagram for vibration-controller design[J] ． IEEE Transactions on Industry Applications，2008，44(2)：566-574． [19] Villwock S，Pacas M．Application of the Welch-method for the identification of two-and three-mass-systems [J]．IEEE Transactions on Industrial Electronics，2008，55(1)：457-466．]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>柔性负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyqt基础知识]]></title>
    <url>%2F2019%2F08%2F24%2Fpyqt5%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[pqqt基础知识汇总 第一个窗口1、if __name__ == “__main__“: 是代表如果这个文件是主程序这运行下面的代码，如果是被别的程序文件调用的话，则运行下面的代码。 2、__init __ 方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象做一些你希望的 初始化 。注意，这个名称的开始和结尾都是双下划线。 3、生成实例（对象）必须以类名()，别忘记了() 4、类中的函数（方法）必须有self，是代表属于这个实例（对象）本身持有的，而外部定义的函数则不需要。 5、QApplication相当于main函数，也就是整个程序（有很多文件）的主入口函数。 ​ 对于一个Gui程序必须至少有一个这样的一个实例来让程序运行。 6、最后一句是调用sys库的exit退出方法，退出条件（参数）是app.exec_()也就是整个窗口关闭。 Qt Designer1、记得在Qt Designer中窗体的layout层次可以通过对象查看器来查看，layout的一些设置可以通过属性编辑器来修该。 2、通常我们使用栅格布局作为顶层布局，将控件放置好之后可以通过右键–布局–栅格布局，将布局充满整个窗体。 3、我们可以先放入控件，然后ctrl选中多个控件，然后点击工具栏上快速布局工具进行布局。 4、在mianwindows中默认会有个centralwidget布局也是继承自QtWidgets.QWidget，表示窗口的中央部分。 信号和槽信号和槽是一种高级接口，应用于对象之间的通信，它是 QT 的核心特性，也是 QT 区别于其它工具包的重要地方。它为高层次的事件处理自动生成所需要的附加代码。在我们所熟知的很多 GUI 工具包中，窗口小部件 (widget) 都有一个回调函数用于响应它们能触发的每个动作，这个回调函数通常是一个指向某个函数的指针。但是，在 QT 中信号和槽取代了这些凌乱的函数指针，使得我们编写这些通信程序更为简洁明了。所有从 QObject 或其子类 ( 例如 Qwidget) 派生的类都能够包含信号和槽。当对象改变其状态时，信号就由该对象发射 (emit) 出去，这就是对象所要做的全部事情，它不知道另一端是谁在接收这个信号。这就是真正的信息封装，它确保对象被当作一个真正的软件组件来使用。槽用于接收信号，但它们是普通的对象成员函数。一个槽并不知道是否有任何信号与自己相连接。而且，对象并不了解具体的通信机制。你可以将很多信号与单个的槽进行连接，也可以将单个的信号与很多的槽进行连接，甚至于将一个信号与另外一个信号相连接也是可能的，这时无论第一个信号什么时候发射系统都将立刻发射第二个信号。总之，信号与槽构造了一个强大的部件编程机制。 说实话对于像我这样的新手来说看着就蛋疼，想学会它没办法，我们还是简化一下概念吧：所有QObject类都可以使用信号槽，换句话来说继承自pyqt中的类基本上都可以使用信号槽机制。当然非QObject也是可以通过其他一些办法来使用信号槽的。 仅仅有了信号和槽是不行的，我们还需要了解：信号(Signal)、槽(slot)、连接(connect)、动作事件(action)、发射(emit)、发送者、接受者等等一些列的知识。 在PyQt中接受者和发送者必须是个对象（实例）！ PyQt中的控件中提供了很多信号和槽方法，大家可以多多使用Qt Designer 设计参考！ 槽其实就个函数（方法），Qt5中的槽函数不在限定必须是slot，可以是普通的函数、类的普通成员函数、lambda函数等。编译期间就会检查信号与槽是否存在！ 信号的connect连接最好放在__init__析构函数里面，这样只会声明一次连接，如果在类方法（函数中）使用的话，要记得disconnect，否则connect会连接多次，导致程序异常。 信号槽函数不用加 ()，否则可能会导致连接异常。 PyQt信号和槽传递额外参数使用Pyqt编程过程中，经常会遇到给槽函数传递额外参数的情况。但是信号-槽机制只是指定信号如何连接到槽，信号定义的参数被传递给槽，而额外的参数（用户定义）不能直接传递。 而传递额外参数又是很有用处。你可能使用一个槽处理多个组件的信号，有时要传递额外的信息。 一种方法是使用lambda表达式。 1234567891011121314151617181920212223242526272829from PyQt4.QtCore import *from PyQt4.QtGui import * class MyForm(QMainWindow): def __init__(self, parent=None): super(MyForm, self).__init__(parent) button1 = QPushButton('Button 1') button2 = QPushButton('Button 1') button1.clicked.connect(lambda: self.on_button(1)) button2.clicked.connect(lambda: self.on_button(2)) layout = QHBoxLayout() layout.addWidget(button1) layout.addWidget(button2) main_frame = QWidget() main_frame.setLayout(layout) self.setCentralWidget(main_frame) def on_button(self, n): print('Button &#123;0&#125; clicked'.format(n)) if __name__ == "__main__": import sys app = QApplication(sys.argv) form = MyForm() form.show() app.exec_() 解释一下，on_button是怎样处理从两个按钮传来的信号。我们使用lambda传递按钮数字给槽，也可以传递任何其他东西—甚至是按钮组件本身（假如，槽打算把传递信号的按钮修改为不可用） 第2个方法是使用functools里的partial函数。 1button1.clicked.connect(partial(self.on_button, 1))button2.clicked.connect(partial(self.on_button, 2)) 《Rapid GUI Program with Python and QT》 P143例子。 自定义信号emit及传参12345678910111213141516171819202122232425262728293031from f1 import Ui_MainWindowfrom PyQt5 import QtWidgets,QtCoreimport sys,timeclass MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): # 继承QWidget和Ui_MainWindow _signal = QtCore.pyqtSignal() # 定义信号 def __init__(self): super(MyWindow, self).__init__() self.setupUi(self) #加载窗体 self.pushButton.clicked.connect(self.prn) # 按钮1链接到prn槽函数 # self.pushButton_2.clicked.connect(self.prn) self._signal.connect(self.mysignalslot) # 将信号连接到mysignalslot def prn(self): print('打印测试') time.sleep(1) print('延时1秒') self._signal.emit() #发射信号 def mysignalslot(self): # 自定义槽函数 print('我是slot')if __name__ == '__main__': import sys app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() # 创建实例 mywindow.show() # 使用QtWidgets的show()方法 sys.exit(app.exec_()) main.py的程序如上所示。实现功能： 注意：当信号与槽函数的参数数量相同时，它们参数类型要完全一致。信号与槽不能有缺省参数。 当信号的参数与槽函数的参数数量不同时，只能是信号的参数数量多于槽函数的参数数量，且前面相同数量的参数类型应一致，信号中多余的参数会被忽略。此外，在不进行参数传递时，信号槽绑定时也是要求信号的参数数量大于等于槽函数的参数数量。这种情况一般是一个带参数的信号去绑定一个无参数的槽函数。 当然可以出传递的参数类型有很多种：str、int、list、object、float、tuple、dict等等 单个文件打开 QFileDialog.getOpenFileName()多个文件打开 QFileDialog.getOpenFileNames() 文件夹选取 QFileDialog.getExistingDirectory() 文件保存 QFileDialog.getSaveFileName() 123456789101112131415161718192021222324252627282930313233343536373839404142from PyQt5 import QtWidgetsfrom PyQt5.QtWidgets import QFileDialog class MyWindow(QtWidgets.QWidget): def __init__(self): super(MyWindow,self).__init__() self.myButton = QtWidgets.QPushButton(self) self.myButton.setObjectName("myButton") self.myButton.setText("Test") self.myButton.clicked.connect(self.msg) def msg(self): directory1 = QFileDialog.getExistingDirectory(self, "选取文件夹", "C:/") #起始路径 print(directory1) fileName1, filetype = QFileDialog.getOpenFileName(self, "选取文件", "C:/", "All Files (*);;Text Files (*.txt)") #设置文件扩展名过滤,注意用双分号间隔 print(fileName1,filetype) files, ok1 = QFileDialog.getOpenFileNames(self, "多文件选择", "C:/", "All Files (*);;Text Files (*.txt)") print(files,ok1) fileName2, ok2 = QFileDialog.getSaveFileName(self, "文件保存", "C:/", "All Files (*);;Text Files (*.txt)") print(fileName2,ok2) if __name__=="__main__": import sys app=QtWidgets.QApplication(sys.argv) myshow=MyWindow() myshow.show() sys.exit(app.exec_()) 第一个参数parent，用于指定父组件。注意，很多Qt组件的构造函数都会有这么一个parent参数，并提供一个默认值0,这里一般填 self父类； 第三个参数dir，是对话框显示时默认打开的目录，”.”代表程序运行目录,”/“代表当前盘符下根目录，注意，这里跟平台有关，例如windows可填”C:\“等，Linux下填写”/“根目录 第四个参数Filter，是对话框后缀名过滤器，有Image File(.jpg *png)就让他只能显示后缀名是jpg或者是png的文件。Text Files(.txt)代表后缀名为.txt的文件。All Files()则代表是各种类型的文件。如果需要使用多个过滤器，使用&quot;;;&quot;分割，比如`”JPEG Files(.jpg);;PNG Files(*.png)”；` 各种对话框PyQt5提供了一系列标准的对话框，常见的有：消息对话框QMessageBox、颜色对话框QColorDialog、字体对话框QFontDialog、输入对话框QInputDialog以及文件对话框QFileDialog 1. 颜色对话框和字体对话框这两种对话框分别可以让用户进行颜色和字体选择。两者用法相似，所以就放在一起讲了： 123456789101112131415161718192021222324252627282930313233343536373839import sysfrom PyQt5.QtWidgets import QApplication, QWidget, QTextEdit, QColorDialog, QFontDialog, QPushButton, \ QHBoxLayout, QVBoxLayoutclass Demo(QWidget): def __init__(self): super(Demo, self).__init__() self.text_edit = QTextEdit(self) # 1 self.color_btn = QPushButton('Color', self) # 2 self.font_btn = QPushButton('Font', self) self.color_btn.clicked.connect(lambda: self.open_dialog_func(self.color_btn)) self.font_btn.clicked.connect(lambda: self.open_dialog_func(self.font_btn)) self.h_layout = QHBoxLayout() self.h_layout.addWidget(self.color_btn) self.h_layout.addWidget(self.font_btn) self.v_layout = QVBoxLayout() self.v_layout.addWidget(self.text_edit) self.v_layout.addLayout(self.h_layout) self.setLayout(self.v_layout) def open_dialog_func(self, btn): if btn == self.color_btn: # 3 color = QColorDialog.getColor() if color.isValid(): self.text_edit.setTextColor(color) else: # 4 font, ok = QFontDialog.getFont() if ok: self.text_edit.setFont(font)if __name__ == '__main__': app = QApplication(sys.argv) demo = Demo() demo.show() sys.exit(app.exec_()) QTextEdit控件用于显示文本颜色和字体变化； 实例化两个按钮分别用于打开颜色对话框和字体对话框，然后进行信号和槽的连接； 如果是color_btn被按下的话，则调用QColorDialog的getColor()方法显示颜色对话框，当选择一种颜色后其十六进制的值会保存在color变量中，但如果点击对话框中的取消(Cancel)按钮的话，则color为无效值。通过isValid()方法判断color是否有效，若有效的话则通过setTextColor()方法设置QTextEdit的文本颜色； 如果是font_btn被按下的话，则调用QFontDialog的getFont()方法显示字体对话框，该方法返回两个值，分别为QFont和bool值，如果用户选择了一种字体并按下确定(Ok)的话，则font保存所选择的QFont值，并且ok为True。若按下取消(Cancel)的话，则bool为False。当ok为True时，调用setFont()方法设置QTextEdit的文本字体。 2 输入对话框输入对话框一共有五种输入方法： 下面请看示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from zh22_2 import Ui_MainWindowfrom PyQt5 import QtWidgets,QtCorefrom PyQt5.QtWidgets import QApplication, QWidget, QInputDialog, QLineEdit, QTextEdit, QPushButton, \ QGridLayoutimport sys,timeclass MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): # 继承QWidget和Ui_MainWindow def __init__(self): super().__init__() self.setupUi(self) #加载窗体 self.name_btn.clicked.connect(self.name_f) self.gender_btn.clicked.connect(self.gender_f) self.age_btn.clicked.connect(self.age_f) self.score_btn.clicked.connect(self.score_f) self.info_btn.clicked.connect(self.info_f) def name_f(self): name, ok = QInputDialog.getText(self, 'Name Input', 'Please enter the name:') if ok: self.name_line.setText(name) def gender_f(self): gender_list = ["Man", "Woman"] gender, ok = QInputDialog.getItem(self, 'Geender Input', 'Please select the gender', gender_list) if ok: self.gender_line.setText(gender) def age_f(self): age, ok = QInputDialog.getInt(self, 'Age Input', 'Please enter the age:') if ok: self.age_line.setText(str(age)) def score_f(self): score, ok = QInputDialog.getDouble(self, 'Score Input', 'Please select the score:') if ok: self.score_line.setText(str(score)) def info_f(self): info, ok = QInputDialog.getMultiLineText(self, 'Info Input', 'Please enter the info:') if ok: self.info_line.insertPlainText(info)if __name__ == '__main__': app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() # 创建实例 mywindow.show() # 使用QtWidgets的show()方法 sys.exit(app.exec_()) 前面就是实例化按钮、单行输入框和文本编辑框并通过布局管理器进行排列，我们重点来看下槽函数： 如果是name_btn被点击的话，则调用QInputDialog的getText(parent, str, str)方法，第一个参数为指定的父类，第二个为输入框的标题，第三个为输入框提示。方法会返回一个字符串和一个布尔值，若点击输入框的ok按钮，则变量ok就为True，接着我们调用QLineEdit的setText()方法将其文本设为所输入的内容； getItem(parent, str, str, iterable, int, bool)方法需要多设置几个参数，前三个与getText()相同，第四个参数为要加入的选项内容，这里我们传入了item_list列表，可以让用户选择男性或女性。第五个参数为最初显示的选项，0代表刚开始显示第一个选项，即Female。最后一个参数是选项内容是否可编辑，这里设为False，不可编辑。 其他方法的使用都是类似的，这里就进行省略了。]]></content>
      <tags>
        <tag>pyqt</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础]]></title>
    <url>%2F2019%2F08%2F14%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[第一个python程序 在写代码之前，请千万不要用复制-粘贴把代码从页面粘贴到你自己的电脑上。写程序也讲究一个感觉，你需要一个字母一个字母地把代码自己敲进去，在敲代码的过程中，初学者经常会敲错代码：拼写不对，大小写不对，混用中英文标点，混用空格和Tab键，所以，你需要仔细地检查、对照，才能以最快的速度掌握如何写程序。 直接运行py 有同学问，能不能像.exe文件那样直接运行.py文件呢？在Windows上是不行的，但是，在Mac和Linux上是可以的，方法是在.py文件的第一行加上一个特殊的注释： #!/usr/bin/env python3 print(‘hello, world’) 然后，通过命令给hello.py以执行权限： $ chmod a+x hello.py 就可以直接运行hello.py了 python基础 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。此外，Python还提供了列表、字典等多种数据类型，还允许创建自定义数据类型，我们后面会继续讲到。 /除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数 //，称为地板除，两个整数的除法仍然是整数 %,取余 对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符 以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如： ‘ABC’.encode(‘ascii’) b’ABC’ ‘中文’.encode(‘utf-8’) b’\xe4\xb8\xad\xe6\x96\x87’ 要计算str包含多少个字符，可以用len()函数 删除list的元素，用pop(i)方法,i为索引 classmates.pop(i) list = [] ; tuple = () ; dict = { ‘ xiaoming ‘: 15 ,} 要删除一个key，用pop(key)方法，对应的value也会从dict中删除 set set和dict类似，也是一组key的集合，但不存储value。 再议不可变对象 上面我们讲了，str是不变对象，而list是可变对象。 对于可变对象，比如list，对list进行操作，list内部的内容是会变化的，比如： 1234&gt;&gt;&gt; a = [&apos;c&apos;, &apos;b&apos;, &apos;a&apos;]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 而对于不可变对象，比如str，对str进行操作呢： 12345&gt;&gt;&gt; a = &apos;abc&apos;&gt;&gt;&gt; a.replace(&apos;a&apos;, &apos;A&apos;)&apos;Abc&apos;&gt;&gt;&gt; a&apos;abc&apos; 虽然字符串有个replace()方法，也确实变出了&#39;Abc&#39;，但变量a最后仍是&#39;abc&#39;，应该怎么理解呢？ 我们先把代码改成下面这样： 123456&gt;&gt;&gt; a = &apos;abc&apos;&gt;&gt;&gt; b = a.replace(&apos;a&apos;, &apos;A&apos;)&gt;&gt;&gt; b&apos;Abc&apos;&gt;&gt;&gt; a&apos;abc&apos; 要始终牢记的是，a是变量，而&#39;abc&#39;才是字符串对象！有些时候，我们经常说，对象a的内容是&#39;abc&#39;，但其实是指，a本身是一个变量，它指向的对象的内容才是&#39;abc&#39;： 123┌───┐ ┌───────┐│ a │─────────────────&gt;│ &apos;abc&apos; │└───┘ └───────┘ 当我们调用a.replace(&#39;a&#39;, &#39;A&#39;)时，实际上调用方法replace是作用在字符串对象&#39;abc&#39;上的，而这个方法虽然名字叫replace，但却没有改变字符串&#39;abc&#39;的内容。相反，replace方法创建了一个新字符串&#39;Abc&#39;并返回，如果我们用变量b指向该新字符串，就容易理解了，变量a仍指向原有的字符串&#39;abc&#39;，但变量b却指向新字符串&#39;Abc&#39;了： 123456┌───┐ ┌───────┐│ a │─────────────────&gt;│ &apos;abc&apos; │└───┘ └───────┘┌───┐ ┌───────┐│ b │─────────────────&gt;│ &apos;Abc&apos; │└───┘ └───────┘ 所以，对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。 Python的循环有两种，一种是for…in循环，依次把list或tuple中的每个元素迭代出来，看例子： 123names = ['Michael', 'Bob', 'Tracy']for name in names: print(name) 执行这段代码，会依次打印names的每一个元素： 123MichaelBobTracy 第二种循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环。比如我们要计算100以内所有奇数之和，可以用while循环实现： 123456sum = 0n = 99while n &gt; 0: sum = sum + n n = n - 2print(sum) 在循环内部变量n不断自减，直到变为-1时，不再满足while条件，循环退出。 另外的， 如果要提前结束循环，可以用break语句： 1234567n = 1while n &lt;= 100: if n &gt; 10: # 当n = 11时，条件满足，执行break语句 break # break语句会结束当前循环 print(n) n = n + 1print('END') 在循环过程中，也可以通过continue语句，跳过当前的这次循环，直接开始下一次循环。 123456n = 0while n &lt; 10: n = n + 1 if n % 2 == 0: # 如果n是偶数，执行continue语句 continue # continue语句会直接继续下一轮循环，后续的print()语句不会执行 print(n) 字符串方法及注释 capitalize() 把字符串的第一个字符改为大写 casefold() 把整个字符串的所有字符改为小写 center(width) 将字符串居中，并使用空格填充至长度 width 的新字符串 count(sub[, start[, end]]) 返回 sub 在字符串里边出现的次数，start 和 end 参数表示范围，可选。 encode(encoding=’utf-8’, errors=’strict’) 以 encoding 指定的编码格式对字符串进行编码。 endswith(sub[, start[, end]]) 检查字符串是否以 sub 子字符串结束，如果是返回 True，否则返回 False。start 和 end 参数表示范围，可选。 expandtabs([tabsize=8]) 把字符串中的 tab 符号（\t）转换为空格，如不指定参数，默认的空格数是 tabsize=8。 find(sub[, start[, end]]) 检测 sub 是否包含在字符串中，如果有则返回索引值，否则返回 -1，start 和 end 参数表示范围，可选。 index(sub[, start[, end]]) 跟 find 方法一样，不过如果 sub 不在 string 中会产生一个异常。 isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返回 True，否则返回 False。 isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True，否则返回 False。 isdecimal() 如果字符串只包含十进制数字则返回 True，否则返回 False。 isdigit() 如果字符串只包含数字则返回 True，否则返回 False。 islower() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是小写，则返回 True，否则返回 False。 isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。 isspace() 如果字符串中只包含空格，则返回 True，否则返回 False。 istitle() 如果字符串是标题化（所有的单词都是以大写开始，其余字母均小写），则返回 True，否则返回 False。 isupper() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是大写，则返回 True，否则返回 False。 join(sub) 以字符串作为分隔符，插入到 sub 中所有的字符之间。 ljust(width) 返回一个左对齐的字符串，并使用空格填充至长度为 width 的新字符串。 lower() 转换字符串中所有大写字符为小写。 lstrip() 去掉字符串左边的所有空格 partition(sub) 找到子字符串 sub，把字符串分成一个 3 元组 (pre_sub, sub, fol_sub)，如果字符串中不包含 sub 则返回 (‘原字符串’, ‘’, ‘’) replace(old, new[, count]) 把字符串中的 old 子字符串替换成 new 子字符串，如果 count 指定，则替换不超过 count 次。 rfind(sub[, start[, end]]) 类似于 find() 方法，不过是从右边开始查找。 rindex(sub[, start[, end]]) 类似于 index() 方法，不过是从右边开始。 rjust(width) 返回一个右对齐的字符串，并使用空格填充至长度为 width 的新字符串。 rpartition(sub) 类似于 partition() 方法，不过是从右边开始查找。 rstrip() 删除字符串末尾的空格。 split(sep=None, maxsplit=-1) 不带参数默认是以空格为分隔符切片字符串，如果 maxsplit 参数有设置，则仅分隔 maxsplit 个子字符串，返回切片后的子字符串拼接的列表。 splitlines(([keepends])) 在输出结果里是否去掉换行符，默认为 False，不包含换行符；如果为 True，则保留换行符。。 startswith(prefix[, start[, end]]) 检查字符串是否以 prefix 开头，是则返回 True，否则返回 False。start 和 end 参数可以指定范围检查，可选。 strip([chars]) 删除字符串前边和后边所有的空格，chars 参数可以定制删除的字符，可选。 swapcase() 翻转字符串中的大小写。 title() 返回标题化（所有的单词都是以大写开始，其余字母均小写）的字符串。 translate(table) 根据 table 的规则（可以由 str.maketrans(‘a’, ‘b’) 定制）转换字符串中的字符。 upper() 转换字符串中的所有小写字符为大写。 zfill(width) 返回长度为 width 的字符串，原字符串右对齐，前边用 0 填充。 函数 定义默认参数要牢记一点：默认参数必须指向不变对象！ 为什么要设计str、None这样的不变对象呢？因为不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。 文件文件打开模式 打开模式 执行操作 ‘r’ 以只读方式打开文件（默认） ‘w’ 以写入的方式打开文件，会覆盖已存在的文件 ‘x’ 如果文件已经存在，使用此模式打开将引发异常 ‘a’ 以写入模式打开，如果文件存在，则在末尾追加写入 ‘b’ 以二进制模式打开文件 ‘t’ 以文本模式打开（默认） ‘+’ 可读写模式（可添加到其他模式中使用） ‘U’ 通用换行符支持 ​ 文件对象方法 文件对象方法 执行操作 f.close() 关闭文件 f.read([size=-1]) 从文件读取size个字符，当未给定size或给定负值的时候，读取剩余的所有字符，然后作为字符串返回 f.readline([size=-1]) 从文件中读取并返回一行（包括行结束符），如果有size有定义则返回size个字符 f.write(str) 将字符串str写入文件 f.writelines(seq) 向文件写入字符串序列seq，seq应该是一个返回字符串的可迭代对象 f.seek(offset, from) 在文件中移动文件指针，从from（0代表文件起始位置，1代表当前位置，2代表文件末尾）偏移offset个字节 f.tell() 返回当前在文件中的位置 f.truncate([size=file.tell()]) 截取文件到size个字节，默认是截取到文件指针当前位置 OS模块 os模块中关于文件/目录常用的函数使用方法 函数名 使用方法 getcwd() 返回当前工作目录 chdir(path) 改变工作目录 listdir(path=’.’) 列举指定目录中的文件名（’.’表示当前目录，’..’表示上一级目录） mkdir(path) 创建单层目录，如该目录已存在抛出异常 makedirs(path) 递归创建多层目录，如该目录已存在抛出异常，注意：’E:\a\b’和’E:\a\c’并不会冲突 remove(path) 删除文件 rmdir(path) 删除单层目录，如该目录非空则抛出异常 removedirs(path) 递归删除目录，从子目录到父目录逐层尝试删除，遇到目录非空则抛出异常 rename(old, new) 将文件old重命名为new system(command) 运行系统的shell命令 walk(top) 遍历top路径以下所有的子目录，返回一个三元组：(路径, [包含目录], [包含文件])【具体实现方案请看：第30讲课后作业^_^】 以下是支持路径操作中常用到的一些定义，支持所有平台 os.curdir 指代当前目录（’.’） os.pardir 指代上一级目录（’..’） os.sep 输出操作系统特定的路径分隔符（Win下为’\‘，Linux下为’/‘） os.linesep 当前平台使用的行终止符（Win下为’\r\n’，Linux下为’\n’） os.name 指代当前使用的操作系统（包括：’posix’, ‘nt’, ‘mac’, ‘os2’, ‘ce’, ‘java’） os.path模块中关于路径常用的函数使用方法 函数名 使用方法 basename(path) 去掉目录路径，单独返回文件名 dirname(path) 去掉文件名，单独返回目录路径 join(path1[, path2[, …]]) 将path1, path2各部分组合成一个路径名 split(path) 分割文件名与路径，返回(f_path, f_name)元组。如果完全使用目录，它也会将最后一个目录作为文件名分离，且不会判断文件或者目录是否存在 splitext(path) 分离文件名与扩展名，返回(f_name, f_extension)元组 getsize(file) 返回指定文件的尺寸，单位是字节 getatime(file) 返回指定文件最近的访问时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） getctime(file) 返回指定文件的创建时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） getmtime(file) 返回指定文件最新的修改时间（浮点型秒数，可用time模块的gmtime()或localtime()函数换算） 以下为函数返回 True 或 False exists(path) 判断指定路径（目录或文件）是否存在 isabs(path) 判断指定路径是否为绝对路径 isdir(path) 判断指定路径是否存在且是一个目录 isfile(path) 判断指定路径是否存在且是一个文件 islink(path) 判断指定路径是否存在且是一个符号链接 ismount(path) 判断指定路径是否存在且是一个挂载点 samefile(path1, paht2) 判断path1和path2两个路径是否指向同一个文件 标准异常Python标准异常总结 AssertionError 断言语句（assert）失败 AttributeError 尝试访问未知的对象属性 EOFError 用户输入文件末尾标志EOF（Ctrl+d） FloatingPointError 浮点计算错误 GeneratorExit generator.close()方法被调用的时候 ImportError 导入模块失败的时候 IndexError 索引超出序列的范围 KeyError 字典中查找一个不存在的关键字 KeyboardInterrupt 用户输入中断键（Ctrl+c） MemoryError 内存溢出（可通过删除对象释放内存） NameError 尝试访问一个不存在的变量 NotImplementedError 尚未实现的方法 OSError 操作系统产生的异常（例如打开一个不存在的文件） OverflowError 数值运算超出最大限制 ReferenceError 弱引用（weak reference）试图访问一个已经被垃圾回收机制回收了的对象 RuntimeError 一般的运行时错误 StopIteration 迭代器没有更多的值 SyntaxError Python的语法错误 IndentationError 缩进错误 TabError Tab和空格混合使用 SystemError Python编译器系统错误 SystemExit Python编译器进程被关闭 TypeError 不同类型间的无效操作 UnboundLocalError 访问一个未初始化的本地变量（NameError的子类） UnicodeError Unicode相关的错误（ValueError的子类） UnicodeEncodeError Unicode编码时的错误（UnicodeError的子类） UnicodeDecodeError Unicode解码时的错误（UnicodeError的子类） UnicodeTranslateError Unicode转换时的错误（UnicodeError的子类） ValueError 传入无效的参数 ZeroDivisionError 除数为零 以下是 Python 内置异常类的层次结构： BaseException+– SystemExit+– KeyboardInterrupt+– GeneratorExit+– Exception +– StopIteration +– ArithmeticError | +– FloatingPointError | +– OverflowError | +– ZeroDivisionError +– AssertionError +– AttributeError +– BufferError +– EOFError +– ImportError +– LookupError | +– IndexError | +– KeyError +– MemoryError +– NameError | +– UnboundLocalError +– OSError | +– BlockingIOError | +– ChildProcessError | +– ConnectionError | | +– BrokenPipeError | | +– ConnectionAbortedError | | +– ConnectionRefusedError | | +– ConnectionResetError | +– FileExistsError | +– FileNotFoundError | +– InterruptedError | +– IsADirectoryError | +– NotADirectoryError | +– PermissionError | +– ProcessLookupError | +– TimeoutError +– ReferenceError +– RuntimeError | +– NotImplementedError +– SyntaxError | +– IndentationError | +– TabError +– SystemError +– TypeError +– ValueError | +– UnicodeError | +– UnicodeDecodeError | +– UnicodeEncodeError | +– UnicodeTranslateError +– Warning +– DeprecationWarning +– PendingDeprecationWarning +– RuntimeWarning +– SyntaxWarning +– UserWarning +– FutureWarning +– ImportWarning +– UnicodeWarning +– BytesWarning +– ResourceWarning 对象对象 = 属性 + 方法 类是为了让对象实现量产. self指的是类实例对象本身(注意：不是类本身) 若子类会覆盖父类的__init__时，可采用(1)调用父类 父类.__init__(self) (2) supur().__init__() import在Python中，如果import的语句比较长，导致后续引用不方便，可以使用as语法，比如： import dir1.dir2.mod 那么，后续对mod的引用，都必须是dir1.dir2.mod dir1.dir2.mod.X那么，为了简化输入，可以使用as语法： import dir1.dir2.mod as m 那么，后续对mod的引用，可以直接使用m m. X 需要注意的是，使用as语法之后，只能通过as后面名字来访问导入的moudle import mod as m m.X # OK mod.X # Error 下面提供as的完整语法格式，import和from都支持： import modulename as name # 只能通过name来引用 from modulename import attrname as name # 只能通过name来引用 魔法方法(左右两边两个下划线) 魔法方法 含义 基本的魔法方法 new(cls[, …]) 1. new 是在一个对象实例化的时候所调用的第一个方法 2. 它的第一个参数是这个类，其他的参数是用来直接传递给 init 方法 3. new 决定是否要使用该 init 方法，因为 new 可以调用其他类的构造方法或者直接返回别的实例对象来作为本类的实例，如果 new 没有返回实例对象，则 init 不会被调用 4. new 主要是用于继承一个不可变的类型比如一个 tuple 或者 string init(self[, …]) 构造器，当一个实例被创建的时候调用的初始化方法 del(self) 析构器，当一个实例被销毁的时候调用的方法 call(self[, args…]) 允许一个类的实例像函数一样被调用：x(a, b) 调用 x.call(a, b) len(self) 定义当被 len() 调用时的行为 repr(self) 定义当被 repr() 调用时的行为 str(self) 定义当被 str() 调用时的行为 bytes(self) 定义当被 bytes() 调用时的行为 hash(self) 定义当被 hash() 调用时的行为 bool(self) 定义当被 bool() 调用时的行为，应该返回 True 或 False format(self, format_spec) 定义当被 format() 调用时的行为 有关属性 getattr(self, name) 定义当用户试图获取一个不存在的属性时的行为 getattribute(self, name) 定义当该类的属性被访问时的行为 setattr(self, name, value) 定义当一个属性被设置时的行为 delattr(self, name) 定义当一个属性被删除时的行为 dir(self) 定义当 dir() 被调用时的行为 get(self, instance, owner) 定义当描述符的值被取得时的行为 set(self, instance, value) 定义当描述符的值被改变时的行为 delete(self, instance) 定义当描述符的值被删除时的行为 比较操作符 lt(self, other) 定义小于号的行为：x &lt; y 调用 x.lt(y) le(self, other) 定义小于等于号的行为：x &lt;= y 调用 x.le(y) eq(self, other) 定义等于号的行为：x == y 调用 x.eq(y) ne(self, other) 定义不等号的行为：x != y 调用 x.ne(y) gt(self, other) 定义大于号的行为：x &gt; y 调用 x.gt(y) ge(self, other) 定义大于等于号的行为：x &gt;= y 调用 x.ge(y) 算数运算符 add(self, other) 定义加法的行为：+ sub(self, other) 定义减法的行为：- mul(self, other) 定义乘法的行为：* truediv(self, other) 定义真除法的行为：/ floordiv(self, other) 定义整数除法的行为：// mod(self, other) 定义取模算法的行为：% divmod(self, other) 定义当被 divmod() 调用时的行为 pow(self, other[, modulo]) 定义当被 power() 调用或 ** 运算时的行为 lshift(self, other) 定义按位左移位的行为：&lt;&lt; rshift(self, other) 定义按位右移位的行为：&gt;&gt; and(self, other) 定义按位与操作的行为：&amp; xor(self, other) 定义按位异或操作的行为：^ or(self, other) 定义按位或操作的行为：| 反运算 radd(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rsub(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rmul(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rtruediv(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rfloordiv(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rmod(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rdivmod(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rpow(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rlshift(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rrshift(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rand(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） rxor(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） ror(self, other) （与上方相同，当左操作数不支持相应的操作时被调用） 增量赋值运算 iadd(self, other) 定义赋值加法的行为：+= isub(self, other) 定义赋值减法的行为：-= imul(self, other) 定义赋值乘法的行为：*= itruediv(self, other) 定义赋值真除法的行为：/= ifloordiv(self, other) 定义赋值整数除法的行为：//= imod(self, other) 定义赋值取模算法的行为：%= ipow(self, other[, modulo]) 定义赋值幂运算的行为：**= ilshift(self, other) 定义赋值按位左移位的行为：&lt;&lt;= irshift(self, other) 定义赋值按位右移位的行为：&gt;&gt;= iand(self, other) 定义赋值按位与操作的行为：&amp;= ixor(self, other) 定义赋值按位异或操作的行为：^= ior(self, other) 定义赋值按位或操作的行为：|= 一元操作符 pos(self) 定义正号的行为：+x neg(self) 定义负号的行为：-x abs(self) 定义当被 abs() 调用时的行为 invert(self) 定义按位求反的行为：~x 类型转换 complex(self) 定义当被 complex() 调用时的行为（需要返回恰当的值） int(self) 定义当被 int() 调用时的行为（需要返回恰当的值） float(self) 定义当被 float() 调用时的行为（需要返回恰当的值） round(self[, n]) 定义当被 round() 调用时的行为（需要返回恰当的值） index(self) 1. 当对象是被应用在切片表达式中时，实现整形强制转换 2. 如果你定义了一个可能在切片时用到的定制的数值型,你应该定义 index 3. 如果 index 被定义，则 int 也需要被定义，且返回相同的值 上下文管理（with 语句） enter(self) 1. 定义当使用 with 语句时的初始化行为 2. enter 的返回值被 with 语句的目标或者 as 后的名字绑定 exit(self, exc_type, exc_value, traceback) 1. 定义当一个代码块被执行或者终止后上下文管理器应该做什么 2. 一般被用来处理异常，清除工作或者做一些代码块执行完毕之后的日常工作 容器类型 len(self) 定义当被 len() 调用时的行为（返回容器中元素的个数） getitem(self, key) 定义获取容器中指定元素的行为，相当于 self[key] setitem(self, key, value) 定义设置容器中指定元素的行为，相当于 self[key] = value delitem(self, key) 定义删除容器中指定元素的行为，相当于 del self[key] iter(self) 定义当迭代容器中的元素的行为 reversed(self) 定义当被 reversed() 调用时的行为 contains(self, item) 定义当使用成员测试运算符（in 或 not in）时的行为 super()不是父类，而是继承顺序的下一个类super()可以避免重复调用 如果childA基础Base, childB继承childA和Base，如果childB需要调用Base的init()方法时，就会导致init()被执行两次： 1234567891011121314151617class Base(object): def __init__(self): print 'Base create'class childA(Base): def __init__(self): print 'enter A ' Base.__init__(self) print 'leave A'class childB(childA, Base): def __init__(self): childA.__init__(self) Base.__init__(self)b = childB() Base的init()方法被执行了两次 1234enter A Base createleave ABase create 使用super()是可避免重复调用 123456789101112131415161718192021class Base(object): def __init__(self): print 'Base create'class childA(Base): def __init__(self): print 'enter A ' super(childA, self).__init__() print 'leave A'class childB(childA, Base): def __init__(self): super(childB, self).__init__()b = childB()print b.__class__.mro()enter A Base createleave A[&lt;class '__main__.childB'&gt;, &lt;class '__main__.childA'&gt;, &lt;class '__main__.Base'&gt;, &lt;type 'object'&gt;] Python变量前’‘和’*‘的作用在Python的在形参前加’‘和’*‘表示动态形参 在形参前加’*’表示可以接受多个实参值存进数组 12345678910def F(a, *b) print(a) print(b)F(1, 2, 3)&apos;&apos;&apos;1 (2, 3)&apos;&apos;&apos; 对于在形参前加’**’表示表示接受参数转化为字典类型 123456def F(**a) print(a)F(x=1, y=2)#&#123;&apos;x&apos;: 1, &apos;y&apos;: 2&#125; 混合运用 123456789101112def F(a, *b, **c) print(a) print(b) print(c)F(1, 2, 3, x=4, y=5)&apos;&apos;&apos;1(2, 3)&#123;&apos;x&apos;: 4, &apos;y&apos;: 5&#125;&apos;&apos;&apos; 1234567891011def F(*a) print(a)ls = [1, 2, 3]F(ls) #表示列表作为一个元素传入F(*ls) #表示列表元素作为多个元素传入'''([1, 2, 3],)(1, 2, 3)''' 12345678910111213141516def F(**a) print(a)dt = dict(x=1, y=2)F(x=1, y=2) F(**dt) #作为字典传入'''&#123;'x': 1, 'y':2&#125;&#123;'x': 1, 'y':2&#125;函数调用时dt = dict(color='red', fontproperties='SimHei')plt.plot(**dt) 等价于plt.plot(color='red', fontproperties='SimHei')''' python迭代器详解迭代器 迭代是访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 1. 可迭代对象 我们已经知道可以对list、tuple、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，我们把这样的过程称为遍历，也叫迭代。 但是，是否所有的数据类型都可以放到for…in…的语句中，然后让for…in…每次从中取出一条数据供我们使用，即供我们迭代吗？ 123456789101112131415161718192021222324252627&gt;&gt;&gt; for i in 100:... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'int' object is not iterable&gt;&gt;&gt;# int整型不是iterable，即int整型不是可以迭代的# 我们自定义一个容器MyList用来存放数据，可以通过add方法向其中添加数据&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; mylist.add(1)&gt;&gt;&gt; mylist.add(2)&gt;&gt;&gt; mylist.add(3)&gt;&gt;&gt; for num in mylist:... print(num)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyList' object is not iterable&gt;&gt;&gt;# MyList容器的对象也是不能迭代的 我们自定义了一个容器类型MyList，在将一个存放了多个数据的MyList对象放到for…in…的语句中，发现for…in…并不能从中依次取出一条数据返回给我们，也就说我们随便封装了一个可以存放多条数据的类型却并不能被迭代使用。 我们把可以通过for…in…这类语句迭代读取一条数据供我们使用的对象称之为可迭代对象（Iterable）**。 2. 如何判断一个对象是否可以迭代 可以使用 isinstance() 判断一个对象是否是 Iterable 对象： 12345678910111213141516In [50]: from collections import IterableIn [51]: isinstance([], Iterable)Out[51]: TrueIn [52]: isinstance(&#123;&#125;, Iterable)Out[52]: TrueIn [53]: isinstance('abc', Iterable)Out[53]: TrueIn [54]: isinstance(mylist, Iterable)Out[54]: FalseIn [55]: isinstance(100, Iterable)Out[55]: False 3. 可迭代对象的本质 我们分析对可迭代对象进行迭代使用的过程，发现每迭代一次（即在for…in…中每循环一次）都会返回对象中的下一条数据，一直向后读取数据直到迭代了所有数据后结束。那么，在这个过程中就应该有一个“人”去记录每次访问到了第几条数据，以便每次迭代都可以返回下一条数据。我们把这个能帮助我们进行数据迭代的“人”称为迭代器(Iterator)。 可迭代对象的本质就是可以向我们提供一个这样的中间“人”即迭代器帮助我们对其进行迭代遍历使用。 可迭代对象通过__iter__方法向我们提供一个迭代器，我们在迭代一个可迭代对象的时候，实际上就是先获取该对象提供的一个迭代器，然后通过这个迭代器来依次获取对象中的每一个数据. 那么也就是说，一个具备了__iter__方法的对象，就是一个可迭代对象。 12345678910111213141516&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)... def __iter__(self):... """返回一个迭代器"""... # 我们暂时忽略如何构造一个迭代器对象... pass...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(mylist, Iterable)True&gt;&gt;&gt;# 这回测试发现添加了__iter__方法的mylist对象已经是一个可迭代对象了 4. iter()函数与next()函数 list、tuple等都是可迭代对象，我们可以通过iter()函数获取这些可迭代对象的迭代器。然后我们可以对获取到的迭代器不断使用next()函数来获取下一条数据。iter()函数实际上就是调用了可迭代对象的__iter__方法。 1234567891011121314151617&gt;&gt;&gt; li = [11, 22, 33, 44, 55]&gt;&gt;&gt; li_iter = iter(li)&gt;&gt;&gt; next(li_iter)11&gt;&gt;&gt; next(li_iter)22&gt;&gt;&gt; next(li_iter)33&gt;&gt;&gt; next(li_iter)44&gt;&gt;&gt; next(li_iter)55&gt;&gt;&gt; next(li_iter)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 注意，当我们已经迭代完最后一个数据之后，再次调用next()函数会抛出StopIteration的异常，来告诉我们所有数据都已迭代完成，不用再执行next()函数了。** 5. 如何判断一个对象是否是迭代器 可以使用 isinstance() 判断一个对象是否是 Iterator 对象： 1234567In [56]: from collections import IteratorIn [57]: isinstance([], Iterator)Out[57]: FalseIn [58]: isinstance(iter([]), Iterator)Out[58]: TrueIn [59]: isinstance(iter("abc"), Iterator)Out[59]: True 6. 迭代器Iterator 通过上面的分析，我们已经知道，迭代器是用来帮助我们记录每次迭代访问到的位置，当我们对迭代器使用next()函数的时候，迭代器会向我们返回它所记录位置的下一个位置的数据。实际上，在使用next()函数的时候，调用的就是迭代器对象的__next__方法（Python3中是对象的__next__方法，Python2中是对象的next()方法）。所以，我们要想构造一个迭代器，就要实现它的__next__方法。但这还不够，python要求迭代器本身也是可迭代的，所以我们还要为迭代器实现__iter__方法，而__iter__方法要返回一个迭代器，迭代器自身正是一个迭代器，所以迭代器的__iter__方法返回自身即可。 一个实现了iter方法和next方法的对象，就是迭代器。 123456789101112131415161718192021222324252627282930313233343536373839class MyList(object): """自定义的一个可迭代对象""" def __init__(self): self.items = [] def add(self, val): self.items.append(val) def __iter__(self): myiterator = MyIterator(self) return myiteratorclass MyIterator(object): """自定义的供上面可迭代对象使用的一个迭代器""" def __init__(self, mylist): self.mylist = mylist # current用来记录当前访问到的位置 self.current = 0 def __next__(self): if self.current &lt; len(self.mylist.items): item = self.mylist.items[self.current] self.current += 1 return item else: raise StopIteration def __iter__(self): return self if __name__ == '__main__': mylist = MyList() mylist.add(1) mylist.add(2) mylist.add(3) mylist.add(4) mylist.add(5) for num in mylist: print(num) 7. for…in…循环的本质 for item in Iterable 循环的本质就是先通过iter()函数获取可迭代对象Iterable的迭代器，然后对获取到的迭代器不断调用next()方法来获取下一个值并将其赋值给item，当遇到StopIteration的异常后循环结束。 8. 迭代器的应用场景 我们发现迭代器最核心的功能就是可以通过next()函数的调用来返回下一个数据值。如果每次返回的数据值不是在一个已有的数据集合中读取的，而是通过程序按照一定的规律计算生成的，那么也就意味着可以不用再依赖一个已有的数据集合，也就是说不用再将所有要迭代的数据都一次性缓存下来供后续依次读取，这样可以节省大量的存储（内存）空间。 举个例子，比如，数学中有个著名的斐波拉契数列（Fibonacci），数列中第一个数为0，第二个数为1，其后的每一个数都可由前两个数相加得到： 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, … 现在我们想要通过for…in…循环来遍历迭代斐波那契数列中的前n个数。那么这个斐波那契数列我们就可以用迭代器来实现，每次迭代都通过数学计算来生成下一个数。 123456789101112131415161718192021222324252627282930313233class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return selfif __name__ == '__main__': fib = FibIterator(10) for num in fib: print(num, end=" ") 9. 并不是只有for循环能接收可迭代对象 除了for循环能接收可迭代对象，list、tuple等也能接收。 1234li = list(FibIterator(15))print(li)tp = tuple(FibIterator(6))print(tp) Python装饰器 python raise当程序出现错误，python会自动引发异常，也可以通过raise显示地引发异常。一旦执行了raise语句，raise后面的语句将不能执行]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP开发板灯光控制上位机]]></title>
    <url>%2F2019%2F08%2F12%2FDSP%E5%BC%80%E5%8F%91%E6%9D%BF%E7%81%AF%E5%85%89%E6%8E%A7%E5%88%B6%E4%B8%8A%E4%BD%8D%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[想法在前几天对PYQT5的学习以后，自己试着做了一个上位机 来对DSP开发板实现LED亮灭操作。 软硬件:pycharm + PYQT5 + CCS6.0 + DSP开发板 成果上位机界面 LED控制 描述：当按下 &lt;打开1灯&gt;时 1亮、2灭。当按下 &lt;打开2灯&gt;时 2亮、1灭。 遇到的问题 在CCS对GPIO进行写操作时(GPXDTA.bit = ) 无法成功写入，在顺华师兄建议下 改用SET CLEAR操作解决. 至于为何DAT不好用还未知？ 关于串口发送 16进制问题 代码 ui_demo.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233# -*- coding: utf-8 -*-# Form implementation generated from reading ui file 'ui_demo_1.ui'## Created by: PyQt5 UI code generator 5.11.3## WARNING! All changes made in this file will be lost!from PyQt5 import QtCore, QtGui, QtWidgetsclass Ui_Form(object): def setupUi(self, Form): Form.setObjectName("Form") Form.resize(868, 452) self.formGroupBox = QtWidgets.QGroupBox(Form) self.formGroupBox.setGeometry(QtCore.QRect(20, 20, 167, 301)) self.formGroupBox.setObjectName("formGroupBox") self.formLayout = QtWidgets.QFormLayout(self.formGroupBox) self.formLayout.setContentsMargins(10, 10, 10, 10) self.formLayout.setSpacing(10) self.formLayout.setObjectName("formLayout") self.s1__lb_1 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_1.setObjectName("s1__lb_1") self.formLayout.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.s1__lb_1) self.s1__box_1 = QtWidgets.QPushButton(self.formGroupBox) self.s1__box_1.setAutoRepeatInterval(100) self.s1__box_1.setDefault(True) self.s1__box_1.setObjectName("s1__box_1") self.formLayout.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.s1__box_1) self.s1__lb_2 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_2.setObjectName("s1__lb_2") self.formLayout.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.s1__lb_2) self.s1__box_2 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_2.setObjectName("s1__box_2") self.formLayout.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.s1__box_2) self.s1__lb_3 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_3.setObjectName("s1__lb_3") self.formLayout.setWidget(3, QtWidgets.QFormLayout.LabelRole, self.s1__lb_3) self.s1__box_3 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_3.setObjectName("s1__box_3") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.s1__box_3.addItem("") self.formLayout.setWidget(3, QtWidgets.QFormLayout.FieldRole, self.s1__box_3) self.s1__lb_4 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_4.setObjectName("s1__lb_4") self.formLayout.setWidget(4, QtWidgets.QFormLayout.LabelRole, self.s1__lb_4) self.s1__box_4 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_4.setObjectName("s1__box_4") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.s1__box_4.addItem("") self.formLayout.setWidget(4, QtWidgets.QFormLayout.FieldRole, self.s1__box_4) self.s1__lb_5 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_5.setObjectName("s1__lb_5") self.formLayout.setWidget(5, QtWidgets.QFormLayout.LabelRole, self.s1__lb_5) self.s1__box_5 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_5.setObjectName("s1__box_5") self.s1__box_5.addItem("") self.formLayout.setWidget(5, QtWidgets.QFormLayout.FieldRole, self.s1__box_5) self.open_button = QtWidgets.QPushButton(self.formGroupBox) self.open_button.setObjectName("open_button") self.formLayout.setWidget(7, QtWidgets.QFormLayout.SpanningRole, self.open_button) self.close_button = QtWidgets.QPushButton(self.formGroupBox) self.close_button.setObjectName("close_button") self.formLayout.setWidget(8, QtWidgets.QFormLayout.SpanningRole, self.close_button) self.s1__lb_6 = QtWidgets.QLabel(self.formGroupBox) self.s1__lb_6.setObjectName("s1__lb_6") self.formLayout.setWidget(6, QtWidgets.QFormLayout.LabelRole, self.s1__lb_6) self.s1__box_6 = QtWidgets.QComboBox(self.formGroupBox) self.s1__box_6.setObjectName("s1__box_6") self.s1__box_6.addItem("") self.formLayout.setWidget(6, QtWidgets.QFormLayout.FieldRole, self.s1__box_6) self.state_label = QtWidgets.QLabel(self.formGroupBox) self.state_label.setText("") self.state_label.setTextFormat(QtCore.Qt.AutoText) self.state_label.setScaledContents(True) self.state_label.setAlignment(QtCore.Qt.AlignRight|QtCore.Qt.AlignTrailing|QtCore.Qt.AlignVCenter) self.state_label.setObjectName("state_label") self.formLayout.setWidget(2, QtWidgets.QFormLayout.SpanningRole, self.state_label) self.verticalGroupBox = QtWidgets.QGroupBox(Form) self.verticalGroupBox.setGeometry(QtCore.QRect(210, 20, 401, 241)) self.verticalGroupBox.setObjectName("verticalGroupBox") self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalGroupBox) self.verticalLayout.setContentsMargins(10, 10, 10, 10) self.verticalLayout.setObjectName("verticalLayout") self.s2__receive_text = QtWidgets.QTextBrowser(self.verticalGroupBox) self.s2__receive_text.setObjectName("s2__receive_text") self.verticalLayout.addWidget(self.s2__receive_text) self.verticalGroupBox_2 = QtWidgets.QGroupBox(Form) self.verticalGroupBox_2.setGeometry(QtCore.QRect(210, 280, 401, 101)) self.verticalGroupBox_2.setObjectName("verticalGroupBox_2") self.verticalLayout_2 = QtWidgets.QVBoxLayout(self.verticalGroupBox_2) self.verticalLayout_2.setContentsMargins(10, 10, 10, 10) self.verticalLayout_2.setObjectName("verticalLayout_2") self.s3__send_text = QtWidgets.QTextEdit(self.verticalGroupBox_2) self.s3__send_text.setObjectName("s3__send_text") self.verticalLayout_2.addWidget(self.s3__send_text) self.s3__send_button = QtWidgets.QPushButton(Form) self.s3__send_button.setGeometry(QtCore.QRect(620, 310, 61, 31)) self.s3__send_button.setObjectName("s3__send_button") self.s3__clear_button = QtWidgets.QPushButton(Form) self.s3__clear_button.setGeometry(QtCore.QRect(620, 350, 61, 31)) self.s3__clear_button.setObjectName("s3__clear_button") self.formGroupBox1 = QtWidgets.QGroupBox(Form) self.formGroupBox1.setGeometry(QtCore.QRect(20, 340, 171, 101)) self.formGroupBox1.setObjectName("formGroupBox1") self.formLayout_2 = QtWidgets.QFormLayout(self.formGroupBox1) self.formLayout_2.setContentsMargins(10, 10, 10, 10) self.formLayout_2.setSpacing(10) self.formLayout_2.setObjectName("formLayout_2") self.label = QtWidgets.QLabel(self.formGroupBox1) self.label.setObjectName("label") self.formLayout_2.setWidget(0, QtWidgets.QFormLayout.LabelRole, self.label) self.label_2 = QtWidgets.QLabel(self.formGroupBox1) self.label_2.setObjectName("label_2") self.formLayout_2.setWidget(1, QtWidgets.QFormLayout.LabelRole, self.label_2) self.lineEdit = QtWidgets.QLineEdit(self.formGroupBox1) self.lineEdit.setObjectName("lineEdit") self.formLayout_2.setWidget(0, QtWidgets.QFormLayout.FieldRole, self.lineEdit) self.lineEdit_2 = QtWidgets.QLineEdit(self.formGroupBox1) self.lineEdit_2.setObjectName("lineEdit_2") self.formLayout_2.setWidget(1, QtWidgets.QFormLayout.FieldRole, self.lineEdit_2) self.hex_send = QtWidgets.QCheckBox(Form) self.hex_send.setGeometry(QtCore.QRect(620, 280, 71, 16)) self.hex_send.setObjectName("hex_send") self.hex_receive = QtWidgets.QCheckBox(Form) self.hex_receive.setGeometry(QtCore.QRect(620, 40, 71, 16)) self.hex_receive.setObjectName("hex_receive") self.s2__clear_button = QtWidgets.QPushButton(Form) self.s2__clear_button.setGeometry(QtCore.QRect(620, 80, 61, 31)) self.s2__clear_button.setObjectName("s2__clear_button") self.timer_send_cb = QtWidgets.QCheckBox(Form) self.timer_send_cb.setGeometry(QtCore.QRect(260, 390, 71, 16)) self.timer_send_cb.setObjectName("timer_send_cb") self.lineEdit_3 = QtWidgets.QLineEdit(Form) self.lineEdit_3.setGeometry(QtCore.QRect(350, 390, 61, 20)) self.lineEdit_3.setAlignment(QtCore.Qt.AlignRight|QtCore.Qt.AlignTrailing|QtCore.Qt.AlignVCenter) self.lineEdit_3.setObjectName("lineEdit_3") self.dw = QtWidgets.QLabel(Form) self.dw.setGeometry(QtCore.QRect(420, 390, 54, 20)) self.dw.setObjectName("dw") self.line = QtWidgets.QFrame(Form) self.line.setGeometry(QtCore.QRect(700, 30, 20, 351)) self.line.setFrameShape(QtWidgets.QFrame.VLine) self.line.setFrameShadow(QtWidgets.QFrame.Sunken) self.line.setObjectName("line") self.s4__open1_button = QtWidgets.QPushButton(Form) self.s4__open1_button.setGeometry(QtCore.QRect(740, 160, 61, 31)) self.s4__open1_button.setObjectName("s4__open1_button") self.s4__open2_button = QtWidgets.QPushButton(Form) self.s4__open2_button.setGeometry(QtCore.QRect(740, 210, 61, 31)) self.s4__open2_button.setObjectName("s4__open2_button") self.verticalGroupBox.raise_() self.verticalGroupBox_2.raise_() self.formGroupBox.raise_() self.s3__send_button.raise_() self.s3__clear_button.raise_() self.formGroupBox.raise_() self.hex_send.raise_() self.hex_receive.raise_() self.s2__clear_button.raise_() self.timer_send_cb.raise_() self.lineEdit_3.raise_() self.dw.raise_() self.line.raise_() self.s4__open1_button.raise_() self.s4__open2_button.raise_() self.retranslateUi(Form) QtCore.QMetaObject.connectSlotsByName(Form) def retranslateUi(self, Form): _translate = QtCore.QCoreApplication.translate Form.setWindowTitle(_translate("Form", "Form")) self.formGroupBox.setTitle(_translate("Form", "串口设置")) self.s1__lb_1.setText(_translate("Form", "串口检测：")) self.s1__box_1.setText(_translate("Form", "检测串口")) self.s1__lb_2.setText(_translate("Form", "串口选择：")) self.s1__lb_3.setText(_translate("Form", "波特率：")) self.s1__box_3.setItemText(0, _translate("Form", "115200")) self.s1__box_3.setItemText(1, _translate("Form", "2400")) self.s1__box_3.setItemText(2, _translate("Form", "4800")) self.s1__box_3.setItemText(3, _translate("Form", "9600")) self.s1__box_3.setItemText(4, _translate("Form", "14400")) self.s1__box_3.setItemText(5, _translate("Form", "19200")) self.s1__box_3.setItemText(6, _translate("Form", "38400")) self.s1__box_3.setItemText(7, _translate("Form", "57600")) self.s1__box_3.setItemText(8, _translate("Form", "76800")) self.s1__box_3.setItemText(9, _translate("Form", "12800")) self.s1__box_3.setItemText(10, _translate("Form", "230400")) self.s1__box_3.setItemText(11, _translate("Form", "460800")) self.s1__lb_4.setText(_translate("Form", "数据位：")) self.s1__box_4.setItemText(0, _translate("Form", "8")) self.s1__box_4.setItemText(1, _translate("Form", "7")) self.s1__box_4.setItemText(2, _translate("Form", "6")) self.s1__box_4.setItemText(3, _translate("Form", "5")) self.s1__lb_5.setText(_translate("Form", "校验位：")) self.s1__box_5.setItemText(0, _translate("Form", "N")) self.open_button.setText(_translate("Form", "打开串口")) self.close_button.setText(_translate("Form", "关闭串口")) self.s1__lb_6.setText(_translate("Form", "停止位：")) self.s1__box_6.setItemText(0, _translate("Form", "1")) self.verticalGroupBox.setTitle(_translate("Form", "接受区")) self.verticalGroupBox_2.setTitle(_translate("Form", "发送区")) self.s3__send_text.setHtml(_translate("Form", "&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0//EN\" \"http://www.w3.org/TR/REC-html40/strict.dtd\"&gt;\n""&lt;html&gt;&lt;head&gt;&lt;meta name=\"qrichtext\" content=\"1\" /&gt;&lt;style type=\"text/css\"&gt;\n""p, li &#123; white-space: pre-wrap; &#125;\n""&lt;/style&gt;&lt;/head&gt;&lt;body style=\" font-family:\'SimSun\'; font-size:9pt; font-weight:400; font-style:normal;\"&gt;\n""&lt;p style=\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\"&gt;123456&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;")) self.s3__send_button.setText(_translate("Form", "发送")) self.s3__clear_button.setText(_translate("Form", "清除")) self.formGroupBox1.setTitle(_translate("Form", "串口状态")) self.label.setText(_translate("Form", "已接收：")) self.label_2.setText(_translate("Form", "已发送：")) self.hex_send.setText(_translate("Form", "Hex发送")) self.hex_receive.setText(_translate("Form", "Hex接收")) self.s2__clear_button.setText(_translate("Form", "清除")) self.timer_send_cb.setText(_translate("Form", "定时发送")) self.lineEdit_3.setText(_translate("Form", "1000")) self.dw.setText(_translate("Form", "ms/次")) self.s4__open1_button.setText(_translate("Form", "打开1灯")) self.s4__open2_button.setText(_translate("Form", "打开2灯")) ui_demo.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243import sysimport serialimport serial.tools.list_portsfrom PyQt5 import QtWidgetsfrom PyQt5.QtWidgets import QMessageBoxfrom PyQt5.QtCore import QTimerfrom ui_demo_1 import Ui_Formclass Pyqt5_Serial(QtWidgets.QWidget, Ui_Form): def __init__(self): super(Pyqt5_Serial, self).__init__() self.setupUi(self) self.init() self.setWindowTitle("串口小助手") self.ser = serial.Serial() self.port_check() # 接收数据和发送数据数目置零 self.data_num_received = 0 self.lineEdit.setText(str(self.data_num_received)) self.data_num_sended = 0 self.lineEdit_2.setText(str(self.data_num_sended)) def init(self): # 串口检测按钮 self.s1__box_1.clicked.connect(self.port_check) # 串口信息显示 self.s1__box_2.currentTextChanged.connect(self.port_imf) # 打开串口按钮 self.open_button.clicked.connect(self.port_open) # 关闭串口按钮 self.close_button.clicked.connect(self.port_close) # 发送数据按钮 self.s3__send_button.clicked.connect(self.data_send) # 定时发送数据 self.timer_send = QTimer() self.timer_send.timeout.connect(self.data_send) self.timer_send_cb.stateChanged.connect(self.data_send_timer) # 定时器接收数据 self.timer = QTimer(self) self.timer.timeout.connect(self.data_receive) # 清除发送窗口 self.s3__clear_button.clicked.connect(self.send_data_clear) # 清除接收窗口 self.s2__clear_button.clicked.connect(self.receive_data_clear) # 打开1灯 self.s4__open1_button.clicked.connect(self.open_led1) # 打开2灯 self.s4__open2_button.clicked.connect(self.open_led2) # 串口检测 def port_check(self): # 检测所有存在的串口，将信息存储在字典中 self.Com_Dict = &#123;&#125; port_list = list(serial.tools.list_ports.comports()) self.s1__box_2.clear() for port in port_list: self.Com_Dict["%s" % port[0]] = "%s" % port[1] self.s1__box_2.addItem(port[0]) if len(self.Com_Dict) == 0: self.state_label.setText(" 无串口") # 串口信息 def port_imf(self): # 显示选定的串口的详细信息 imf_s = self.s1__box_2.currentText() if imf_s != "": self.state_label.setText(self.Com_Dict[self.s1__box_2.currentText()]) # 打开串口 def port_open(self): self.ser.port = self.s1__box_2.currentText() self.ser.baudrate = int(self.s1__box_3.currentText()) self.ser.bytesize = int(self.s1__box_4.currentText()) self.ser.stopbits = int(self.s1__box_6.currentText()) self.ser.parity = self.s1__box_5.currentText() try: self.ser.open() except: QMessageBox.critical(self, "Port Error", "此串口不能被打开！") return None # 打开串口接收定时器，周期为2ms self.timer.start(2) if self.ser.isOpen(): self.open_button.setEnabled(False) self.close_button.setEnabled(True) self.formGroupBox1.setTitle("串口状态（已开启）") # 关闭串口 def port_close(self): self.timer.stop() self.timer_send.stop() try: self.ser.close() except: pass self.open_button.setEnabled(True) self.close_button.setEnabled(False) self.lineEdit_3.setEnabled(True) # 接收数据和发送数据数目置零 self.data_num_received = 0 self.lineEdit.setText(str(self.data_num_received)) self.data_num_sended = 0 self.lineEdit_2.setText(str(self.data_num_sended)) self.formGroupBox1.setTitle("串口状态（已关闭）") # 发送数据 def data_send(self): if self.ser.isOpen(): input_s = self.s3__send_text.toPlainText() if input_s != "": # 非空字符串 if self.hex_send.isChecked(): # hex发送 input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) else: # ascii发送 input_s = (input_s + '\r\n').encode('utf-8') num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended)) else: pass # 接收数据 def data_receive(self): try: num = self.ser.inWaiting() except: self.port_close() return None if num &gt; 0: data = self.ser.read(num) num = len(data) # hex显示 if self.hex_receive.checkState(): out_s = '' for i in range(0, len(data)): out_s = out_s + '&#123;:02X&#125;'.format(data[i]) + ' ' self.s2__receive_text.insertPlainText(out_s) else: # 串口接收到的字符串为b'123',要转化成unicode字符串才能输出到窗口中去 self.s2__receive_text.insertPlainText(data.decode('iso-8859-1')) # 统计接收字符的数量 self.data_num_received += num self.lineEdit.setText(str(self.data_num_received)) # 获取到text光标 textCursor = self.s2__receive_text.textCursor() # 滚动到底部 textCursor.movePosition(textCursor.End) # 设置光标到text中去 self.s2__receive_text.setTextCursor(textCursor) else: pass # 定时发送数据 def data_send_timer(self): if self.timer_send_cb.isChecked(): self.timer_send.start(int(self.lineEdit_3.text())) self.lineEdit_3.setEnabled(False) else: self.timer_send.stop() self.lineEdit_3.setEnabled(True) # 清除显示 def send_data_clear(self): self.s3__send_text.setText("") def receive_data_clear(self): self.s2__receive_text.setText("") def open_led1(self): input_s = '1' input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended)) def open_led2(self): input_s = '2' input_s = input_s.strip() send_list = [] while input_s != '': try: num = int(input_s[0:2], 16) except ValueError: QMessageBox.critical(self, 'wrong data', '请输入十六进制数据，以空格分开!') return None input_s = input_s[2:].strip() send_list.append(num) input_s = bytes(send_list) num = self.ser.write(input_s) self.data_num_sended += num self.lineEdit_2.setText(str(self.data_num_sended))if __name__ == '__main__': app = QtWidgets.QApplication(sys.argv) myshow = Pyqt5_Serial() myshow.show() sys.exit(app.exec_()) CCS源代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174#include "DSP2833x_Device.h" // DSP2833x 头文件#include "DSP2833x_Examples.h" // DSP2833x 例子相关头文件/**************************************函数声明************************************************/void scib_echoback_init(void);void scib_xmit(int a);void scib_msg(char *msg);/**********************************************************************************************/// 使用前，声明本文件中的相关函数；void configtestled(void);/**************************************宏定义************************************************/#define InitDIR() \ EALLOW; \ GpioCtrlRegs.GPBPUD.bit.GPIO49 = 0; \ GpioCtrlRegs.GPBDIR.bit.GPIO49 = 1; \ GpioDataRegs.GPBCLEAR.bit.GPIO49 = 1; \ EDIS;#define RX_EN GpioDataRegs.GPBCLEAR.bit.GPIO49 = 1;#define TX_EN GpioDataRegs.GPBSET.bit.GPIO49 = 1;/**********************************************************************************************/void main(void)&#123; Uint16 ReceivedChar; //变量定义 char *msg; //指针// 步骤 1. 初始化系统控制:// 设置PLL, WatchDog, 使能外设时钟// 下面这个函数可以从DSP2833x_SysCtrl.c文件中找到.. InitSysCtrl(); InitScibGpio();// 步骤 2. 初始化通用输入输出多路复用器GPIO:// 这个函数在DSP2833x_Gpio.c源文件中被定义了// 这个函数使GPIO控制类寄存器初始化到默认状态// InitGpio(); // 本例不用此子函数 InitDIR();// 本例使用下面的GPIO配置 configtestled();// 总线初始化函数 InitXintf16Gpio(); //zq// 步骤 3. 清除所有中断初始化中断向量表:// 禁止CPU全局中断 DINT;// 初始化PIE控制寄存器到他们的默认状态.// 这个默认状态就是禁止PIE中断及清除所有PIE中断标志// 这个函数放在DSP2833x_PieCtrl.c源文件里 InitPieCtrl();// 禁止CPU中断和清除所有CPU中断标志 IER = 0x0000; IFR = 0x0000;//初始化PIE中断向量表，并使其指向中断服务子程序（ISR）// 这些中断服务子程序被放在了DSP280x_DefaultIsr.c源文件中// 这个函数放在了DSP2833x_PieVect.c源文件里面. InitPieVectTable(); // 步骤 4. 初始化片内外设:// 这个函数可以在DSP280x_CpuTimers.c源文件中找到// InitCpuTimers(); // 这个例子仅初始化了Cpu定时器// 步骤 5. 用户特定的代码 scib_echoback_init(); msg = "\r\n\nled control\0"; //发送语句 scib_msg(msg); //发送函数 msg = "\r\n \n\0"; //发送语句 scib_msg(msg); //发送函数 GpioDataRegs.GPASET.bit.GPIO1 = 1; GpioDataRegs.GPASET.bit.GPIO2 = 1; for(;;) &#123; msg = "\r\n \0"; //发送语句 scib_msg(msg); //发送函数 //等待接收到数据，否则在此循环 while(ScibRegs.SCIRXST.bit.RXRDY !=1) &#123; &#125; // wait for XRDY =1 for empty state //把接收BUF里的数据赋值给ReceivedChar ReceivedChar = ScibRegs.SCIRXBUF.all; msg = " led open \0"; //发送语句 scib_msg(msg); //发送函数 scib_xmit(ReceivedChar); //发送ReceivedChar if(ScibRegs.SCIRXBUF.all == 1) &#123; GpioDataRegs.GPACLEAR.bit.GPIO1 = 1; GpioDataRegs.GPASET.bit.GPIO2 = 1; msg = "1"; scib_msg(msg); &#125; else if (ScibRegs.SCIRXBUF.all == 2) &#123; GpioDataRegs.GPACLEAR.bit.GPIO2 = 1; GpioDataRegs.GPASET.bit.GPIO1 = 1; msg = "2"; scib_msg(msg); &#125;&#125;void configtestled(void) //GPIO初始化函数&#123; EALLOW; GpioCtrlRegs.GPAMUX1.bit.GPIO1 = 0; // GPIO0复用为GPIO功能 GpioCtrlRegs.GPADIR.bit.GPIO1 = 1; // GPIO0设置为输出 GpioCtrlRegs.GPAMUX1.bit.GPIO2 = 0; // GPIO1复用为GPIO功能 GpioCtrlRegs.GPADIR.bit.GPIO2 = 1; // GPIO1设置为输出 EDIS;&#125;void scib_echoback_init()&#123; // Note: Clocks were turned on to the SCIA peripheral // in the InitSysCtrl() function //SCI的工作模式和参数需要用户在后面的学习中，深入的了解一个寄存器底层相关的资料了，多看看芯片手册和寄存器的意思。 //因为28335的寄存器太多了，所以在以后的学习过程中，就不会对寄存器进行详细的注释了。 ScibRegs.SCICTL1.bit.SWRESET =0; ScibRegs.SCICCR.all =0x0007; // 1 stop bit, No loopback // No parity,8 char bits, // async mode, idle-line protocol ScibRegs.SCICTL1.all =0x0003; // enable TX, RX, internal SCICLK, // Disable RX ERR, SLEEP, TXWAKE #if (CPU_FRQ_150MHZ) ScibRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 37.5MHz. ScibRegs.SCILBAUD =0x00E7; #endif #if (CPU_FRQ_100MHZ) ScibRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 20MHz. ScibRegs.SCILBAUD =0x0044; #endif ScibRegs.SCICTL1.all =0x0023; // Relinquish SCI from Reset&#125;// Transmit a character from the SCIvoid scib_xmit(int a) //发送字节的函数&#123; while (ScibRegs.SCICTL2.bit.TXRDY == 0) &#123;&#125; ScibRegs.SCITXBUF=a;&#125;void scib_msg(char * msg) //发送数组的函数&#123; int i; i = 0; TX_EN; while(msg[i] != '\0') &#123; scib_xmit(msg[i]); i++; &#125; RX_EN;&#125;//===========================================================================// No more.//=========================================================================== 参考[1] 参考的串口程序 [2] 王硕,孙洋洋.PyQt5快速开发与实战[M].电子工业出版社:北京,2017]]></content>
      <tags>
        <tag>上位机</tag>
        <tag>DSP开发板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP基础知识一览]]></title>
    <url>%2F2019%2F08%2F10%2FDSP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B8%80%E8%A7%88%2F</url>
    <content type="text"><![CDATA[CCS编程基础CCS开发环境已经为我们封装好了许多片内外设寄存器的结构体，我们只需要包含相应的官方的头文件就可以使用了，那么它的内部具体是如何实现的呢？ 下面来一个典型的例子： 1.使用结构体和联合体A.用struct定义位域的作用：在DSP2833x_Sci.h中有一段: 1234567891011121314struct SCICCR_BITS &#123; // bit description Uint16 SCICHAR:3; // 2:0 Character length control Uint16 ADDRIDLE_MODE:1; // 3 ADDR/IDLE Mode control Uint16 LOOPBKENA:1; // 4 Loop Back enable Uint16 PARITYENA:1; // 5 Parity enable Uint16 PARITY:1; // 6 Even or Odd Parity Uint16 STOPBITS:1; // 7 Number of Stop Bits Uint16 rsvd1:8; // 15:8 reserved 保留&#125;; union SCICCR_REG &#123; Uint16 all; struct SCICCR_BITS bit;&#125;; Uint16 SCICHAR:3 表示定义SCICHAR，它占一个字节中的3位；*注意：必须以4字节对齐！观察上面的SCICCR_BITS的定义也会发现前面定义了3+1+1+1+1+1=8位=1字节 B.再来看union的作用 1234union SCICCR_REG&#123; Uint16 all; struct SCICCR_BITS bit;&#125; 这样定义有什么效果？当我想操作SCICCR_BITS中的每一位时，只需定义union SCICCR_REG reg即可 我们可以整体操作，如：reg.all = 0x0011; 我们可以操作其中一位：reg.bit.PARITY = 0; 还记得c语言中union中的共享同一个内存空间地址么？ 2.使用cmd文件进行数据段与存储器空间映射既然官方已经帮我们做好了上面的一切，上面的东西肯定可以直接使用，那么为什么可以直接使用呢？ 定义一个上面的变量就可以访问到真正硬件上的寄存器了吗？肯定不行！ 我们需要将上面的变量和实际硬件的寄存器存储空间绑定，怎么绑定，通过cmd文件。 下面是官方DSP2833x_GlobalVariableDefs.c中的一段代码： 1234567//----------------------------------------#ifdef __cplusplus#pragma DATA_SECTION("ScicRegsFile")#else#pragma DATA_SECTION(ScicRegs,"ScicRegsFile");#endifvolatile struct SCI_REGS ScicRegs; 官方定义了ScicRegs来操作串口SCI-C的相关的寄存器，但是肯定没法直接使用，还没有做绑定； 使用#pragma DATA_SECTION可以将变量与数据段绑定，变量和数据段是自己定义的，只需要将他们绑定即可； 这样绑定显然还不行，还需要通过cmd文件数据段映射到硬件的寄存器地址空间中去！ 查看DSP2833x_Headers_nonBIOS.cmd文件我们发现其中有这样几行： 12345678910111213141516171819MEMORY&#123; PAGE 0: /* Program Memory */ PAGE 1: /* Data Memory */ ADC : origin = 0x007100, length = 0x000020 /* ADC registers */ SCIB : origin = 0x007750, length = 0x000010 /* SCI-B registers */ SCIC : origin = 0x007770, length = 0x000010 /* SCI-C registers */ I2CA : origin = 0x007900, length = 0x000040 /* I2C-A registers */ &#125; SECTIONS&#123; AdcRegsFile : &gt; ADC, PAGE = 1 ScibRegsFile : &gt; SCIB, PAGE = 1 ScicRegsFile : &gt; SCIC, PAGE = 1 I2caRegsFile : &gt; I2CA, PAGE = 1&#125; MEMORY代表内存空间，PAGE0是程序空间， PAGE1是数据空间； (还记得第一课的介绍么？28335采样的哈佛总线结构，程序与数据分开了~) SECTIONS代表需要映射的段； 通过上面的映射后，操作ScicRegs就可以实际操作串口了，目的也就达到了； 时钟TMS320F28335上有一个基于PLL电路的片上时钟模块，如图1所示，为CPU及外设提供时钟有两种方式： 一种是用外部的时钟源，将其连接到X1引脚上或者XCLKIN引脚上，X2接地；另一种是使用振荡器产生时钟，用30MHz的晶体和两个20PF的电容组成的电路分别连接到X1和X2引脚上，XCLKIN引脚接地。 我们常用第二种来产生时钟。此时钟将通过一个内部PLL锁相环电路，进行倍频。由于F28335的最大工作频率是150M，所以倍频值最大是5。其中倍频值由PLLCR的低四位和PLLSTS的第7、8位来决定。其详细的倍频值可以参照TMS320F28335的Datasheet。 三种时钟输入的接法: 如果我们希望DSP工作在某一个频率下，我们就可以对Uint16 val, Uint16 divsel两个参数进行设定。说白了就相当于乘10，除2 (30*10/2 = 150MHZ) GPIO引脚GPIO（General-Purpose Input/Output）——通用输入/输出口 DSP28335 GPIO模块分为三类IO口：PORTA(0-31),PORTB(32-63),PORTC(64-87) 对GPIO模块的设置主要通过三类寄存器来完成，分别是：控制寄存器、数据寄存器、中断寄存器。 1、控制寄存器 12345678910111213141516171819 GPxCTRL; // GPIO x Control Register (GPIO0 to 31) //设置采样窗周期T=2*GPXCTRL*Tsysclk； GPxQSEL1; // GPIO x Qualifier Select 1 Register (GPIO0 to 15)(32-47) GPxQSEL2; // GPIO x Qualifier Select 2 Register (GPIO16 to 31)(48-63) //每两位控制一个引脚，确定是3周期采样还是6周期采样或者不用采样 GPxMUX1; // GPIO x Mux 1 Register (GPIO0 to 15)(32-47)(64-79) GPxMUX2; // GPIO x Mux 2 Register (GPIO16 to 31)(48-63)(80-95) //配置各个引脚的功能，0：I/O功能，1：外设功能。 GPxDIR; // GPIO x Direction Register (GPIO0 to 31)(32-63)(64-95) //配置每个引脚是输入还是输出，0：数字量输入；1：数字量输出。 GPxPUD; // GPIO x Pull Up Disable Register (GPIO0 to 31)(32-63)(64-95) //使能或禁止内部上拉 0：开启上拉，1：禁止上拉 2、数据寄存器 123456 GPxDAT; // GPIO Data Register (GPIO0 to 31)(32-63)(64-95) GPxSET; // GPIO Data Set Register (GPIO0 to 31)(32-63)(64-95)——置位 GPxCLEAR; // GPIO Data Clear Register (GPIO0 to 31)(32-63)(64-95) GPxTOGGLE; // GPIO Data Toggle Register (GPIO0 to 31)(32-63)(64-95)—反转 3、中断寄存器 12345678910111213141516 GPIOXINT1SEL; // XINT1 GPIO Input Selection GPIOXINT2SEL; // XINT2 GPIO Input Selection GPIOXNMISEL; // XNMI_Xint13 GPIO Input Selection GPIOXINT3SEL; // XINT3 GPIO Input Selection GPIOXINT4SEL; // XINT4 GPIO Input Selection GPIOXINT5SEL; // XINT5 GPIO Input Selection GPIOXINT6SEL; // XINT6 GPIO Input Selection GPIOXINT7SEL; // XINT7 GPIO Input Selection GPIOLPMSEL; // Low power modes GP I/O input select 可以对GPIO0-63进行外部中断设置； 具体定义在DSP28335Gpio.h中，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950struct GPIO_CTRL_REGS &#123; union GPACTRL_REG GPACTRL; // GPIO A Control Register (GPIO0 to 31) union GPA1_REG GPAQSEL1; // GPIO A Qualifier Select 1 Register (GPIO0 to 15) union GPA2_REG GPAQSEL2; // GPIO A Qualifier Select 2 Register (GPIO16 to 31) union GPA1_REG GPAMUX1; // GPIO A Mux 1 Register (GPIO0 to 15) union GPA2_REG GPAMUX2; // GPIO A Mux 2 Register (GPIO16 to 31) union GPADAT_REG GPADIR; // GPIO A Direction Register (GPIO0 to 31) union GPADAT_REG GPAPUD; // GPIO A Pull Up Disable Register (GPIO0 to 31) Uint32 rsvd1; union GPBCTRL_REG GPBCTRL; // GPIO B Control Register (GPIO32 to 63) union GPB1_REG GPBQSEL1; // GPIO B Qualifier Select 1 Register (GPIO32 to 47) union GPB2_REG GPBQSEL2; // GPIO B Qualifier Select 2 Register (GPIO48 to 63) union GPB1_REG GPBMUX1; // GPIO B Mux 1 Register (GPIO32 to 47) union GPB2_REG GPBMUX2; // GPIO B Mux 2 Register (GPIO48 to 63) union GPBDAT_REG GPBDIR; // GPIO B Direction Register (GPIO32 to 63) union GPBDAT_REG GPBPUD; // GPIO B Pull Up Disable Register (GPIO32 to 63) Uint16 rsvd2[8]; union GPC1_REG GPCMUX1; // GPIO C Mux 1 Register (GPIO64 to 79) union GPC2_REG GPCMUX2; // GPIO C Mux 2 Register (GPIO80 to 95) union GPCDAT_REG GPCDIR; // GPIO C Direction Register (GPIO64 to 95) union GPCDAT_REG GPCPUD; // GPIO C Pull Up Disable Register (GPIO64 to 95)&#125;; struct GPIO_DATA_REGS &#123; union GPADAT_REG GPADAT; // GPIO Data Register (GPIO0 to 31) union GPADAT_REG GPASET; // GPIO Data Set Register (GPIO0 to 31) union GPADAT_REG GPACLEAR; // GPIO Data Clear Register (GPIO0 to 31) union GPADAT_REG GPATOGGLE; // GPIO Data Toggle Register (GPIO0 to 31) union GPBDAT_REG GPBDAT; // GPIO Data Register (GPIO32 to 63) union GPBDAT_REG GPBSET; // GPIO Data Set Register (GPIO32 to 63) union GPBDAT_REG GPBCLEAR; // GPIO Data Clear Register (GPIO32 to 63) union GPBDAT_REG GPBTOGGLE; // GPIO Data Toggle Register (GPIO32 to 63) union GPCDAT_REG GPCDAT; // GPIO Data Register (GPIO64 to 95) union GPCDAT_REG GPCSET; // GPIO Data Set Register (GPIO64 to 95) union GPCDAT_REG GPCCLEAR; // GPIO Data Clear Register (GPIO64 to 95) union GPCDAT_REG GPCTOGGLE; // GPIO Data Toggle Register (GPIO64 to 95) Uint16 rsvd1[8];&#125;; struct GPIO_INT_REGS &#123; union GPIOXINT_REG GPIOXINT1SEL; // XINT1 GPIO Input Selection union GPIOXINT_REG GPIOXINT2SEL; // XINT2 GPIO Input Selection union GPIOXINT_REG GPIOXNMISEL; // XNMI_Xint13 GPIO Input Selection union GPIOXINT_REG GPIOXINT3SEL; // XINT3 GPIO Input Selection union GPIOXINT_REG GPIOXINT4SEL; // XINT4 GPIO Input Selection union GPIOXINT_REG GPIOXINT5SEL; // XINT5 GPIO Input Selection union GPIOXINT_REG GPIOXINT6SEL; // XINT6 GPIO Input Selection union GPIOXINT_REG GPIOXINT7SEL; // XINT7 GPIO Input Selection union GPADAT_REG GPIOLPMSEL; // Low power modes GP I/O input select&#125;; 注意：GPIO相关寄存器介绍 1、GPxMUX寄存器（功能选择寄存器） 每个I/O口都有一个功能选择寄存器，功能选择寄存器主要用于选择I/O工作在特殊功能还是通用数组I/O模式。在复位时，所有GPIO配置成通用数字模式。 1）如果GPxMUX.bit = 0，配置成通用数字I/O功能； 2）如果GPxMUX.bit = 1 2 3，配置成特殊外设功能口（如SCI、CAN）； I/O的输入功能和外设的输入通道总是被使能的，输出通道是通用数组I/O和特殊外设复用的。如果引脚配置成通用数组I/O功能，相应的外设功能将被禁止。 2、GPxDIR（方向控制寄存器） 每个I/O口都有数据方向控制寄存器，数据方向控制寄存器用于设置通用数字I/O为输入还是输出口，在复位时，引脚的默认状态为输入状态。 1）如果GPxDIR.bit = 0，引脚设置为通用数字量输入； 2）如果GPxDIR.bit = 1，引脚设置为通用数字量输出； 复位时，GPxMUX和GPxDIR默认值都为0，所以在复位时，引脚的默认状态为数字I/O输入。 3、GPxDAT寄存器（数据寄存器） 每个I/O口都有一个数据寄存器，数据寄存器是可读可写寄存器。 1）I/O设置为输出功能时，如果GPxDAT.bit = 0，那么操作将会使相应的引脚拉低； 2）I/O口设置为输入功能时，如果GPxDAT.bit = 0，反映相应的引脚状态为低电平； 3）I/O口设置为输出功能时，如果GPxDAT.bit = 1，那么操作将会使相应的引脚拉高； 4）I/O口设置为输入功能时，如果GPxDAT.bit = 1，反映相应的引脚状态为高电平。 需要说明的是，当用户试图改变一个数字I/O的状态时，不要改变另一个I/O的引脚状态。 4、GOxSET寄存器（置位寄存器） 每个I/O口都有一个置位寄存器，置位寄存器是只写寄存器，任何读操作都返回0，如果相应的引脚配置成数据量输出，写1后相应的引脚会置高，写0时没有反映。 1）如果GPxSET.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxSET.bit = 1，那么操作将会使引脚置高。 5、GPxCLEAR寄存器（清除寄存器） 每个I/O口都有一个清除寄存器，清除寄存器是只写寄存器，任何读操作都返回0。 1）如果GPxCLEAR.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxCLEAR.bit = 1，将相应的引脚置成低电平。 6、GPxTOGGLE寄存器（取反触发寄存器） 每个I/O口都有一个取反触发寄存器，该寄存器是只写寄存器，任何读操作都返回0。 1）如果GPxTOGGLE.bit = 0，没有影响； 2）引脚设置为输出时，如果GPxTOGGLE.bit = 1，那么操作将使相应的引脚取反。 中断1.中断系统 在这里我们要十分清楚DSP的中断系统。C28XX一共有16个中断源，其中有2个不可屏蔽的中断RESET和NMI、定时器1和定时器2分别使用中断13和14。这样还有12个中断都直接连接到外设中断扩展模块PIE上。说的简单一点就是PIE通过12根线与28335核的12个中断线相连。而PIE的另外一侧有12*8根线分别连接到外设，如AD、SPI、EXINT等等。 PIE共管理12*8=96个外部中断。这12组大中断由28335核的中断寄存器IER来控制，即IER确定每个中断到底属于哪一组大中断（如IER |= M_INT12; 说明我们要用第12组的中断，但是第12组里面的什么中断CPU并不知道需要再由PIEIER确定）。 接下来再由PIE模块中的寄存器PIEIER中的低8确定该中断是这一组的第几个中断，这些配置都要告诉CPU（我们不难想象到PIEIER共有12总即从PIEIER1-PIEIER12）。另外，PIE模块还有中断标志寄存器PIEIFR，同样它的低8位是来自外部中断的8个标志位，同样CPU的IFR寄存器是中断组的标志寄存器。由此看来，CPU的所有中断寄存器控制12组的中断，PIE的所有中断寄存器控制每组内8个的中断。除此之外，我们用到哪一个外部中断，相应的还有外部中断的寄存器，需要注意的就是外部中断的标志要自己通过软件来清零。而PIE和CPU的中断标志寄存器由硬件来清零。 12345678EALLOW; // This is needed to write to EALLOW protected registers PieVectTable.XINT2 = &amp;ISRExint; //告诉中断入口地址EDIS; // This is needed to disable write to EALLOW protected registersPieCtrlRegs.PIECTRL.bit.ENPIE = 1; // Enable the PIE block使能PIEPieCtrlRegs.PIEIER1.bit.INTx5= 1; //使能第一组中的中断5IER |= M_INT1; // Enable CPU 第一组中断EINT; // Enable Global interrupt INTMERTM; // Enable Global realtime interrupt DBGM 也就是说，12组中的每个中断都要完成上面的相同配置，剩下的才是去配置自己的中断。如我们提到的EXINT，即外面来个低电平我们就进入中断，完成我们的程序。在这里要介绍一下，DSP的GPIO口都可以配置为外部中断口，其配置方法如下： 1234567891011121314151617181920212223242526272829GpioCtrlRegs.GPBMUX2.bit.GPIO54 = 0; //选择他们是GPIO口GpioCtrlRegs.GPBMUX2.bit.GPIO55 = 0;GpioCtrlRegs.GPBMUX2.bit.GPIO56 = 0;GpioCtrlRegs.GPBMUX2.bit.GPIO57 = 0; GpioCtrlRegs.GPBDIR.bit.GPIO54 = 0;//选择他们都是输入口GpioCtrlRegs.GPBDIR.bit.GPIO55 = 0;GpioCtrlRegs.GPBDIR.bit.GPIO56 = 0;GpioCtrlRegs.GPBDIR.bit.GPIO57 = 0; GpioCtrlRegs.GPBQSEL2.bit.GPIO54= 0;//使GPIO时钟和系统时钟一样 且支持GPIOGpioCtrlRegs.GPBQSEL2.bit.GPIO55= 0;GpioCtrlRegs.GPBQSEL2.bit.GPIO56= 0;//配置输入口权限，对于选择为输入口的需配置GPACTRL,GPBCTRL,GPAQSEL1GpioCtrlRegs.GPBQSEL2.bit.GPIO57= 0;//GPAQSEL2, GPBQSEL1, and GPBQSEL2寄存器所有输入信号与CPU输出系统时钟同步； GpioIntRegs.GPIOXINT3SEL.bit.GPIOSEL = 54;//中断3选择GPIOGpioIntRegs.GPIOXINT4SEL.bit.GPIOSEL = 55;GpioIntRegs.GPIOXINT5SEL.bit.GPIOSEL = 56;GpioIntRegs.GPIOXINT6SEL.bit.GPIOSEL = 57; XIntruptRegs.XINT3CR.bit.POLARITY= 0;//触发模式为下降沿触发XIntruptRegs.XINT4CR.bit.POLARITY= 0;XIntruptRegs.XINT5CR.bit.POLARITY= 0;XIntruptRegs.XINT6CR.bit.POLARITY= 0; XIntruptRegs.XINT3CR.bit.ENABLE = 1;//使能中断XIntruptRegs.XINT4CR.bit.ENABLE = 1;XIntruptRegs.XINT5CR.bit.ENABLE = 1;XIntruptRegs.XINT6CR.bit.ENABLE = 1; 注意一点就是外部中断1和2只能对GPIO0—GPIO31配置；外部中断3和4、5、6、7只对GPIO32—GPIO63配置。 GPIO分为A(0-31)、B(32-63)、C(64-87);C组的不能配置为外部中断； 2.如何开启某个中断？ 设置中断向量。例如：PieVectTable.ADCINT = &amp;adc_isr;等打开PIE控制器。PieCtrlRegs.PIECTRL.bit.ENPIE = 1;使能PIE中对应外设的中断（相应group的相应pin）。例如：PieCtrlRegs.PIEIER1.bit.INTx8 = 1; PieCtrlRegs.PIEIER1.bit.INTx6 = 1;等使能CPU的相应中断（INT1~INT12）IER |= M_INT1;使能CPU响应中断EINT、ERTM;; 3.中断标志有几级？作用是什么？ 中断标志主要有三级CPU（有16个标志位）、PIE（有12组每组有12个标志位）和外设（有的外设没有）。 标志位在中断发生后锁存中断状态，即表示中断发生。在CPU响应中断后，会自动清除cpu级别的标志位IFR bit，同时将INTM bit 置位，以防止其它中断的发生； CPU在从PIE中取中断向量时PIE会自动清除PIE级别的标志位PIEIFRx.y。所以在进入中断处理程序后除了外设所有中断位都已经清除。 而中断处理程序中需要清除PIEACKx和外设的中断标志位（如果有的话）。 在CPU响应一个中断后，在进入ISR的时候，默认会关断全局中断，即在执行中断服务程序时，不会有其他中断来打断CPU，包括本次的中断事件。另外，如果外设的中断标志位不清除，不会循环进入这个中断服务函数，这个外设中断被阻断了。所以只有清除外设的中断服务程序，才能响应下一次的外设中断。PIEACK同理，如果没有PIEACK，这组所有中断都被阻断。 参考文献[1] TMS320F2833x Datasheet [2] 风雨也无晴 CSDN [3] GPIO blog]]></content>
      <tags>
        <tag>DSP28335</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP 串口接口及通信通信]]></title>
    <url>%2F2019%2F08%2F09%2F%E4%B8%B2%E5%8F%A3%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[对串口通信进行简单知识点梳理串行通信可以分为两大类： 同步通信：典型 I2C,SPI 异步通信：典型 SCI(serial communication interface,串行通信接口) 进行串口异步通信接口，一般可以看作UART口(Universal Asynchronous Receiver Transmitter：通用异步收发器/异步串行通信口) UART、COM指物理接口形式(硬件), TTL、RS232、RS485 指电平标准(电信号) SCI 串口先来补充一个概念:FIFO（First Input First Output），即先进先出队列。 SCI模块介绍TMS320F28335内部有三个SCI模块，SCIA、SCIB、SCIC。 每一个SCI模块都有一个接收器和发送器，SCI的接收器和发送器各有一个16级的FIFO(First In First Out先入先出)队列，它们都还有自己独立的使能位和中断位；可以工作在半双工或全双工模式。 1. SCI的CPU 接口SCI 模块具有两个引脚， SCITXDA 和 SCIRXDA，分别实现发送数据和接收数据的功能，这两个引脚对应于 GPIOF 模块的第4和第5位，在编程初始化的时候，需要将GPIOFMUX 寄存器的第4和第5位置为1，才能使得这两个引脚具有发送和接收的功能，否则就是普通的I/O引脚。外部晶振通 PLL 模块产生了CPU 的系统时钟SYSCLKOUT，然后SYSCLKOUT经过低速预定标器之后输出低速时钟LSPCLK 供给SCI。要保证SCI的正常运行，系统控制模块下必须使能SCI的时钟，也就是在系统初始化函数中需要将外设时钟控制寄存器PCLKCR的SCIAENCLK位置1。从下图，我们可以清楚的看到SCIA可以产生两个中断，SCIRXINTA 和SCITXINTA，即发送中断和接收中断。 2. SCI相关寄存器 SCICR：SCI通信参数设置寄存器，设置数据位，停止位，奇偶校验位。 SCICTL1：使能SCI的发送接收功能 注：SW RESET需置1 SCILBAUD、SCIHBAUD：通信速率（波特率）的设置。 SCICTL2：使能接收发送中断，以及发送中断标志位。 SCIRXST：接收相关标志位。 SCIRXBUF：8位发送缓存寄存器 SCITXBUF：8位接收缓存寄存器。 SCI中断配置： SCI的中断采用三级中断管理。分别是SCI外设中断，PIE中断，CPU中断。SCIA的PIE中断是第九组，分别是INT９.１和INT９.２.PIE中断的配置在前面已经说过了，此处不多说。注意：在中断不要忘记将PIEACK写１清除。 SCI的FIFO模式： FIFO：先入先出队列。SCI采用这种模式时，接收或者发送完指定字节数量的数据后，才进入中断处理。这样可以节省了CPU的使用效率，CPU不用每次接收完一个字节的数据后就进入中断处理。 相关寄存器： SCIFFTX：配置发送的数据量，使能SCI的FIFO模式，使能中断等 SCIFFRX：配置接收的数据量，使能接收中断等。 在学习FIFO模式时，遇到一个问题，就是接收完指定数量字节的数据后，总是重复进入两次发送中断，一次找不到原因。下面贴出代码，希望各位读者不吝赐教。 3. SCI 模块发送和接收数据的工作原理 SCI 模块的工作原理如下图所示，之所以SCI 能工作于全双工模式，是因为它有独立的数据发送器和数据接收器，这样能够保证SCI既能够同时进行，也能够独立进行发送和接收的操作。 SCI 发送数据的过程如下：如下图右半部分所示， 在FIFO功能使能的情况下， 首先，发送数据缓冲寄存器SCITXBUF从TX FIFO 中获取由 CPU 加载的需要发送的数据，然后 SCITXBUF将数据传输给发送移位寄存器TXSHF， 如果SCI的发送功能使能， TXSHF 则将接收到的数据逐位逐位的移到 SCITXD 引脚上。 SCI接收数据的过程如下：如X下图的左半部分所示，首先，接收移位寄存器 RXSHF 逐位逐位的接收来自于 SCIRXD 引脚的数据， 如果 SCI 的接收功能使能， RXSHF 将这些数据传输给接收缓冲寄存器 SCIRXBUF，CPU 就能从 SCIRXBUF 读取外部发送来的数据。当然，如果 FIFO 功能使能的话， SCIRXBUF 会将数据加载到RX FIFO 的队列中， CPU 再从FIFO 的队列读取数据。 4. SCI数据格式 在 SCI 中，通信协议体现在 SCI 的数据格式上。 通常将 SCI 的数据格式称之为可编程的数据格式，原因就是可以通过 SCI 的通信控制寄存器 SCICCR 来进行设置，规定通信过程中所使用的数据格式。 在空闲线模式下， SCI 发送或者接收一帧的数据格式如图 4 所示，其中 LSB 是数据的最低位， MSB 是数据的最高位。 使用 SCICCR 进行数据格式编程 12345678SciaRegs.SCICCR.bit.SCICHAR=8;//选择数据长度，为 8 个数据位SciaRegs.SCICCR.bit.PARITYENA=1;//开启极性功能，值为 0 的时候取消极性功能SciaRegs.SCICCR.bit.PARITY=0;//在开启极性功能的前提下，该位值为 0 时选择偶极性，值为 1 时选择奇极性SciaRegs.SCICCR.bit.STOPBITS=0;//选择停止位，该位为 0 时有 1 个停止位，该位为 1 时有 2 个停止位 当然，上述这几个语句，我们也可以合并成如下的语句： 1SciaRegs.SCICCR.all=0x13; 5. SCI通信波特率 所谓的波特率就是指每秒所能发送的位数。SCI波特率设置寄存器SCIHBAUD和SCILBAUD，0-15是高字节与低字节连在一起，构成16位波特率设置寄存器BRR。BRR = SCIHBAUD + SCILBAUD 如果1&lt;= BRR &lt;=65535，那么SCI波特率=LSPCLK / ( (BRR+1) * 8 )，由此，可以带入你需要的波特率，既可以得到BRR的值；如果BRR = 0，那么SCI波特率=LSPCLK/ 16 我们举例来进行说明。例如外部晶振位 30M，经过 PLL 之后 SYSCLKOUT 为 150MHz，然后，当低速预定标器 LOSPCP 的值为 2 的时候， SYSCLKOUT 经过低速预定标器之后产生&gt;低速外设时钟 LSPCLK 为 37.5MHz，也就是说 SCI 的时钟为 37.5MHz。如果我们需要 SCI 的波特率为 19200，则将 LSPCLK 和波特率的数值代入式 1，便可得到BRR=243.14，由于寄存器都是正整数，所以省略掉小数后可以得到 BRR=243。将 243 转换成 16 进制是 0xF3，因此 SCIHBAUD 的值为 0， SCIHBAUD 的值为 0XF3。由于省略了小数，将会产生 0.06%的误差。 当 LSPCLK 为 37.5M 时，对于 SCI 常见的波特率，其寄存器的值如下表所示： 6. 串口SCI编程 A. 先初始化IO管脚 (以SCI-A为例，SCI-B、SCI-C的初始化方法一样，就是照着改对应的管脚就行) 1234567891011121314void InitSciaGpio() //初始化SCIA的GPIO管脚为例子&#123;EALLOW;//根据硬件设计决定采用GPIO28/29和GPIO35/36中的哪一组。这里以35/36为例//定义管脚为上拉GpioCtrlRegs.GPBPUD.bit.GPIO36 = 0;GpioCtrlRegs.GPBPUD.bit.GPIO35 = 0;//定义管脚为异步输入GpioCtrlRegs.GPBQSEL1.bit.GPIO36 = 3;//配置管脚为SCI功能管脚GpioCtrlRegs.GPBMUX1.bit.GPIO36 = 1;GpioCtrlRegs.GPBMUX1.bit.GPIO35 = 1;EDIS;&#125; B. SCI初始化配置 12345678910111213void scia_init()&#123;SciaRegs.SCICCR.all =0x0007; // 1 stop bit, No loopback// No parity,8 char bits,// async mode, idle-line protocolSciaRegs.SCICTL1.all =0x0003; // enable TX, RX, internal SCICLK,// Disable RX ERR, SLEEP, TXWAKESciaRegs.SCICTL2.bit.TXINTENA =1; //发送中断使能SciaRegs.SCICTL2.bit.RXBKINTENA =1;//接收中断使能SciaRegs.SCIHBAUD =0x0001; // 9600 baud @LSPCLK = 37.5MHz.SciaRegs.SCILBAUD =0x00E7;SciaRegs.SCICTL1.all =0x0023; // Relinquish SCI from Reset&#125; C. 接着进行中断的配置 123456EALLOW; // This is needed to write to EALLOW protected registersPieVectTable.SCIRXINTA = &amp;sciaRxIsr;PieVectTable.SCITXINTA = &amp;sciaTxIsr;PieVectTable.SCIRXINTB = &amp;scibRxIsr;PieVectTable.SCITXINTB = &amp;scibTxIsr;EDIS; // This is needed to disable write to EALLOW protected registers D. 上面是将SCIA和SCIB的中断服务程序连到PIE的中断表中，发生中断就会跑到你的ISR去了，下面是开中断： 1234567PieCtrlRegs.PIECTRL.bit.ENPIE = 1; // Enable the PIE blockPieCtrlRegs.PIEIER9.bit.INTx1=1; // PIE Group 9, int1PieCtrlRegs.PIEIER9.bit.INTx2=1; // PIE Group 9, INT2PieCtrlRegs.PIEIER9.bit.INTx3=1; // PIE Group 9, INT3PieCtrlRegs.PIEIER9.bit.INTx4=1; // PIE Group 9, INT4IER = 0x100; // Enable CPU INTEINT; 这样串口基本就OK了。 参考文献[1] 接口及协议总结[2] SCI通信]]></content>
      <tags>
        <tag>上位机</tag>
        <tag>DSP28335</tag>
        <tag>串口通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSP电机平台上位机编写(Python+pyqt)]]></title>
    <url>%2F2019%2F08%2F06%2FDSP%E5%B9%B3%E5%8F%B0%E4%B8%8A%E4%BD%8D%E6%9C%BA%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[准备给DSP电机平台增加一个电机转速调节的上位机。 目的： 1.学习python，并利用它做点东西 2.对串口通信有更加深刻的了解 计划步骤： 1.先把Python的基础知识有大体了解 2.学习pyqt并绘制上位机界面 3.做好串口通信的工作 环境的搭建环境搭建参考： 环境搭建 再由.ui转成.py时，再最后添加：参考 Ui_MainWindow 要和前面类名一致 1234567891011class MyWindow(QtWidgets.QMainWindow, Ui_MainWindow): def __init__(self): super(MyWindow, self).__init__() self.setupUi(self)if __name__ == '__main__': import sys app = QtWidgets.QApplication(sys.argv) mywindow = MyWindow() mywindow.show() sys.exit(app.exec_()) 槽函数 未完待续~~]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>上位机</tag>
        <tag>Python</tag>
        <tag>pyqt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[柔性负载-肖曦]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%9F%94%E6%80%A7%E8%B4%9F%E8%BD%BD-%E8%82%96%E6%9B%A6%2F</url>
    <content type="text"><![CDATA[关于肖老师柔性负载的文献综述一、柔性负载建模–中心刚体-悬臂梁系统(欧拉-伯努利梁)[5]应用于 工业机器人中的柔性机械臂。单柔性连杆伺服驱动系统如下图所示。图中：u(x,t)为挠性负载在x处的挠度；θm(t)为伺服电机转轴的转角；Ta为伺服电机驱动转矩。 二、PMSM驱动柔性负载相关公式：[2]在对柔性负载建模后， 该系统的传递函数如下:(Ta表示电磁转矩，参数Ω 表示系统谐振频率， 而 Fa 则可以表示每一阶模态频率的谐振程度，通过系统模型和参数可以很容易地观察系统谐振状况。)(一般选取一阶模态，即Fa，Ω均为标量) 转速环和电流环传递函数 系统控制框图 (1)电流环特征–柔性负载影响小 刚性电流环开环传递函数 与上面的 柔性负载 电流环传递函数相比，其差别主要在分母的第三项。故可以对其分母第三项单独分析。 但是，由于柔性负载的振动频率与电流环带宽差别较大，因而，柔性对 PMSM 电流环的影响较小. (2)转速环环特征–柔性负载影响大 刚性负载 转速外环控制框图 柔性负载 转速外环控制框图（在不考虑电流环影响下，由系统框体可得） 转速环开环伯德图 结论： 在 PMSM 直接驱动柔性负载系统中，负载柔性对系统电流环影响较小，对转速环影响较大。 需要在 柔性负载振荡频率处 进行谐振补偿 三、柔性负载常用控制方法 转速环设计(1)PI设计[3] 转速环的开环传递函数(II型系统) （系统谐振模态幅值 η ） 在不考虑电流内环的影响，采用PI调节器 。转速外环的控制框图如下 (2)状态反馈 + PI 调节器控制[4] 在PI调节器的基础上，估计系统谐振模态幅值 η 和 负载转矩 TL 其中 GFF(s)是Wm与 TL 解耦得到 并存在 k1、k2、kP、kI 四个可调参数，因而四个极点能够任意配置. 参考文献[1]丁有爽,肖曦.基于负载位置反馈的永磁同步电机驱动柔性负载谐振抑制方法[J].电工技术学报,2017,32(11):96-110. [2]丁有爽,肖曦.永磁同步电机直接驱动柔性负载控制方法[J].电工技术学报,2017,32(04):123-132. [3]丁有爽,肖曦.基于极点配置的永磁同步电机驱动柔性负载PI调节器参数确定方法[J].中国电机工程学报,2017,37(04):1225-1239. [4]丁有爽,肖曦.基于状态反馈和转矩补偿的永磁同步电机驱动柔性负载控制方法[J].中国电机工程学报,2017,37(13):3892-3900. [5]丁有爽,肖曦.伺服系统柔性负载建模方法研究[J].中国电机工程学报,2016,36(03):818-827. [6] Hori Y，Sawada H，Chun Y．Slow resonance ratio control for vibration suppression and disturbance rejection in torsional system[J]．IEEE Transactions on Industrial Electronics，1999，46(1)：162-168]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>柔性负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于加入站内搜索]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%85%B3%E4%BA%8E%E5%8A%A0%E5%85%A5%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[今天按照网上的加入本站搜索功能 无论如何都加不进去，后来在查看 NEXT给的官方解读中看到解决方法网址如下，查看Local Search方法 https://theme-next.org/docs/third-party-services/search-services]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于DSP平台双UDE 参数调试]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%85%B3%E4%BA%8EDSP%E5%B9%B3%E5%8F%B0%E5%8F%8CUDE%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1.挑选了几组典型数据画图，这里面的滤波器参数都是随意设置的（因为现在还无法确定kp和alpha之间的关系） 2.xx_xxx3.fig 中红色为实际数值 蓝色为指令，绿色为经过滤波器的输出结果(画图程序在最后)。 3.图中的时间 10000点 = 5s 4.Main_twoloop190801.c为源程序。 5.文件夹中 01-07为电流环调试过程, 08-11为速度环调试过程, 12 给了一个比较极端的速度环滤波器参数。 数据对应参数： spd_Factor spd_kp spd_ki iq_Factor iq_kp iq_ki 01参数 0.1667 0.8 0.0015 0.007 1.0 0.0025 05参数 0.1667 0.8 0.0015 0.007 2.1 0.0025 06参数 0.1667 0.8 0.0015 0.007 2.5 0.0025 10参数 0.1667 3.0 0.0015 0.007 2.5 0.0025 12参数 0.0007 0.8 0.0015 0.007 2.5 0.0025 6.画图函数 1234567891011121314figure(1)plot(t,spd,'r','LineWidth',1)hold onplot(t,spdr-5,'g','LineWidth',1)hold onplot(t,spdc-5,'b','LineWidth',1)grid onfigure(2)plot(t,iq,'r','LineWidth',1)hold onplot(t,iqr,'g','LineWidth',1)hold onplot(t,iqc,'b','LineWidth',1)grid on]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>PMSM</tag>
        <tag>UDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不懂的知识点 查阅 汇总]]></title>
    <url>%2F2019%2F08%2F01%2F%E4%B8%8D%E6%87%82%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[单工、半双工和全双工的区别 一、单工1、数据只在一个方向上传输，不能实现双方通信。 2、栗子：电视、广播。 二、半双工1、允许数据在两个方向上传输，但是同一时间数据只能在一个方向上传输，其实际上是切换的单工。 2、栗子：对讲机。 三、全双工1、允许数据在两个方向上同时传输。 2、栗子：手机通话。 TCP/IP （传输控制协议）Transmission Control Protocol TCP 用于应用程序之间的通信。当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信。 IP 用于计算机之间的通信。IP 是无连接的通信协议。它不会占用两个正在通信的计算机之间的通信线路。这样，IP 就降低了对网络线路的需求。每条线可以同时满足许多不同的计算机之间的通信需要。 TCP/IP 意味着 TCP 和 IP 在一起协同工作。TCP 负责应用软件（比如你的浏览器）和网络软件之间的通信。IP 负责计算机之间的通信。TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。IP 负责将包发送至接受者。 api （应用程序编程接口）API 是一套明确定义的各种软件组件之间的通信方法。 http、MQTT、CoAPHTTP是一个简单的请求-响应协议，它通常运行在TCP之上。它指定了客户端可能发送给服务器什么样的消息以及得到什么样的响应。 MQTT（Message Queuing Telemetry Transport，消息队列遥测传输协议），是一种基于发布/订阅（publish/subscribe）模式的”轻量级”通讯协议，该协议构建于TCP/IP协议上，由IBM在1999年发布。MQTT最大优点在于，可以以极少的代码和有限的带宽，为连接远程设备提供实时可靠的消息服务。支持长连接！适用于抄表 CoAP 由于物联网中的很多设备都是资源受限型的，即只有少量的内存空间和有限的计算能力，所以传统的HTTP协议应用在物联网上就显得过于庞大而不适用。 IETF的CoRE工作组提出了一种基于REST架构的CoAP协议;是一种在物联网世界的类web协议。不支持长连接 基于UDO 可靠性不高 适用于智能家居 OSI七层模型/TCP/IP五层模型：OSI七层模型(整个过程以公司A和公司B的一次商业报价单发送为例子进行讲解。) &lt;1&gt; 应用层 OSI参考模型中最靠近用户的一层，是为计算机用户提供应用接口，也为用户直接提供各种网络服务。我们常见应用层的网络服务协议有：HTTP，HTTPS，FTP，POP3、SMTP等。 实际公司A的老板就是我们所述的用户，而他要发送的商业报价单，就是应用层提供的一种网络服务，当然，老板也可以选择其他服务，比如说，发一份商业合同，发一份询&gt; 价单，等等。 &lt;2&gt;表示层 表示层提供各种用于应用层数据的编码和转换功能,确保一个系统的应用层发送的数据能被另一个系统的应用层识别。如果必要，该层可提供一种标准表示形式，用于将计算机内部的多种数据格式转换成通信中采用的标准表示形式。数据压缩和加密也是表示层可提供的转换功能之一。 由于公司A和公司B是不同国家的公司，他们之间的商定统一用英语作为交流的语言，所以此时表示层（公司的文秘），就是将应用层的传递信息转翻译成英语。同时为了防止别的公司看到，公司A的人也会对这份报价单做一些加密的处理。这就是表示的作用，将应用层的数据转换翻译等。 &lt;3&gt;会话 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。 会话层的同事拿到表示层的同事转换后资料，（会话层的同事类似公司的外联部），会话层的同事那里可能会掌握本公司与其他好多公司的联系方式，这里公司就是实际传递过程中的实体。他们要管理本公司与外界好多公司的联系会话。当接收到表示层的数据后，会话层将会建立并记录本次会话，他首先要找到公司B的地址信息，然后将整份资料放进信封，并写上地址和联系方式。准备将资料寄出。等到确定公司B接收到此份报价单后，此次会话就算结束了，外联部的同事就会终止此次会话。 &lt;4&gt;传输层 传输层建立了主机端到端的链接，传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务，包括处理差错控制和流量控制等问题。该层向高层屏蔽了下层数据通信的细节，使高层用户看到的只是在两个传输实体间的一条主机到主机的、可由用户控制和设定的、可靠的数据通路。我们通常说的， TCP UDP 就是在这一层。端口号既是这里的“端”。 传输层就相当于公司中的负责快递邮件收发的人，公司自己的投递员，他们负责将上一层的要寄出的资料投递到快递公司或邮局。 &lt;5&gt;网络层 本层通过IP寻址来建立两个节点之间的连接，为源端的运输层送来的分组，选择合适的路由和交换节点，正确无误地按照地址传送给目的端的运输层。就是通常说的IP层。这一层就是我们经常说的IP协议层。IP协议是Internet的基础。 网络层就相当于快递公司庞大的快递网络，全国不同的集散中心，比如说，从深圳发往北京的顺丰快递（陆运为例啊，空运好像直接就飞到北京了），首先要到顺丰的深圳集散中心，从深圳集散中心再送到武汉集散中心，从武汉集散中心再寄到北京顺义集散中心。这个每个集散中心，就相当于网络中的一个IP节点。 &lt;6&gt;数据链路层 将比特组合成字节,再将字节组合成帧,使用链路层地址 (以太网使用MAC地址)来访问介质,并进行差错检测。数据链路层又分为2个子层：逻辑链路控制子层（LLC）和媒体访问控制子层（MAC）。MAC子层处理CSMA/CD算法、数据出错校验、成帧等；LLC子层定义了一些字段使上次协议能共享数据链路层。 在实际使用中，LLC子层并非必需的。 这个没找到合适的例子 &lt;7&gt; 物理层 实际最终信号的传输是通过物理层实现的。通过物理介质传输比特流。规定了电平、速度和电缆针脚。常用设备有（各种物理设备）集线器、中继器、调制解调器、网线、双绞线、同轴电缆。这些都是物理层的传输介质。 快递寄送过程中的交通工具，就相当于我们的物理层，例如汽车，火车，飞机，船。 [ TCP/IP五层模型 TCP/IP五层协议和OSI的七层协议对应关系如下。 在每一层都工作着不同的设备，比如我们常用的交换机就工作在数据链路层的，一般的路由器是工作在网络层的。 在每一层实现的协议也各不同，即每一层的服务也不同。下图列出了每层主要的协议。 鉴权 鉴权（authentication）是指验证用户是否拥有访问系统的权利。 CIG、IOCM、DM Server、Mongo DBCIG (Cell Interconnection Gateway) 信元互连网关 $\color{red}{未完待续~~}$]]></content>
      <tags>
        <tag>IOT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交流电机前言讲座考试总结]]></title>
    <url>%2F2019%2F07%2F29%2F%E4%BA%A4%E6%B5%81%E7%94%B5%E6%9C%BA%E5%89%8D%E6%B2%BF%E8%AE%B2%E5%BA%A7%E8%80%83%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[简单的做一下知识点梳理。 电机的控制问题 交流电机 同步电机 附赠：电机的应用]]></content>
      <tags>
        <tag>PMSM</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于404问题及感谢]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%85%B3%E4%BA%8E404%E9%97%AE%E9%A2%98%E5%8F%8A%E6%84%9F%E8%B0%A2%2F</url>
    <content type="text"><![CDATA[CNAME里面是写xxx.github.io 解析域名的时候最好用CNAME并 解析到 xxx.github.io GitHub 仓库里的Setting 最好也改成www.xxx.xxx 顺便感谢一下搭建博客参考网站 UP：CodeSheep UP：吃饱睡觉的猫 遇见西门]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于dsp平台电流环解耦]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%85%B3%E4%BA%8EDSP%E5%B9%B3%E5%8F%B0%E7%94%B5%E6%B5%81%E7%8E%AF%E8%A7%A3%E8%80%A6%2F</url>
    <content type="text"><![CDATA[120V电压 400rpm的给定电流 应该加的补偿值 ​ttt = 0.00105 \* \_IQmpy(pi_id.Fbk,pi_spd.Fbk) \* 4500 \* 9 /(volt1.DcBusVolt\*409.9) + 0.065 \* pi_spd.Fbk \* 4500 /(volt1.DcBusVolt\*409.9); 下面是解耦后应该的公式. ​ipark1.Qs = pi_iq.Out + ttt;​ 在未解耦的时候，测得ipark1.Qs = 0.515 . 换算成真实的电压值为(乘当前的电流值) 0.515 * 120 V=61.8V 测得ttt = 0.22 . 换算成真实的电压值为26.4V. ttt的主要成份是反电势 ​flux\*we=0.1552\*400\*4\*3.14/30=26.0V​ 所以证明电流环解耦 程序是正确的. 实验结果 （右为解耦）]]></content>
      <tags>
        <tag>DSP平台</tag>
        <tag>PMSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first generage blog]]></title>
    <url>%2F2019%2F07%2F23%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[第一个博客 杠杠滴]]></content>
  </entry>
</search>
